{"pageProps":{"posts":[{"frontmatter":{"slug":"/blog/js-til-2021-09-03","date":"Fri, 03 Sep 2021 14:03:09 GMT","title":"Custom async iterator in JavaScript","epoch":"1630677789","excerpt":"Today I learned how to implement and use an custom async iterator in JavaScript."},"markdownBody":"\nYou can write an async iterator/generator of your own in JavaScript.\n\nThe motivation behind the idea was that\n\n1. I wanted to call AWS.Comprehend.batchDetectEntities and pass 309 items of texts in an array.\n1. When I did the first time, AWS returned \"Throttoling Exception\". It turned out that the current settings allow only 10 items per second and a \"batch\" call is considered as N calls (number of calls in a batch call) not a call for the quota.\n1. So I thought I would split the array of texts into pieces so each piece has 10 items in it, and call the API over and over with 2 seconds interval.\n\nIt would be something like this:\n\n```\nasync function getPiece(data) {\n    const piece = data.splice(0, Math.min(10, data.length))\n    await new Promise((resolve) => setTimeout(resolve, 2000));\n    return piece\n}\n\nconst data = [ ... ]\n(async () => {\n    const results = []\n    const cloned = [...data]\n    while (cloned.length > 0) {\n        const piece = await getPiece(cloned)\n        const result = await AwsApiCall(piece)\n        results.push(result)\n    }\n    console.log(results)\n})()\n```\n\nIt waits 2 seconds and then pulls out the first 10 items from inputs and calls AWS API with the set of 10 items, process the result and see if there are more inputs.\n\nIt works, but I felt itchy because the content of the inputs or `cloned` at a particular moment was ambiguous from the code. The `getPiece` function modifies the content of `cloned` which is not obvious from the main function, but the main function is relying on the fact that the content would eventually become empty.\n\nThere could be many ways to write cleaner code than this. For example, the number of times the loop in the main function should iterate can be pre-calculated in the main function, thus it's not necessary for the main function to rely on the `getPiece` function to manage the content of inputs.\n\nBut I found it easy to use a generator/iterator here. The similar code could be written like this if we use an async generator:\n\n```\nasync function* dataSplitter(data) {\n    const cloned = [...data]\n    while (cloned.length > 0) {\n        const piece = cloned.splice(0, Math.min(10, cloned.length))\n        await new Promise((resolve) => setTimeout(resolve, 2000));\n        yield piece\n    }\n}\n\nconst data = [ ... ]\n(async () => {\n    const results = []\n    for await (const piece of dataSplitter(data)) {\n        const result = await AwsApiCall(piece)\n        results.push(result)\n    }\n    console.log(results)\n})()\n```\n\nThe `dataSplitter` yields 10 items every 2 seconds until there are no more data, and the main function doesn't have to know how it manages the content of the data. All the main function has to do is to call the AWS API for each piece that the generator yields in the `for await of` loop which was also new to me.\n"},{"frontmatter":{"slug":"/blog/2021-06-13_five-hour-course-drivers-license-ny","date":"Sun, 13 Jun 2021 22:49:20 GMT","title":"ニューヨークで運転免許取得へ動き出した","epoch":"1623624560","excerpt":"ニューヨーク州で運転免許を取得するための次のフェーズ、5時間講習を受講した話"},"markdownBody":"\n### 5時間講習を受けた\n\n[前回からの続き](/blog/2020-01-11_--------------45d1865c8b3a)。ニューヨーク州で普通自動車の運転免許を取るためには、仮免の交付を受けた後でまずは安全運転のための講習を受けなくてはならない。これを通称5時間講習という。\n\n### 仮免取ってからパンデミック中\n\n今思えばパンデミックで何もできなくなる寸前の2019年末、重い腰を上げて普通自動車の仮免を取った。当時の思惑は第一に財布に入るIDが欲しいということで、それ以外にはまあ今後車で出かける熱が高まったときに免許を取ろうと思うかもなあ、程度だった。\n\nそして2020年。3月からオフィスは閉鎖され、ひたすらWFHの日々である。週末に地下鉄に乗ってマンハッタンに行くことも憚られるニューヨーク市内の蔓延っぷり。元々引きこもり体質ではあるので、自分自身は土日どころか2週間以上アパートのドアから先に足を踏み出さない生活をしていても苦にはなっていなかった気がする（なおアパートはワンルーム）が、とはいえたまに外に出て散歩する程度でずいぶん気分が変わることも発見して、やはりこんな生活は長くは続けられないなあと思いつつ、だからって出かけられないので家にいた、2020年。\n\n[姉妹都市になって60周年を迎えた東京都](https://www.kateigaho.com/travel/74816/)とは大きく違い、ニューヨーク市の蔓延っぷりは酷かったこと、当時の大統領のホームタウンだったことや政治力のある街だったこと、などいろいろあって、2021年に入ってワクチンが承認されたとたん、すごい勢いで接種が行われた。結果、2021年の現大統領の宣言にあった7月4日を待たずにいろんなことが解除され、ほぼ普通の生活をしてよいことになったのが6月なかばのこと。\n\n### 自家用車に乗れる便利さ\n\nしかし実は会社の同僚など周囲を見渡すと、昨年中から結構みなさんお出かけをしていた。さすがこちらの人はメンタルが強いのか弱いのか、我慢できないだけなのか、みなさんリベラルだし科学的な判断を降せる人たちだけども、パンデミックな中でも、「週末は何してた？」「うん、アップステイトでAirBnBで友達とパーティした」みたいな会話が行われる。ドン引きしている自分の方が普通じゃない感じだった。\n\nそこでしかし気づいたのは、車で同居家族と出かけてAirBnBして庭でBBQして帰ってくるみたいな行動は、普段から会っていない人と会うことがほとんどないので、感染リスクも低いよねってこと。これが公共交通機関に乗ってレストランで食事という行動とは全然違う。鍵は車で行くかどうか。\n\n運転免許があったらなー気分が高まる。\n\n### めんどくさいこと\n\n普通は仮免を取ったらすぐに本免へむけて動くんだろうけど、当時は必要性をあまり感じていなかくて、とにかくめんどくさいという気分が上回っていたので自分は何もしていなかった。\n\nめんどくさいと感じていた理由は3点。\n\n1. 本免許を受けるための路上試験を受けるためには、まず座学の安全運転講習を受けなくてはならない。5時間。教室で。\n1. 路上試験の予約が取りづらい。ウンヶ月先まで予約でいっぱいとか。\n1. 路上試験を受ける車は自分で用意しなければならない。なおその車を運転して路上試験会場まで行くには運転免許証が必要。なので、自分以外の運転できる人と、運転していい車を用意しなくてはならない。\n\nこれだけめんどくさいことがあったらめんどくさいでしょう。ってことでめんどくさかった。\n\nが、時は来た。\n\nパンデミック中のどこかのタイミングで、5時間講習がZoomでオンラインでやってよいことになったのである。教室で拘束されるよりは100倍めんどくさくない。\n\nその他2点は変わらないのだけど、最初にしなければならないことが少しだけめんどくさくなくなったせいで、がぜんやっちゃおうかな気分が出てきた。それと、おそらくWFHが1年をすぎて、自分のメンタルも相当やられているというか、旅行に行きたいんだ！いい加減にどっか行きたいんだ！しかしビザの関係で国外には出られないんだ！国内で出かけたかったら車があるのとないのとでは大違いなんだ！1年以上日本に行ってないから国際免許もないし[*1]()！\n\n*1 なお、ニューヨーク州では、[州内に「居住している」人がいわゆる国際免許で車を運転するのは不法行為と認識される可能性がある](https://nytrafficticket.com/may-i-drive-in-new-york-with-an-international-or-foreign-license/)。周囲の日本人のみなさんでこれを気にしている人に一人たりとも会ったことがないのでおそらく運用上問題になったことがないんだろうけども。これはもしかしたら日本人の特権で、見た目がラテン系だったりすると気をつけなければならないことなのかもしれない。\n\nというわけで時が来たので、5時間講習の申し込みから始めることにした。\n\n### 5時間講習をどこで受けるか\n\nオンライン5時間講習はいろんな教習所で開催されていて、価格も中央値も平均値も$50くらい。調べていてわかったのは、5時間講習を受けた教習所で教師ありトレーニングを受けて、路上試験の予約をしてもらって、当日も車と教師に付き添ってもらうというパッケージを買うのが自然ということ。路上試験会場へ行く車と運転者を用意するというめんどくさい点を金で解決し、DMVのサイトで予約というダルい作業も丸投げできるというか、金で買うというか。\n\nで、そういうパッケージには、試験に向けてのアドバイスと練習をしてくれるいわゆる教習所の機能ももちろんあって、全然運転していない自分としてはまあ練習しておきたい。そうなると今度は練習の日程を予約しなければならないわけで、これがダルいってことになる。少し安いところの練習日程表を見ても、結構予約でいっぱいで、これを探して取るのがダルい。\n\n結果、ニューヨーク州で運転免許を取りたい日本人なら誰でも知ってる[Fuji Driving School](https://www.fujidrivingschool.com/)さんでやることにした。講習を日本語で受けたいと思っていたわけでもないんだけど、5時間講習から路上試験予約、で予約できた日時に合わせて（なるべく直前に）2回の練習をしたい、というこちらの要望を伝えるだけであとは全部やってくれて、一度書いた内容を何度も伝え直すこっちのサービスあるあるなダルさは一切なし。ちょっとお高めなんだろうけども、こっちは丸一年分のお出かけ予算がダブついてるんだから、と自分に言い聞かせ、ともすれば「めんどくさいからやっぱりやめた」となりがちな自分の心を自分のお金で殴って、5時間講習を予約した。\n\n### 5時間講習の受験とその後の手続き\n\n5時間講習は土曜日の朝10時から。途中5分休憩を2回とお昼休憩30分1回を挟みつつ、15時には終わった。途中でインターネット回線が切れて焦ったが数分で復旧してことなきを得る。\n\n講習受講前に、すでにメールで講習受講後に返信する内容として、練習するか、路上試験の予約代行をするか、当日の付き添いは、などの内容のアンケートが来ていたので、家の近くの路上試験会場の希望と、日時はそこそこ融通がきくのでなるべく早めに、あと試験日の直近で2回練習したい、できれば試験日と同じ曜日に、という希望を伝えた。上に書いたあるあるで、こっちのサービスって何かとこういう「Aは1でBは1か2、Cは3、Bが1ならDも希望」みたいなのをまとめて伝えると、返信が来て「Aです。Bはどうしますか」という返信が来がちでダルいことが多いんだけど、簡潔に完璧にこちらの希望をこなしていただきました。感謝。これに少し高いかもという値段に見合う価値があると思うかどうかは人それぞれだけど、私はおすすめします。\n\n[Fuji Driving School](https://www.fujidrivingschool.com/)\n\n路上試験は6週間ほど先に決まった。続く。\n\n"},{"frontmatter":{"slug":"/blog/20201122-bt-toggle","date":"Sun, 22 Nov 2020 16:43:23 GMT","title":"Android quick settings tile that [dis]connects a Bluetooth headset","epoch":"1606063403","excerpt":"I wrote a simple Android app in Kotlin for the first time in months for myself to get used to the current Kotlin development again"},"markdownBody":"\n## Motivation\nI stopped writing code for Android and for iOS sometime ago because my job doesn't need them at the moment. I started to feel wary of my not being able to write code for apps as well and as spontaneous as I used to be. I needed to write code in my spare time.\n\n## Contributing code to an open source project\nI think I first heard about [Signal](https://github.com/signalapp) a few years ago but it caught my attention again lately when I read a story or two about the situation in Hong Kong. I went to check it out and found it is being built open source and pull requests are being merged from time to time.\n\nSince then I am looking at its code regularly, both iOS and Android and submitted a handful of pull requests. Some of them are merged which felt good.\n\nBut it's not my code. I won't submit a pull request to Signal written in Kotlin because I am pretty sure that it doesn't help the project. If I think about my next job interview, I believe I should be able to write Android apps in Kotlin, smoothly. It requires a practice. So let's practice.\n\n## The three virtues of a good programmer\nI was looking for a topic to write code for; you know, [the three virtues of a good programmer](http://threevirtues.com/). I found one; I am using Sony WI-1000x Bluetooth headset every day and quite a few times I have been having to disconnect it from my phone to connect it to my laptop, and the other way around.\n\nThe problem is that in order to disconnect a headset from Android **without killing Bluetooth** (the Covid-19 app needs it being turned on), you have multiple steps to do. A few times a day. Annoying.\n\n![Disconnect a headset from an Android phone](/videos/disconnect-1.mp4)\n\n![Connect a headset to an Android phone](/videos/connect-1.mp4)\n\n## App requirements\n\nThese are the requirements that I had in mind while I was writing the app:\n\n* A quick settings tile that allows you to connect and to disconnect simply by tapping the quick settings icon\n* The app remembers the device to connect and to disconnect\n* You can select a device for the app to remember in a simple list UI\n* Don't turn off Bluetooth itself; connect/disconnect only the specific device\n\nAlso, these are the non-feature requirements, that are the reasons why I wanted to write code of the app:\n\n* Use Kotlin\n* Use Kotlin Coroutine for async operations\n* Use AndroidX ViewModel\n* May not use a DI framework but make the modules as friendly as possible to write automated tests\n* i.e. Avoid fat activity\n\n## Coding highlights\n\nI found a few interesting points while writing the app.\n\n### 1. Both A2DP and HEADSET had to be disconnected to disconnect the headset\n\nI am not sure if it is with WI-1000x or it is the way for all Bluetooth headphones, but I found that you can connect to the headset by calling `connect` method only to the HEADSET profile, but [you must call `disconnect` method both to HEADSET profile and to A2DP profile](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/AndroidBluetoothAdapter.kt#L94-L113) in order to disconnect the headphones from the phone.\n\nBecause the code to receive a profile object is done asynchronously, I ended up writing code like this ([more code here](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/AndroidBluetoothAdapter.kt#L19-L55)) below to wait for both profiles before moving on to allow the app to intereact with the headphone. I feel there must be a better way of writing this type of *wait all events to happen before moving on* kind of code like `Promise.all` in JavaScript but wasn't able to find it atm.\n\n```\nsuspend fun onReady() {\n    coroutineScope {\n      while (true) {\n        delay(1000L)\n        if (headsetProxy != null && a2dpProxy != null) break\n      }\n    }\n  }\n```\n\n### 2. You must release the profile objects before leaving\n\nAndroid Studio was kind enough to let me know that the profile objects are leaking when I implemented the [TileService](https://developer.android.com/reference/android/service/quicksettings/TileService) for the app. I was kind of wondering the necessity of doing it but it was not obvious when I was writing the [Activity](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/MainActivity.kt) for the app.\n\nBut surely you need to [clear profile objects](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/AndroidBluetoothAdapter.kt#L115-L118) before [you leave](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/MainTileService.kt#L21-L39).\n\n### 3. You cannot [dis]connect Bluetooth headphones by Android SDK\n\n!!! Surprise! I wasn't able to find a simple and easy way to connect or to disconnect the headphones in Android SDK. Well there may be one, but simple web searching and browsing SDK website did not show me a `connect` or `disconnect` method to do it.\n\nI figured that folks out there who wanted to do that were calling the [connect](https://android.googlesource.com/platform/frameworks/base.git/+/48695cb07352cb3033dc11bb828e1ca947dfc04d/core/java/android/bluetooth/BluetoothA2dp.java#279) method and the [disconnect](https://android.googlesource.com/platform/frameworks/base.git/+/48695cb07352cb3033dc11bb828e1ca947dfc04d/core/java/android/bluetooth/BluetoothA2dp.java#319) method that were marked as `@UnsupportedAppUsage` through reflection. It worked ok, and I have no intention to publish this app for anybody to use, so [I just settled with it](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/AndroidBluetoothAdapter.kt#L82-L92).\n\n```\nval connect = BluetoothHeadset::class.java.declaredMethods.findLast {\n  it.name.equals(\"connect\")\n}\nconnect?.setAccessible(true)\nconnect?.invoke(headsetProxy, device)\n```\n\n### 4. You cannot collect two StateFlows in a launch block\n\n`collect` method of `StateFlow` blocks? I was not able to run this code as intended:\n\n```\nselectedIndexJob = CoroutineScope(Dispatchers.Main).launch {\n  viewModel.selectedIndex.collect { index ->\n    if (index < 0) return@collect\n    listAdapter.notifyItemChanged(index)\n    sharedPreferencesAdapter.setLastSelectedAddress(viewModel.getDevice(index).address)\n  }\n  viewModel.previousIndex.collect { index ->\n    if (index < 0) return@collect\n    listAdapter.notifyItemChanged(index)\n  }\n}\n```\n\n... because the second `collect` (or the block of it) was never called even when the state was updated. I had to write like this:\n\n```\nselectedIndexJob = CoroutineScope(Dispatchers.Main).launch {\n  viewModel.selectedIndex.collect { index ->\n    if (index < 0) return@collect\n    listAdapter.notifyItemChanged(index)\n    sharedPreferencesAdapter.setLastSelectedAddress(viewModel.getDevice(index).address)\n  }\n}\npreviousIndexJob = CoroutineScope(Dispatchers.Main).launch {\n  viewModel.previousIndex.collect { index ->\n    if (index < 0) return@collect\n    listAdapter.notifyItemChanged(index)\n  }\n}\n```\n\n... that is fine, but had to struggle a little bit of why the second block was not executed.\n\n### 5. AndroidX ViewModel and constructor injection\n\n[`by viewModels()`](https://developer.android.com/topic/libraries/architecture/viewmodel) is an easy way to get a view model instance but it requires the view model to have a default constructor. [You have to create your own `ViewModelProvider.Factory`](https://github.com/fumiakiy/BTToggle/blob/4780b091fc754040e14e96bb3c34d7fecb48c80d/app/src/main/java/com/luckypines/android/bttoggle/MainViewModel.kt#L47-L55) that instantiates a new object if you want to do constructor injection.\n\n### 6. lifecycleScope.launchWhenStarted ???\n\nI wrote the app on Saturday, November 21 2020. I'm writing this part the following day, and one of my favorite news letters [**Android Dagashi** issued a new post](https://androiddagashi.github.io/issue/147-2020-11-22/). In it was [the link to the SDK document about Kotlin Flows](https://developer.android.com/kotlin/flow/stateflow-and-sharedflow), and I learned that there is this thing called `launchWhenStarted`. The description goes:\n\n> \"In the previous example that used launchWhenStarted to collect the flow, when the coroutine that triggers the flow collection suspends as the View goes to the background, the underlying producers remain active.\"\n\nOh, so with that you don't have to keep a `Job` object, launch it and cancel it because it's handled by the context? Huh. I was doing it manually but you probably don't need this code any more (and the code I wrote may be a wrong way to do it in the first place?). I'll update the code when I have bandwidth.\n\n## Conclusion\n\nWhat ended up being implemented is here: [BTToggle app in GitHub/fumiakiy](https://github.com/fumiakiy/BTToggle/)\n\nMy life is now a little bit easier with this app and I am satisfied.\n\n![Disconnect a headset from an Android phone](/videos/disconnect-2.mp4)\n\n![Connect a headset to an Android phone](/videos/connect-2.mp4)\n"},{"frontmatter":{"slug":"/blog/20200815-m5stickc-mhz19b","date":"Sun, 16 Aug 2020 14:16:51 GMT","title":"M5StickCとMH-Z19Bで部屋の二酸化炭素を計測して記録する","epoch":"1597587411","excerpt":"MH-Z19BをM5StickCに接続して、CO2センサーのデータを読み取ってGoogle Sheetに記録するプログラムを書く。","ogImage":"/images/m5stickc-mhz19b-complete.png"},"markdownBody":"\n[前回](/blog/20200815-m5stickc-http)からの続き。\n\n## MH-Z19B\n\n![M5StickCとMH-Z19B](/images/m5stickc-mhz19b.png)\n\n[MH-Z19B](https://www.winsen-sensor.com/sensors/co2-sensor/mh-z19b.html)を[M5StickC](https://m5stack.com/collections/m5-core/products/stick-c)に接続して、CO2センサーのデータを読み取ってGoogle Sheetに記録するプログラムを書く。\n\nまずMH-Z19BについてるピンとM5StickCを接続する。MH-Z19Bに電力を供給するために、MH-Z19Bの`Vinピン`とM5StickCの`5V out`をつなぎ、MH-Z19Bの`GNDピン`とM5StickCの`GND`をつなぐ。MH-Z19BからデータをM5StickCで読み取るために、MH-Z19Bの`Txピン`とM5StickCの`G0`、MH-Z19Bの`Rxピン`とM5StickCの`G26`をつなぐ。M5StickC側のGPIOピンはなんでもいいっぽいが、つないだピン番号をコードに書く必要がある（後述）。\n\n|||\n|---|---|\n| [![MH-Z19Bのピン接続](/images/mhz19b-pins.png)](/images/mhz19b-pins.png)| [![M5StickCのピン接続](/images/m5stickc-pins.png)](/images/m5stickc-pins.png) |\n\n## MH-Z19Bのデータを読むコード\n\n接続できたらM5StickCでデータを読むコードを書く。日本語で書かれたM5StickCでMH-Z19Bを扱う記事を読むと、[UIFlowでPythonでコードを書いている例](https://kitto-yakudatsu.com/archives/7286#toc21)や[M5CloudでPythonを使う例](https://medium.com/tichise/m5stack%E3%81%A7%E5%AE%B6%E3%81%A8%E3%82%AA%E3%83%95%E3%82%A3%E3%82%B9%E3%81%AE%E4%BA%8C%E9%85%B8%E5%8C%96%E7%82%AD%E7%B4%A0%E6%BF%83%E5%BA%A6%E3%82%92%E8%A8%88%E6%B8%AC%E3%81%97%E3%81%9F-3553f7a1434d)が出てきてArduino/Cなコードが出てこなくて一瞬焦ったが、ArduinoのLibrary Managerで[MH-Z19を検索して出てくるライブラリ](https://github.com/WifWaf/MH-Z19)を`#include`して普通に使えた。\n\n実際のコードを書くときにライブラリの[examples/BasicUsage](https://github.com/WifWaf/MH-Z19/blob/master/examples/BasicUsage/BasicUsage.ino)を参考にしたんだが、M5StickCで使う場合は一部コメントアウトされている方を使う必要があったりする。別にポイントってほどのことではないが、Serial1をbeginで開くときに指定する実引数でボーレート(BAUDRATE)を9600にして、上述のTxとRxの番号を指定することで正しくデータを読み取れるようになる。この辺がサンプルコードの中では`HardwareSerial`になっていたり(`Serial1`でよい)、RX_PINとTX_PINの値が違っていたり(M5StickCではGPIO10やGPIO11へのアクセスはない)するが、まあ見ればわかるって話だと思う。[ESP32にはSerial1とSerial2っていう系統があるっていうのはこの記事で知った](https://lang-ship.com/blog/work/m5stickc-uartserial/)。\n\nソース: [https://github.com/fumiakiy/M5StickC/](https://github.com/fumiakiy/M5StickC/commit/6038aa3dbb8ade0c36a88142eef4f365a9dece8a#diff-ac3ed53726558e3635fc88015490f1bc)\n\n![CO2センサー完成。気温は高めに出る。](/images/m5stickc-mhz19b-complete.png)\n\n\n## Google Sheetに記録するコード\n\nCO2センサーのデータをSerialに出力してそれっぽいデータが取れていることが確認できたので、これをGoogle Sheetに送って記録しておくことにする。Google SheetにひもづけたApp ScriptをWeb Deployすれば勝手にHTTPのエンドポイントを作ってくれるので、下記のような単純なApp Scriptで、HTTP POSTでJSONを受け取ってGoogle Sheetに追加できる。\n\n```\nfunction writeData(ar) {\n  const d = new Date()\n  const aar = [...ar, d.getTime(), d.toLocaleString()]\n  const sheet = SpreadsheetApp.getActiveSheet();\n  sheet.insertRowBefore(1);\n  sheet.getRange(1, 1, 1, aar.length).setValues([aar])\n}\n\nfunction doPost(e) {\n  const d = e.postData.contents;\n  const j = JSON.parse(d);\n  writeData(j);\n  return ContentService.createTextOutput(\"ok\");\n}\n```\n\nこのコードをWeb DeployすればURLが割り当てられるので、それに対してM5StickCからHTTP POSTすればよい。めんどくさいので認証は省いてURLを知っていれば誰でもアクセスできるようにしちゃった。\n\nソース: [https://github.com/fumiakiy/M5StickC/](https://github.com/fumiakiy/M5StickC/commit/07fb585184442a125a88a250bdc4a14d8c23076b#diff-ac3ed53726558e3635fc88015490f1bc)\n\nあとは1日データを溜めて、グラフ化してみよう。"},{"frontmatter":{"slug":"/blog/20200815-m5stickc-http","date":"Sat, 15 Aug 2020 22:45:54 GMT","title":"M5StickCでベッドサイドのランプをOn/Offする","epoch":"1597531554","excerpt":"実際はSonoff Basic R3が楽しかったので味をしめて、何かIoT的なおもちゃでもっと遊べないかなあと思っていた。Sonoff Basic R3を調べる中で、要するにおもしろいのはESP32というWifiとBluetoothが入ったマイクロコンピュータであるのがわかったので、ESP32の開発ボードを眺めては何かできないかなあと。","ogImage":"/images/m5stickc.png"},"markdownBody":"\n[Sonoff Basic R3で電源コードにWifi経由でコマンドを送れる](/blog/2020-05-23_Sonoff-Basic-R3-------------------f5498b92027b)ようになって数ヶ月。以前に比べて進化した生活を送れてはいるものの、一つとてもめんどくさいことがある。Apple開発者プログラムに年間購読料を納めないと、iOSアプリのProvisioning Profileが一週間でexpireすること。なんで配布するわけでもないアプリを自分のiPhoneでだけ使うために年間1万円もお布施せねばならんのだ。初詣のお賽銭でもそんなに払わないのに。\n\nとかいいつつ、実際は[Sonoff Basic R3](https://sonoff.tech/product/wifi-diy-smart-switches/basicr3)が楽しかったので味をしめて、何かIoT的なおもちゃでもっと遊べないかなあと思っていた。Sonoff Basic R3を調べる中で、要するにおもしろいのは[ESP32というWifiとBluetoothが入ったマイクロコンピュータ](https://www.espressif.com/en/products/socs/esp32)であるのがわかったので、[ESP32の開発ボード](https://www.espressif.com/en/products/devkits)を眺めては何かできないかなあと。\n\n## WFHとCO2\n\n[弊社も来年の夏まで自宅作業が決定した](https://www.facebook.com/hello.iandco/posts/1186593688375252)ので、仕事環境を少し改善しなければと机や椅子を物色してみたものの、狭い部屋にこれ以上家具を置く気にもならなかった。そんな中、[WFHで環境改善の文脈で部屋の二酸化炭素濃度を測るのが流行っている](https://bunshun.jp/articles/-/36791)のを知る。これだ。\n\n\"Arduino CO2 sensor\"とかで検索すると、[MH-Z19BというCO2センサーがある](https://www.winsen-sensor.com/sensors/co2-sensor/mh-z19b.html)のがわかった。ArduinoではなくESP32の開発ボードとこのセンサーでCO2濃度をGoogle Sheetか何かに送って可視化するというプロジェクトをやってみよう。\n\n## M5StickCとMH-Z19B\n\nそういうことをしている例を検索してみたところ、[M5StickC](https://m5stack.com/collections/m5-core/products/stick-c)という製品に行き着いた。中にESP32 Picoってのが入ってて、LCD液晶とボタンが2つと、そして汎用IOピンがある。[M5StickCにMH-Z19Bをくっつけて使うためのケースを売っている人もいて](https://kitto-yakudatsu.com/archives/7286) (*1)、この組み合わせは成功例があるらしい。これだ。\n\n```\n*1 正確には、MH-Z19Bの利用例を探していてこの記事を見つけ、それでM5StickCというものの存在を初めて知りました。\n```\n\nM5StickCもMH-Z19BもAmazonで売っているのだが、検索してみるとそれよりだいぶ安い通販サイトがいくつかある。M5StickCはいろんな場所で売っているのだが、MH-Z19BとM5StickCを両方売っていて安いサイトを探し回った結果、[Banggood](https://www.banggood.com/)で買うことにした。決め手は、最安ではないもののそれなりに安価で両方売っていることと、[MH-Z19Bにピンをはんだ付けしてあるバージョンを売っている](https://www.banggood.com/MH-Z19-MH-Z19B-Infrared-CO2-Sensor-Module-Carbon-Dioxide-Gas-Sensor-for-CO2-Monitor-0-5000ppm-MH-Z19B-NDIR-with-Pin-p-1693604.html?rmmds=search&cur_warehouse=CN)こと。はんだごてを買うよりジャンパーワイヤーを買う方が安いし、まだはんだごてを買うほどのことでもないし。まあこの二酸化炭素センサーをずっと使い続けるならはんだ付けする日も来るかもしれないけど。\n\n説明を読むと、USやカナダにも倉庫を持っていてそこからの発送ならそんなに時間もかからなそうとか。Banggoodの評判を検索するとあまりよろしくないのだが、内容を読むと「洋服を買ったら偽物だった」「クリスマスに欲しかったのに届かなかった」など、いやそれはどうなのよ的なものが多かったので、せいぜい$35だしというわけで、念のためPayPal支払いで7月27日に発注した。\n\n発注して数時間後にはUSPSのトラッキングコードが割り当てられた。しかしそこから8月10日までの二週間、動きは一切なし。8月10日になってNJのUSPSに届き、そこからなぜかPAを経由して8月13日に無事に届いた。USPSのラベルの下に中国語が書かれた別のラベルが貼ってあって、それにはJFK宛てみたいに書いてあったので、結局中国のどこかからどうにかしてJFKに来て、そこから国内発送されたっぽいんだけど、詳細はわからず。でも送料**90セント**、保険70セントで三週間で届いたのでなんの文句もありません。\n\n![M5StickC](/images/m5stickc.png)\n\n## M5StickCでベッドサイドランプのON/OFF\n\nCO2の前にまずは簡単そうなほうをということで、M5StickCをWifiにつなげて、HTTP POSTでコマンドをSonoff Basic R3に送ることで、M5StickCのボタンを押したときにベッドサイドランプをOn/Offできるようにする。M5StickCのWifiでHTTP POSTする方法はそこら中に例があるので詳細は書かないが、一点だけ少しハマった。\n\nArduinoの`setup() -> loop()`を使ってプログラムを書いた。まずはArduinoのプログラム例でありがちな、`loop()`の中でなんらかの処理をして、`delay(1000)`とかで1秒寝てから`return`し、次`のloop()`が呼ばれる、みたいなコードを書いた。その`loop`の中で`M5.BtnA.isPressed()`を見て「ボタンが押されたら...」というコードを書いたんだが、ボタンを押しても全然反応しない。\n\n`M5.update()`で状態を更新した後、`M5.BtnA.isPressed()`を見ると今その瞬間にボタンが押されたのかどうかがわかるという仕組みなので、`delay(1000)`とかやってると、その1秒間のどこかでボタンを押しても、次に調べたときには`isPressed`はもう見えなくなっているってことだと思う。\n\n結局、`loop()`のなかで`delay`して処理を1秒ごとにするようなことはやめて、`loop()`はクロックに応じて実行されるのだが、中で現在時刻を`millis()`で取って、以前の`millis`と比較して、1000ms経過してたら処理する、ってなコードに変更してうまくいった。\n\n```\nunsigned long lastMillis = 0;\nunsigned long tick = 0;\nunsigned long INTERVAL = 1000; // 1sec\n\nvoid loop() {\n  M5.update();\n  unsigned long currentMillis = millis();\n  if (currentMillis - lastMillis > INTERVAL) {\n    statusUpdate(tick++);\n    lastMillis = millis();\n  }\n  if (M5.BtnA.isPressed()) {\n    toggleSwitch();\n    powerOff();\n  }\n}\n```\n\nこの機能を使うのは、夜寝る前にランプをONにするときと、寝る直前にOFFにするときだけなので、ONやOFFにしたらその度に本体の電源を切ることにした。M5StickCには小さいバッテリーが入っているので、こまめに電源を切っておけば結構長いこと充電なしで使えるはず。\n\nソース: [https://github.com/fumiakiy/M5StickC/](https://github.com/fumiakiy/M5StickC/commit/c64dc7fb78ee0732c97ae0a726fbdf3e2940c4a1)\n\n[次はCO2センサーにかかろう](/blog/20200815-m5stickc-mhz19b)と思う。\n\n"},{"frontmatter":{"slug":"/blog/2020-05-23_Sonoff-Basic-R3-------------------f5498b92027b","date":"Sat, 23 May 2020 21:14:59 GMT","title":"Sonoff Basic R3でセキュアなスマートホームを構築する","epoch":"1590268499","excerpt":"我が家でもようやく手元のスマホでベッドサイドのランプを消せるようになりました。"},"markdownBody":"\n我が家でもようやく手元のスマホでベッドサイドのランプを消せるようになりました。\n\n### ホモ・サピエンスになりたい\n\n我が家のベッドサイドにはIKEAのKvart(3 heads)というフロアランプを置いているのだが、これが首ごとにスイッチがあってしかも1つ1つが暗いので常に3つつけるせいで、寝る前に手を伸ばして3つのスイッチを切るのがうっとうしくて仕方ない。うっとうしくて仕方ないと言いつつ5年間毎日やっていたので、実はうっとうしくないのかもしれない。\n\n![IKEA Kvart floor lamp](/images/0*-6XxE5q1W_65wZ1C.jpg)\n\nしかし先日[Rebuild.fm (ep.246](http://rebuild.fm/264/) を聞いていた)ら、手で電気を消すなんて原始人かと（ポッドキャスト越しに）言われたことと、Work from homeで少し時間と体力に余裕があるので、重い腰をあげてスマートホーム（）に挑戦することにした。目標は、このフロアランプの3つのスイッチすべてをスマホでOn/Offできるようにすること。当然ながら簡単なのは、適当なスマートプラグを買って壁のアウトレットに刺して、ランプのコードをそのスマートプラグに刺すことだろう。でもなあ。\n\n### Arduinoで自作しようかな\n\nただランプの電源をOn/Offするだけ、ボイスコントロールも不要、IFTTTもAlexaも外出先からの操作も一切不要。むしろ、これらの機能は付いていて欲しくない。プラグとスマホが同じ空間にある前提で、スマホからプラグにコマンドを送る機能だけが欲しい。プラグからインターネットに出て行って欲しくない。代わりに何らかのAPIが欲しい。謎のうんちゃらCloudに登録とかしたくない。うんちゃらCloudから我が家の電気系統にアクセスなんてして欲しくない。\n\nという条件からまず考えたのが、Arduinoを使ってBluetoothでコントロールできるプラグを自作すること。その手の電気工事関連の資格は一切持っていないし、小学生のときに電子工作が流行ってはんだ付けはできないこともない、くらいのスキルセットだが、まあYouTubeでどうにかなるのでは。\n\n調べてみると、電源を入れたり切ったりするにはリレーという部品を使うらしい（その程度の知識もありませんでしたよ）。高ボルテージの電気を扱うのは危険だよねってことで、[IoT Relayなる製品](https://www.sparkfun.com/products/14236)もあることを発見。\n\n![Sparkfun IoT Relay](/images/0*XEb_J-JCb0nuY-3G.jpg)\n\n$30弱か…。これにArduino UnoとBluetoothモジュールをつけて、うーん随分かかるなあ。面白そうではあるけども、この4口もあるタップをただランプのOn/Offのためだけに買うのもバカバカしい。\n\n普通のArduinoで使えるリレーモジュールなら$5くらいで買えるので、それの使い方を調べてみた。見つけたのが[Arduinoで家電を操作しようというタイトルのこの記事](https://www.circuitbasics.com/build-an-arduino-controlled-power-outlet/)。何と電源タップのコードをちょん切って、電気が通る線をリレーにつないでいる。なるほど、これでリレーについているI/Oピンへ信号を送ることで電気を通したり通さなかったりできるということか。こりゃあ面白そう。\n\n![Cut the wire](/images/0*_eVKi14W8beaQ_YN.jpg)\n\n電源コードを切って出てくる3本のワイヤーがそれぞれ何で、どれがどれかをどうやって調べるのか、などなどを調べて机上の空論的な自信をつけたので、パーツをAmazonのカートに入れてみる。うーん、Arduino Unoとブレッドボードとワイヤーと、それに電源コードと交換用アウトレットなどを一通り揃えるとやっぱりそれなりの値段になるよね。あと、Arduino UnoとBluetoothモジュールを買って、それをベッドサイドに固定してランプのOn/Offのためだけに使うのってやっぱり無駄なんじゃないかなあ。\n\n### Sonoff Basic R3\n\nArduino経由じゃなくて、本体にHTTPサーバーか何か入っていてAPIで操作できるスマートプラグはないのか、検索してみた。Belkinの製品にAPIがあるらしいんだけども、やっぱり外部のサイトに登録とかすることになる。それはやりたくない。\n\nもう少し探して、[Sonoff](https://sonoff.tech/)に行き着いた。どうも主にEU向けに製品を作っているみたいだが、USで使っている人もいるみたい。[OSSのファームウェアを焼けばHTTPで操作できる](https://github.com/arendst/Tasmota)という情報があってドキュメントを見に行ったら、最新の[R3](https://sonoff.tech/product/wifi-diy-smart-switches/basicr3)では[DIYモードがあらかじめ入っていて、HTTPで操作できる](http://developers.sonoff.tech/sonoff-diy-mode-api-protocol.html)こと、[DIYモードにすると公式アプリからすら接続できなくなり、APIでしか操作できないらしい](https://notenoughtech.com/featured/sonoff-r3-diy-mode/)ことを発見した。これはスマートプラグではないが、求めていたのはこれじゃないか？\n\n![Sonoff Basic (https://www.electroschematics.com/sonoff-basic-wi-fi-switch/)](/images/1*pyH9Mh1_WMal12XwjvJ1nA.png)\n\nSonoff BasicはどうやらWifiチップとリレーがつながっているだけのボードで、Arduinoとリレーを使うときのような要領で電源コードの中間にこいつを挟むように配線すれば、あとはWifiで電気をOn/Offできるということらしい。GNDを配線する場所がないのもUS向けではないためなのか、ちょっと不安になるんだけども、YouTubeを見ても多分アメリカの人が使っているし、大丈夫だと信じる。こいつにすれば、これ1つ($10)と電源延長コード($5)だけ用意すればおしまいなので、早速Amazonで注文した。[Sonoff Basic R3自体は1つ$5くらい](https://www.itead.cc/sonoff-basicr3-wifi-diy-smart-switch.html)だし、延長コードなんて$1くらいで買えるものだけども、送料を入れればこんなもんだろう。\n\n### Cut the wire\n\nSonoff Basic R3につなぐので、あえてアースのない2 prongの延長コードを買った。アースはアースでSonoff Basic R3の外でつないでしまえばいいんだけども、考えたくなかったので。さあぶった切るぞ。\n\n![ラジオペンチで切る](/images/1*k9_0GznkTEHg0Dvg2I8pWw.png)\n\n![切れた](/images/1*CrjsC8gLN4DR9Ok1yt5yQg.png)\n\nワイヤーストリッパーもないので、引き続きラジオペンチでガワをむく。\n\n![力の込め具合を調節して](/images/1*5eqBKdZmEb4shffqKiQ0PA.png)\n\n![ワイヤーむけた](/images/1*yA4kwVtGmXmaaCkx-dO4ww.png)\n\n### Sonoff Basic R3に接続\n\n切った両端でガワをむいたら、Sonoff Basic R3のターミナルに接続する。ターミナルにはそれぞれNとLがあって、ニュートラルをNに、電線をLにつなぐ。2本の線のどっちがニュートラルかは、[byte sizedのYouTube動画で教えてくれた](https://youtu.be/lRCH67wfwNE?t=140)。USでは、極性のあるプラグ（2つのプラグで大きさが異なる）では大きいプラグの方がニュートラル。さらにニュートラルの方には、コードの外側にギザが付けられている、とのこと。見てみると確かにコードにギザが付いている方が大きな方のプラグの側になっている。\n\n![両端をSonoff Basicのターミナルへ](/images/1*m9_ppljWMni3YcY44niXmg.png)\n\nSonoffのホームページにも書いてあるし上の写真では外してあるカバーにも書いてある通り、両端ともターミナルの上の方がNで下がLになっている。コードのギザを確認して、ワイヤーを突っ込みネジを締める。\n\n![このコードの外側のギザは反対側にはないので、こっちがニュートラル](/images/1*v-7SPSMANi3VKyBwVLRxXg.png)\n\n付属品のコードを押さえるカバーを付けて、化粧カバーをつければ完成。ニュートラルと電線を間違えないようになんども確認した。\n\n![ギザのある方がNにつながっている](/images/1*00BdgcE11ByYJHj2rGSgoQ.png)\n\n![こちらもギザのある方がNにつながっている](/images/1*TNSZNsAw0LaQ14Hqbq5fLA.png)\n\n![完成した延長コード](/images/1*tHwsW0QBTwtlnCfYBIbNbw.png)\n\n### ファームウェアを更新\n\nまずはDIYモードの前に最新のファームウェアに更新する。Sonoff BasicはR3が出るまでは公式DIYモードはなくて、[内部で使われているWifiチップ用のファームウェア](https://tasmota.github.io/docs/)を焼いて遊ぶのが一般的だったらしい。\n\nR3でSonoffが公式DIYモードを用意したんだが、最初のバージョンでは本体の内部カバーも開けて中にあるピンにジャンパーをはめてDIYモードにする方式で、さらにDIYモードになったR3はあらかじめ設定されている名前のSSIDのWifiへ接続しようとするので、こちらでその名前のアクセスポイントを立てて、接続させて、それからいろいろ操作するという手順だったらしく、[R3のDIYモードなんか使い物にならん](https://notenoughtech.com/featured/sonoff-r3-diy-mode/)的な投稿が多い。\n\nSonoffはその辺敏感に反応して、現在のDIYモードはジャンパーもいらないし、R3のハードウェアスイッチを長押しするだけでDIYモードに入って、あとはR3に直接アドホック接続して設定をすれば良くなっているので、簡単になった。ただそのバージョンのファームウェアが入っているかどうかわからないので、とりあえずはファームウェアの更新をするという話。\n\nR3を電源に接続するとQuick Setupモードになるので、あらかじめダウンロードしておいた[eWelinkアプリ](https://play.google.com/store/apps/details?id=com.coolkit&amp;hl=en)でデバイスを検出して、SSIDやパスワードを設定してR3を自宅の無線ルーターに接続する。ファームウェアの更新もアプリでできる。本稿執筆時点では3.5.0が入った。\n\n### Let’s DIYモード\n\nファームウェアを更新したらいよいよDIYモードである。まずはR3に付いているハードウェアボタンを長押しして、青いランプを点滅させる（5秒くらい）。規則正しい点滅と、短・短・長の点滅の2種類があるので規則正しい方にする。\n\n![DIYモード](/images/1*6gUE3c7zBU9ochjTPin4Kg.gif)\n\nこのときR3は**ITEAD_xxxx**という名前のSSID（**xxxx**部分はブツによって異なる）でアドホックWifiを作っているので、それに接続して（パスワードは[Sonoffの開発者向けページ](http://developers.sonoff.tech/sonoff-diy-mode-api-protocol.html)に書いてある）、ブラウザーで**http://10.10.7.1**にアクセスする。そこで自宅のWifiのSSIDとパスワードを設定できる（11acとかの5Ghzには対応していない）。設定したらR3は再起動し、うまくいけば自宅のWifiネットワークに接続する。自宅Wifiのルーターの管理画面に行けばIPアドレスはわかるだろう。\n\n### WiresharkでmDNSの中身をみる\n\nあとはAPIドキュメントに沿って、curlでIPにむけてPOSTでも送ってやればいいんだろう…と思ったらそうでもなかった。IPとポート（デフォルトのポートも[Sonoffの開発者向けページ](http://developers.sonoff.tech/sonoff-diy-mode-api-protocol.html)に書いてある）が分かっても、R3のデバイスIDがわからないと、POSTするデータを作れない。R3はデバイスIDをmDNSでマルチキャストしていて、[Windows用のクライアントアプリ](https://github.com/itead/Sonoff_Devices_DIY_Tools)もあるのだが、mac OS用のアプリはない。仕方ないので、Wiresharkで通信をのぞき見た。必要なのは、eWeLink_nnnnnnnnのnnnnnnnの部分。\n\n![Wiresharkは万能](/images/1*bVHikqIMdjbxwpeRzTi6Cg.png)\n\nIPアドレス、ポート、デバイスIDの3つがわかれば、curlでHTTP POSTを送るだけで電源On/Offができる。\n\n```\n$ curl -X POST -d '{\"deviceid\":\"100090380b\",\"data\":{\"switch\":\"off\"}}' http://192.168.mm.nn:8081/zeroconf/switch</a>\n```\n\n```\n{\"seq\":2,\"error\":0}\n```\n\n毎晩curlするのもめんどいので、OnとOffのボタンがあるだけのAndroidとiOSのアプリを作った。\n\n### スマートでセキュアでプライベートなホーム\n\nこうしてベッドサイドランプを手元のスマホでOn/Offできるようになった。ボイスコントロールとかもやろうと思えばAndroid/iOSそれぞれのアプリ内でできるだろうし、謎のなんちゃらCloudにデバイスやアカウントを登録しない、自宅LAN内で完結したプライベート スマート ホームになった…気がする。R3はhttpしか話さないのだが、自宅の無線ネットワークに入られない限りはまあ大丈夫かなと。\n\n電源On/Offできればそれでよいタイプの家電製品を操作するならSonoff Basicはほんとに簡単だった。何かネタがあれば今後も使いたい。もっとも、Sonoff BasicはただWifiチップのESP8266とリレーを組み合わせただけなので、[ESP8266の開発ボード](https://www.amazon.co.jp/dp/B078WM1YQW/)を使えばいろいろ面白いことができるのかもしれない。\n\n"},{"frontmatter":{"slug":"/blog/2020-01-11_--------------45d1865c8b3a","date":"Sat, 11 Jan 2020 22:49:20 GMT","title":"ニューヨークで仮免を取った","epoch":"1578782960","excerpt":"前回からの続き。仮免を取るための筆記テストを受けるために、12月30日の朝10:30のアポイントメントをDMVのWebサイトで取った。"},"markdownBody":"\n### ニューヨークで仮免を取った\n\n[前回からの続き](/blog/2019-12-29_-------------------e3f6436b0b67)。仮免を取るための筆記テストを受けるために、12月30日の朝10:30のアポイントメントをDMVのWebサイトで取った。\n\n### テスト勉強\n\n12月23日で仕事を納めて、後の一週間でDMVのサイトにある[Drivers manual and practice tests](https://dmv.ny.gov/driver-license/drivers-manual-practice-tests)をひたすらやり、答えをほぼ覚える。これ以外の準備は何もしなかった。他の人が書いているブログなどを読んでも、ほとんどこれと同じ問題が出ると書いてあるし。\n\n### 当日\n\n12月30日当日。10時10分にマンハッタンのダウンタウンのDMVオフィスへ到着。ここにした理由は、単に以前近くに住んでいて場所を知っているからというだけだった。中に入ると、アポイントメントのメールに付いていたQRコードを読み取る機械が2台設置されていたが、両方ともout of orderでそこに職員が立っている。その人に「筆記テストを受けにきた」と伝え、アポイントメントのメールを印刷した紙を見せると、「今10:10だからまだ時間じゃない」。\n\n「じゃあここで待ってればいいの？」\n「いや、んー」、横にいたもう一人に\n「この人、筆記テストのアポイントメントを持っているんだけど」\n「筆記テストにアポなんかない」\nは？DMVのWebで取ったアポなんだけど？\n\n結局その横にできていた30人くらいの列に並ばされた。最初はなんの行列かもわからなかったけど、並んでいるうちにこれは写真を撮るのを待つ列だと判明。まわってきた別の職員からピンクの紙を受け取って署名する。写真を撮っていいよという同意らしい。順番が近づくと、ブース内の別の職員から今日の目的を聞かれて「テスト受験」と答えると、Tで始まる番号札を渡された。\n\n順番がまわってきて写真を撮る。写真を撮ったら、番号が呼ばれるまで待機。待機エリアにはベンチが大学の教室くらいの数あったが、すでに満席。呼ばれている番号を眺めると、30番くらい後だった。まあ薄々想定してはいたけども、結局アポイントメントを取った意味はゼロだった。最初に「じゃあここで待ってればいいの？」って聞かずに時間まで待っていたとしたら、20分後に同じ行列に並ばされることになっていたはず。早速の洗礼である。\n\n写真を撮る場所のすぐ横にテストエリアがあって、PCが10台くらい置いてあって、それでテストを受ける（マウスで選択するだけ）のだが、常時1、2人しか受験しておらず、それでいて番号はちっとも進まない。まあそんなことだろうとは思っていたよ。スマホにダウンロードしておいたPDFでテスト内容をひたすら復習。\n\n90分くらい経過しただろうか、とうとう妻の番号（自分の番号の1つ前）が呼ばれたので、二人でブースへ向かう。\n\n### 用意すべき書類\n\nニューヨーク州でDMV発行のIDを取るには、1) 誕生日を証明する書類、2) アメリカに滞在する許可を持っていることを証明する書類、および 3) ニューヨーク州に住んでいることを証明する書類が必要で、1はパスポート、2はI-94、ビザ、グリーンカードやEAD、3はbank statementかutility billが定番なんだが、妻には3がない。DMVのサイトにはしかし、「Parent/Spouse ID (in the same last name or with proof of relationship) (if issued within 1 year)」という項目があって、夫たる僕がIDを出して、同姓の妻がパスポートと（グリーンカード取得作業中に無駄に取った）日本領事館発行の婚姻証明を提示すればよいはずだったので、一緒にブースにいった。\n\nところがDMVの職員は、「これじゃ彼女の証明にならない」。\n\n「DMVのサイトで妻が一緒に住んでいる同姓の人であることが証明になるって書いてあった」\n「彼女の居住証明がないとだめ。bank statementかutility billか」\n「いやWebサイトに書いてあった」\n「ここではダメ」\n\nしばらく粘ったが取りつく島はまったくなく、結局妻はテストを受けられなかった。お役所関連のことでWebに書いてある内容なんか信じてはいけない。\n\n妻は待合所へ戻り、僕はテストを受ける端末番号を知らされて、テストを受ける。15分くらい。テストを始めると、最初に誕生日や名前を確認する画面が出てくる。その後テストの練習として、画面に円が書かれていて選択肢として「Triangle」や「Rectangle」が並んでいる中から「Circle」を選ぶという問題が出てくるので、それに答えると、次に本番のテストが始まる。\n\n見たことのある問題がほとんどだったが見たことのない問題も4問くらいあった。テストは英語で受けた。証明書を出すときにどの言語で受けるか聞かれるのだが、妻が「Japanese」と言ったら「日本語なんてあったっけ？」という始末。日本人の方は、Lower ManhattanのDMVはやめた方がいいかもしれない。\n\n### テスト、視力検査、仮免発行\n\nテストは、2問間違えたが無事合格。画面に「pass」と出るので、テストを終えて、すぐ横にある20人ほど並んでいる行列にまた並ぶ。この行列の目的は、最終的な証明の確認、視力検査、およびお支払い。この行列もまた遅々として進まず、1時間くらい並んだ気がする。道理でテスト端末は空いているのにTの番号は全然呼ばれないわけだ。端末が空くたびに人を入れていたら、この行列も100人を超えてしまうだろう。\n\nようやく順番がまわってきて、もう一度パスポート、SSNカード、bank statementなどを見せる。視力検査はおなじみのランドルト環の表のようなもの、でもランドルト環ではなく大文字アルファベットが書いてある紙がブースの少し奥に天井からぶら下がっていて、それの上から4番目？くらいを読み上げる方式。問題なく見えた。その場で印刷した紙が渡され、それが写真付きのカードが届くまでのlearners permitとなる。これを使う予定はないけども。\n\nこうしてすべてが終わり、出口に向かったのは午後1時。3時間コースだった。この時点でオフィスへの入館規制がかかっていて、オフィスの外の歩道に列が伸びていた。うっかりアポを午後にとっていたりしたらとんでもない目にあったところだった。\n\n写真付きのカードは1月10日に無事に郵送されてきた。これで当初の目的の1つだった「財布に入れておけるIDが欲しい」という目的は達成。有効期限は5年後の誕生日まで。さて、実際の運転免許証を取りに行くかどうか。どうしようかなあ。\n\n### DMVに行く前にこれを読んでいる方のために\n\nDMVのWebサイトはあてにならないが、できる準備はなんでもしていこうという観点から、アポをとって確認メールは印刷していった方がいいと思う。アポは1日の初めの枠、8時とか9時とかにしておいて、その時間につくように行けば、たとえアポが無意味でも並ぶ時間は短くなるはず。ただしDMVオフィスによってはWalk inでの筆記テスト受験はできないとWebに書いてある場所もあるので、その場合アポは必須なんだと思う。Webは信用しないと言いつつ、自分に不利になりかねない情報は受け止めて対策しておく必要がある。\n\nSSNとパスポート、ビザやグリーンカードはあるとして、自分名義現住所のbank statementとutility billを印刷して持って行くこと。DMVのWebサイトに色々書いてある、この書類は何ポイントこれは何ポイント、そして全部で何ポイント必要、とかいう内容は信用しない。bank statementかutility bill。これしか聞かれなかった。他の書類はまったく取り合ってもらえないと思った方がいい。H4では働けないなどの理由でSSNもなく、奥様の居住を証明することが難しい場合、SSNなしでも作れる場所で銀行口座を作っておくとか、Utilityの契約時に奥様の名前にするとかしておくと何かと便利だと思う。\n\nあとは、半日無駄に過ごすつもりで、時間が無為に過ぎることを容認できるくらい心に余裕がある状態のときに行きましょう。DMVで3時間なら「早めに終わったね」くらいの感じです。6時間（うち5時間45分は何もしない時間）とかを許容するつもりで行きましょう。\n\n[5時間講習の受講](/blog/2021-06-13_five-hour-course-drivers-license-ny/)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-12-29_-------------------e3f6436b0b67","date":"Sun, 29 Dec 2019 16:59:24 GMT","title":"ニューヨーク市で仮免を取ろうと思った","epoch":"1577638764","excerpt":"ニューヨーク市で暮らし始めて6年。重い腰をあげて受験しようと思いました。"},"markdownBody":"\n### ニューヨークで仮免を取ろうと思った\n\nニューヨーク市で暮らし始めて6年。日常生活で運転を必要とすることがないので、めんどくささから運転免許証を取るための行動をこれまでまったく起こさなかった。\n\nめんどくさいなあと思っていた理由は二つあって、一つはDMVに関する悪評。DMVイコールめんどくさい役所という話をあらゆる場面で聞かされていたので、とにかく行きたくなかった。ニューヨーク市に住み始めて最初の4年を、DMVオフィスから2ブロックの場所で過ごしたにも関わらず。もう一つは、平日に休みを取ってまで行きたくなかったこと。DMVの悪評とも相まって、無駄な時間を過ごすことが確定している場所へわざわざ行きたくなかった。\n\nところで、ニューヨーク州はでかい。ニューヨークといえばマンハッタンやブルックリンのごく一部を思い浮かべてしまうが、ニューヨーク州にはナイアガラの滝もあるし、スキー場やハイキングスポットや紅葉の季節のドライブスポットなんかもたくさんある。もちろん海岸もあればワイナリーだってある。ニューヨークに住んでいて、週末にふらっとこれらの場所に行かないのはなんとももったいない。\n\nだがしかし、これらの場所にふらっと「公共の交通機関で」行けるわけではない（行ける場所もある）。もちろんナイアガラの滝なんかはニューヨーク市内からだと飛行機に乗る距離だし、スキー場なんかは自家用車がないと厳しい。ハイキングなら電車で行き帰りできる場所もなくはないが、限られる。バスツアーもあるけど、行きたい場所すべてにバスツアーが催行されているわけではない。要するに、運転できるとのできないのとでは行動範囲が全然違うという当たり前のことを実感し始めている。\n\n運転免許証があるともう1つ便利なことは、とにかくこの街であらゆる場面で求められる「ID」つまり身分証明書になること。生年月日、顔写真、そしてニューヨークに住所を持っていることを証明してくれるので、美術館にいけば言い値で入れるし、バーで酒を飲めるし、FedExで書類を発送できるのである。パスポートで代替できることも多いけども、財布には入らない。したがって持っていくのを忘れる。会社のパーティでバーに行き、IDを持っていくのを忘れて、同僚にビールを買ってきてもらうことも一度や二度ではない。\n\nさて、2019年も終わりに近づき、会社は仕事を納めて1月2日まで暇である。例年は帰省していたのだが、今年はビザなどの関係で日本へはいけないので、そのままニューヨーク市にとどまっている。暇である。しかしここはアメリカ合衆国。クリスマス休暇が終わってから年末年始はそこまでお休みじゃない。DMVもお役所なので、12月30日でも開いているのである。平日である。自分は休みである。暇である。DMVで無駄な時間を過ごすチャンスである。\n\n一念発起して運転免許証を取りにDMVへいくことにした。\n\n[ニューヨークで仮免を取った](/blog/2020-01-11_--------------45d1865c8b3a)に続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-07-04_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------Android----------------a4747b4edca5","date":"Thu, 04 Jul 2019 20:45:26 GMT","title":"Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る","epoch":"1562273126","excerpt":"Tensorflowで作ったモデルをTensorflow Liteのモデルに変換するまでの苦労話"},"markdownBody":"\n### Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る\n\n### TensorflowのモデルをTensorflow Liteのモデルに変換したい\n\n[前回なんとなく雰囲気で作ったTensorflowのモデル](/blog/2019-06-15_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------1ae6347c67fc)をAndroidアプリで利用するには、まずTensorflowで訓練したモデルをTensorflow Liteで読めるものに変換しなければならない。[ドキュメント](https://www.tensorflow.org/lite/convert)にはなんだかサラッと[コマンドで変換できそうなことが書いてある](https://www.tensorflow.org/lite/convert/cmdline_examples)のでやってみる。\n\n入力としてモデルを渡すのでまずモデルをシリアライズしてファイルにする。[DNNClassifierにはexport_saved_modelっていうメソッドがある](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#export_saved_model)のでこれを呼べばいいんだろうが、こいつに渡す引数がよくわからない。DNNClassifier savedmodelあたりでググり倒してようやく[それっぽいコードを見つけた](https://stackoverflow.com/a/55737532)のだが、これは直接Tensorflow Liteのクラスとメソッドを使ってインメモリのオブジェクトを変換する方法で、いやまあ別に動けばいいんだけども、渡しているものも何が何だかわからん。加えて、モデルを作る過程では一切出てこないセッションだのグラフだのというオブジェクトを「そこにある」前提で扱うコードになっている。`dnn/input_from_feature_columns/input_layer/concat:0`とか`dnn/logits/BiasAdd:0`とか一体どこから出てきたのか。\n\n### Tensorflow Liteで予測してみる\n\nとりあえず上記のコードを[前回のスクリプト](https://gist.github.com/fumiakiy/c1f8fe23b36b0a8984a12cd2bb54cd0e)へコピペして、classifierのtrainをした後で呼び出してみると、確かにconverted_model.tfliteファイルができあがるので、ひとまずこれをTensorflow Liteで推測に使ってみることにした。Tensorflow LiteのドキュメントをみるとPythonでも使えるぽいので、ひとまずスクリプトを書いてみる。\n\nTensorflow LiteのドキュメントではInterpreterオブジェクトを作ってrunメソッドを呼び出せば結果がoutput引数に返されるぽいことが書いてあるのだが、PythonのInterpreterオブジェクトにはrunメソッドがない。[ググって見つけたこのコード](https://stackoverflow.com/a/51093144)にしたがって、[input_dataだけ自前の配列に変えたスクリプト](https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751)を書いて実行してみる。\n\n```\n$ python tflite1.py\n\nValueError: Cannot set tensor: Dimension mismatch\n```\n\nはて。次元が合わないというので、配列の配列にしてみる。\n\n```\n# input_data = e\ninput_data = [e]\n…\n$ python tflite1.py\n…\nValueError: Cannot set tensor: Dimension mismatch\n```\n\n同じエラー。配列の要素を一つだけ渡してみる。\n\n```\n# input_data = [e]\ninput_data = e[0]\n```\n\n```\n…\n$ python tflite1.py\n…\nValueError: Cannot set tensor: Dimension mismatch\n```\n\nまたまた同じエラー。何を渡せばいいんだかわからないので、とりあえず第一引数で使っている`input_details`の中身をダンプしてみることに。\n\n```\n[\n{'index': 0, 'shape': array([1], dtype=int32), 'quantization': (0.0, 0L), ‘name’: ‘Const’, 'dtype': &lt;type 'numpy.int64'&gt;},\n{'index': 1, 'shape': array([1], dtype=int32), 'quantization': (0.0, 0L), ‘name’: ‘Const_1’, 'dtype': &lt;type 'numpy.int64'&gt;}, …\n```\n\nつまりinput_details[0][‘index’]は“0”であると。このtensorはint64型のデータで、shapeは要素数1の配列であるということ…なのかな? ということは、これなら通るのか?\n\n```\n# input_data = e[0]\ninput_data = [e[0]]\n```\n\n```\n…\n$ python tflite1.py\n[[1. 0. 0. 0. 0. 0. 0. 0.]]\n```\n\nなんか出てきた。要素数8の配列なので、おそらくそれぞれの数値がLABELつまりモデルを作った文字データ(**a, b, c, d, e, h, o, y**)に対応していて、それを示す値が0/1でかえってきた? つまりこのデータから予想される文字は「a」ってこと?\n\nしかし**e[0]**の値しか渡していないのだからこれが正しいわけがないので、input_dataを正しい形にすべく、こんなコードにして、51件の数値を全部渡してみることにした。\n\n```\n# input_data = [e[0]]\nfor i in (range(len(e) — 1)):\n    interpreter.set_tensor(input_details[i]['index'], [e[i]])\n```\n\n```\n…\n$ python tflite1.py\n[[0.1134394 0.09876031 0.1299585 0.1381347 0.07306363 0.17048864\n 0.09710578 0.17904899]]\n```\n\nなんかそれっぽい値がかえってきた。これがそれぞれの文字かもしれない可能性を表す数値なんだろうか。eではなくyのデータを与えてみると、\n\n```\n# for i in (range(len(e) — 1)):\n#     interpreter.set_tensor(input_details[i]['index'], [e[i]])\nfor i in (range(len(y) — 1)):\ninterpreter.set_tensor(input_details[i]['index'], [y[i]])\n```\n\n```\n…\n$ python tflite1.py\n[[0. 0. 1. 0. 0. 0. 0. 0.]]\n```\n\nつまり100%「c」って予想ってこと? ふーむ。output_detailsの方をダンプしてみると、こうなっていて、そういう解釈で良さそうな気がする。\n\n```\n[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: &lt;type ‘numpy.float32’&gt;}]\n```\n\nここまで試行錯誤を重ねて、あとはtrainingのstep数やhidden_unitsの中身やらをあれこれいじってモデルを作り直して、また変換して[tflite1.py](https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751)を実行してみて、というのを繰り返してみたが、なんとも今一つの結果しか得られない。らちが開かないので、Tensorflow Liteへの変換過程を変えて、Saved Modelとやらにエクスポートすればもう少しそのファイルに何か書いてあるんじゃなかろうかと、DNNClassifierのexport_saved_modelを呼ぶ方法を探すことにした。\n\n### TensorflowのDNNClassifierをSavedModelとして出力する\n\nもう一度「DNNClassifier “saved model”」あたりでググっていくつかそれっぽいサンプルを見ていてようやく[この記事](http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/)の中にコピペできそうなコードを見つけた。早速ちょいちょい書き換えて実行してみる。\n\n```\ndef export_tflite2(classifier, data):\n   feature_columns = []\n   for i in range(len(data)):\n        feature_columns.append(\n          tf.feature_column.numeric_column(key=str(i))\n        )\n```\n\n```\n    feature_spec = tf.feature_column.make_parse_example_spec(\n                     feature_columns\n                   )\n```\n\n```\n    # Build receiver function, and export.\n    serving_input_receiver_fn = tf.estimator.export.\n        build_parsing_serving_input_receiver_fn(feature_spec)\n    export_dir = classifier.export_savedmodel(\n                   'export', serving_input_receiver_fn\n                 )\n    print(export_dir)\n```\n\nなんと「export/1562177753」にそれらしきファイルができた。ファイルはバイナリーでそのままでは読めなかったので、これの中身を調べる方法を探すと、**saved_model_cli**というコマンドがある。実行すると、中に見慣れた文字列が。\n\n```\n$ saved_model_cli show --dir export/1562177753 --all\n…\n outputs['logits'] tensor_info:\n dtype: DT_FLOAT\n shape: (-1, 8)\n name: dnn/logits/BiasAdd:0\n…\n```\n\nこれは良いものなのでは? 早速[tflite_convertコマンド](https://www.tensorflow.org/lite/convert/cmdline_examples)にかけて、Tensorflow Liteのモデルに変換してみる。\n\n```\n$ tflite_convert --output_file=./model1.tflite --saved_model_dir=export/1562177753\n…\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, EXPAND_DIMS, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, TILE. Here is a list of operators for which you will need custom implementations: AsString, ParseExample.\n```\n\nTensorflow Liteのランタイムには存在しないオペレーター(ここではAsStringとParseExample)を使っているので、変換できませんと。使っているのは誰なのかもよくわからんので、とにかくググる。[ParseExampleに関しては、このSOの答え](https://stackoverflow.com/a/55693825)が見つかった。export_saved_modelするときのやり方を少し変えればいいっぽい。やってみる。\n\n```\n# feature_columns = []\n# for i in range(len(data)):\n#     feature_columns.append(tf.feature_column.numeric_column(key=str(i)))\n```\n\n```\n# feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n```\n\n```\n# Build receiver function, and export.\n # serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n```\n\n```\nfeatures = {}\nfor i in range(len(data)):\n    key = str(i)\n    features[key] = tf.convert_to_tensor(np.array(data[i]))\n```\n\n```\nserving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)\n export_dir = classifier.export_savedmodel('export', serving_input_receiver_fn)\n```\n\n無事export/1562179024にファイルができたので、先ほどのsaved_model_cliコマンドで中身をみてみるとだいぶ内容が変わっていた。まあ気にせずtflite_convertを再度実行してみる。\n\n```\n$ tflite_convert — output_file=./model1.tflite — saved_model_dir=export/1562179024\n…\nValueError: No ‘serving_default’ in the SavedModel’s SignatureDefs. Possible values are ‘predict’.\n```\n\nさっきのSOの答えに書いてあったのはこれか、ということでオプションを足して再度実行。SOに書いてあるのとはオプションの名前が違った(signature_def_keyではなくsaved_model_signature_key)。\n\n```\n$ tflite_convert --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024\n…\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, CAST, CONCATENATION, FULLY_CONNECTED, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AsString.\n```\n\nParseExampleは使わなくなったけど、AsStringはまだエラーのまま。 — allow_custom_opsをつけて実行すればmodel1.tfliteはできるけども、これを先ほどのTensorflow Liteのコードに読み込むと以下のエラーで結局使えない。\n\n```\nValueError: Didn’t find custom op for name ‘AsString’ with version 1\nRegistration failed.\n```\n\nAsStringなんて簡単に実装できるんじゃないのかと思って[custom operatorを自分で実装する方法のドキュメント](https://www.tensorflow.org/lite/guide/ops_custom)を読んでみたけども、C++で書いてTensorflow全体をビルドしなおすってこと? よくわからんが手に負えなさそうなのでやめて、AsStringを使わないような変換を行う方法を探してみる。\n\nもう一度saved_model_cliを実行してみると、中にDT_STRING型の値を出力するtensorがあるのを見つけた。\n\n```\noutputs[‘classes’] tensor_info:\n dtype: DT_STRING\n shape: (-1, 1)\n name: dnn/head/predictions/str_classes:0\n```\n\nカテゴリー分けしたときのカテゴリー名(ラベル)を持っているのだろうか、よくわからないが使わないのでこれをモデルに含まないようにすればいいんじゃないかと。\n\ntflite_convertコマンドにはoutput_arrayというオプションがあって、これにtensorの名前を指定できるらしい。とりあえず今出力して欲しいのは予測結果だけなので、同じsaved_model_cliの出力にあった「dnn/head/predictions/probabilities:0」だけが出てくればいいやということで、output_arrayに指定してみる。\n\n```\n$ tflite_convert — output_file=./model1.tflite — saved_model_signature_key=”predict” — saved_model_dir=export/1562179024 — output_array=dnn/head/predictions/probabilities:0\n…\nValueError: Invalid tensors ‘dnn/head/predictions/probabilities:0’ were found.\n```\n\nエラー。いや待てよ、さっきダンプしたoutput_detailsによると、このtensorの名前は「dnn/head/predictions/probabilities」みたい。\n\n```\n[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: &lt;type ‘numpy.float32’&gt;}]\n```\n\nというわけで「:0」を除いてみる。\n\n```\n$ tflite_conver --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024 --output_array=dnn/head/predictions/probabilities\n```\n\nできた。おお、できたよ。再度[Tensorflow Liteで予測するスクリプト](https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751)を実行してみると実行自体はできた。出力は相変わらず意味がよくわからないけど。\n\n```\n$ python tflite1.py\n```\n\n```\n[[0.11178369 0.10516622 0.11024905 0.13274346 0.10095505 0.15692514\n 0.09929805 0.18287939]]\n```\n\n### Tensorflow Liteのモデルに入力する値\n\n同じモデルに色々な文字のデータを入れて試してみても、何か腹落ちするデータが得られないことがしばらく続いて、そろそろやる気もなくなって来たころ。\n\n何か変だなと思っていたのは、LiteではないTensorflowのclassifierを使ったテストではそれなりに当たりの予測を出すことが多いのに、なぜかLiteになると全然当たらないということ。渡している入力値がおかしいんだろうか。\n\nもう一度saved_model_cliの出力を見直すと、saved modelを使っていなかったときのコード(Tensorflow Liteのクラスとメソッドを使ってclassifierオブジェクトを直接変換したコード)では入力tensorとして「dnn/input_from_feature_columns/input_layer/concat」を使っていて、それに対して現状のsaved_model_cliの出力の中にはそういう入力tensorは存在せず、代わりに「Const_1」「Const_2」というtensorが全部で51個あるのがわかった。**51個**。これはこっちが渡そうとしている1つ1つのデータを入れる場所に違いないので、ループでset_tensorしている今のコードで大丈夫のはず…なんだけど、[input_details[i][‘index’]と、配列の添字ではなくあえてオブジェクトのindexキーを使ってデータをセットしている](https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751)のはなんでなんだろう、と思いついて、input_detailsの中身をダンプしてみると、\n\n```\n[\n{‘index’: 0, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const’, ‘dtype’: &lt;type ‘numpy.int64’&gt;},\n{‘index’: 2, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_10’, ‘dtype’: &lt;type ‘numpy.int64’&gt;},\n{‘index’: 3, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_11’, ‘dtype’: &lt;type ‘numpy.int64’&gt;}, …\n```\n\n0番の要素のindexは0だけど、1番の要素のindexは2になっているし、名前もConst_10でConst_1ではない! ということは、テストデータの1番要素の値をinput_details[1][‘index’]に入れてしまうと、1番のtensorにセットすべき値を2番のtensor(本来10番要素の値を入れる場所)にセットしていることになってしまうのでは?\n\n配列の要素とtensorの並びを正規化して、それから正しいtensorに値をset_tensorするようにコードを書き直してみた。\n\n```\n# for i in (range(len(y) — 1)):\n#     interpreter.set_tensor(input_details[i]['index'], [y[i]])\n```\n\n```\nindices = [0] * 51\nfor detail in input_details:\n    vindex = 0\n    m = re.match(r'Const_(\\d+)', detail['name'])\n    if (m is None):\n        vindex = 0\n    else:\n        vindex = int(m.group(1))\n    indices[vindex] = detail['index']\n```\n\n```\nfor i in range(len(y)-1):\n    interpreter.set_tensor(indices[i], [y[i]])\n```\n\n実行してみる。\n\n```\n$ python tflite1.py\n```\n\n```\n[[0. 0. 0. 0. 0. 0. 0. 1.]]\n```\n\nおお、yをyと予想したっぽい! yをeにして再度実行してみる。\n\n```\n# for i in range(len(y)-1):\n#     interpreter.set_tensor(indices[i], [y[i]])\nfor i in range(len(e)-1):\n    interpreter.set_tensor(indices[i], [e[i]])\n```\n\n```\n$ python tflite1.py\n```\n\n```\n[[0. 0. 0. 0. 1. 0. 0. 0.]]\n```\n\neも正しく予測している! このTensorflow Liteのモデルは良いものなのでは? Androidアプリで実行してみよう!\n\nというわけで次回へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-06-15_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------1ae6347c67fc","date":"Sat, 15 Jun 2019 15:44:21 GMT","title":"Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖を手書き認識デバイスにする","epoch":"1560613461","excerpt":"ジャケット（についてるsnap tag）からJacquardの糸に触れたときの連続的なデータを使って遊んでみる。"},"markdownBody":"\n### Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖を手書き認識デバイスにする\n\n[前回まで](/blog/2019-06-01_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google------------8b56fb4732d7)のコードで、ジャケット（についてるsnap tag）から飛んでくる情報、特にJacquardの糸に触れたときのなんというか連続的なデータをアプリで取得できるようになった。今のところこの情報は公式アプリでもデモ目的以外には使われていなくて、Double TapやBrush In/Outなどのイベントはsnap tag側で先に判別された上で、そのIDがある種デジタルな、離散的な値として飛んできて、公式アプリはそれに対して反応することができるだけになっている。\n\n連続的なデータを使って何かできないかということで思いついたのが、袖をなぞって発生する一連のデータを文字として認識させて、キーボードのように使えるようにすること。それを実現するために、何度もある文字をなぞったデータを機械学習にかけて、15本の糸のなぞられ方から文字を判別するモデルを作成して、あわよくばそれをアプリ上で実行してキーボードのように使う。\n\n機械学習とかやったことないしPythonもほとんど書いたことないのだが、まあStackoverflowでなんとかなるだろうと思って始めてみた。\n\n### 学習用データの収集\n\n学習させたいデータがないと学習させられないので、まずはデータを作る。最初は糸1本ごとの触られっぷりを示すらしき1バイトのデータをそれぞれカラムとして保存して、さらに1つの文字を書くのにある程度の数のデータが連続して送られてくるので、1文字の書き始めと書き終わりまでの系列データを並べた2次元の配列を考えた。一方の軸の大きさは15で固定で、もう一方の軸の大きさは文字の書き終わりまでに送られていたイベント数ということになる。\n\nこのアプリ上で、教師あり学習用のデータを作るためにa, b, cの文字ををそれぞれ10回程度ずつなぞって、書き終わるたびに1つの文字のデータとして記録して、ログを吐き出した。したがって吐き出されたデータはこんなのが並んでるファイルとして保存される。\n\n```\n0,0,c,d,0,1,2,3,4,5,0,0,0,0\n0,0,0,0,4,3,2,1,a,a,a,a,0,0\n0,0,0,0,c,d,0,1,2,3,4,5,0,0\n0,0,0,0,0,0,4,3,2,1,a,a,a,0\n…\n```\n\nさて少量ながらデータが集まったので、解析してみる。ググってStackoverflowして行くと、やろうとしていることはいわゆるカテゴリー（クラス）分けで、「教師あり学習で分類を行う」ということになるらしい（反対語? は「教師なし学習」「回帰」らしい）。”supervised classification algorithm” とか、さらに”small dataset”で検索してみると、support vector machine (SVM) またはNaive Bayesというアルゴリズムがあること、これらのアルゴリズムをお手軽に使えるツールとしてscikit-learnというのが使えることがわかった。scikit-learnがPythonで使えるモノであるということは知ってるし、Naive BayesアルゴリズムはCPANモジュールもあって前職でお遊びながら使ったことがあった、という程度の知識でスタート。\n\n### scikit-learnで使える学習データを作る\n\nscikit-learnとNaive Bayesで検索してみるといくつか入門記事が見つかるのだが、そのほとんどが[irisつまりアヤメのデータからそのアヤメの分類を当てるという例](https://www.ritchieng.com/machine-learning-iris-dataset/)が出てくるので、とりあえずそれをやってみる。写経して動かして、ははあなるほど、と。\n\n次にirisのデータを自分のデータに置き換える方法を探ってみる。irisのデータは1つの種ごとに4つのカラム（petalのwidthとlength, sepalのwidthとlength）があって、それらが3種類のアヤメに分けられるらしい。それをこんな風にデータ化している。\n\n```\n[\n [ 5.1 3.5 1.4 0.2]\n [ 4.9 3. 1.4 0.2]\n [ 4.7 3.2 1.3 0.2]\n [ 4.6 3.1 1.5 0.2]\n [ 5. 3.6 1.4 0.2]\n …\n]\n```\n\nこちらが使いたいデータはさっき書いたように1つの文字に対して2次元の配列になっている。こういう風に単純に置き換えて渡してみてもエラーになる。\n\n```\n[\n [\n [0,0,12,13,0,1,2,3,4,5,0,0,0,0],\n [0,0,0,0,4,3,2,1,10,10,10,10,0,0],\n [0,0,0,0,12,13,0,1,2,3,4,5,0,0],\n [0,0,0,0,0,0,4,3,2,1,10,10,10,0],\n …\n ],\n …\n]\n```\n\n```\nValueError: Found array with dim 3. Expected &lt;= 2\n```\n\n次元が多すぎるってことで、次元を減らすために適当にググって見つけたこんなコードで適当に次元を減らす。\n\n```\nX = X.reshape(X.shape[0], -1)\n```\n\n一応走りはするものの、こんな次元の減らし方では特徴を正しく捉えたデータになっているとは思えない。\n\nつまり、糸1本1本を別のカラム（次元）として扱うとこれら連続データをただしく渡せないことがわかったので、データの取り方を変えて、1イベントにつき1列で15本の糸のデータを表す値を作ることにした。具体的にはこんな感じで0本目の糸のデータには2**0を、1本目の糸のデータには2**1をそれぞれかけて…とやって1つの大きな数値を作ることにした。これで意味が失われていないのかすごく疑問だが、まあドンマイ。\n\n```\nvar data = 0\nbytes.forEach { byte -&gt;\n val digit = Math.pow(2.toDouble(), (bytes.size — 1).toDouble())\n data += Math.ceil(digit * byte).toInt()\n}\n```\n\nこうして、1つの文字に対してこんな数値が並んだデータを再度a, b, cごとに10ずつくらい作り直して、Python再開。\n\n```\n5931008,\n5308416,\n5013504,\n3473408,\n…\n```\n\n### scikit-learnでモデルを作る\n\nさっきと同じように、irisのデータを作っている箇所を自分のデータ用に変形してみる。当然ながら、それぞれの文字のそれぞれの学習データの大きさは同じでないといけないので、適当にファイルをいくつか眺めて、1つの文字のデータ行数を51にして、連続する同じデータはすべて削ってしまうことにした。これもデータの意味を消してしまっている気がするがドンマイ。\n\nPerlスクリプトをちょいちょい書いてデータを揃えたら、そのファイルをPythonで読み取って、データを食わせてテストしてみる。データ数が少なすぎるし信頼性は低いけど、とりあえずできた。\n\n```\nimport os\nimport re\n```\n\n```\nimport numpy as np\n# from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\n```\n\n```\nLABELS = [‘a’, ‘b’, ‘c’]\n```\n\n```\n# read all data and make an array of array of data\npath = ‘./normalized/’\ndata = {‘a’: [], ‘b’: [], ‘c’: []}\nfor f in os.listdir(path):\n datum = []\n m = re.match(r’(\\w)_(\\d)\\.txt’, f)\n if (m is None):\n continue\n char = m.group(1)\n with open(os.path.join(path + f), ‘r’) as lines:\n for line in lines:\n datum.append(int(line.rstrip()))\n data[char].append(datum)\n```\n\n```\n# construct the ml stuff\n```\n\n```\ntarget = []\nsamples = []\nfor char, ar in data.items():\n for datum in ar:\n target.append(LABELS.index(char))\n samples.append(datum)\n```\n\n```\n# clf = SVC()\nclf = GaussianNB()\nX = np.array(samples)\ny = np.array(target)\nX_train, X_test, y_train, y_test = train_test_split(samples, target, test_size=0.1, random_state=0)\n```\n\n```\nclf.fit(X_train, y_train)\nprint clf.score(X_train, y_train)\n```\n\n```\nprint clf.predict(X_test)\nprint y_test\n```\n\n```\n$ python t1.py\n0.7941176470588235\n[2 0 2 0]\n[2, 1, 2, 2]\n```\n\n### Tensorflowで同じことをやってみる\n\n目的は作ったモデルをAndroidアプリにデプロイして、入力されたデータから文字を推測することなので、scikit-learnで作ったモデルだとやりづらそう。\n\nTensorflowでモデルを作ればTensorflow LiteでAndroidアプリ上で利用できることは知っていたので、scikit-learnで作ったモデルをTensorflowのモデルに変換できるのかを調べたが、まあよくわからないけど簡単にはできませんよね。Tensorflowはニューラルネットワークのためのもので、SVCとかGaussianNBとかはそうじゃないとかなんとかよくわからん。\n\nそれじゃあTensorflowで似たような感じで分類を行うことはできるのかと思って、”Tensorflow iris”とかでググったら[TensorflowにもDNNClassifierというものがあってアヤメの分類ができる](https://www.tensorflow.org/guide/premade_estimators)らしい。早速それを行なっているコードをサンプルから探して、実行してみて、自分のデータで置き換えてみるというさっきと同じ作業を今度はTensorflowのコードでやってみる。まあ[できたのはできた](https://gist.github.com/fumiakiy/cc9107e2f4b66dbc0a20ee1eabc04dc1)。全然当たってないけど。\n\n```\n$ python tf.py\n```\n\n```\nTest set accuracy: 0.788\n```\n\n```\nPrediction is “2” (100.0%), expected “2”\n```\n\n```\nPrediction is “2” (66.6%), expected “0”\n```\n\n```\nPrediction is “2” (66.6%), expected “1”\n```\n\n### 文字を増やす\n\nここまでやっている間ずっと心配に思っていたのは、aと**d**やbと**h**やcと**e**を本当にこんなデータで区別できるのかということ。そこで、d, e, h, o, yの5つの文字をさらに10ずつくらいログにとって、同じPerlスクリプトで整形して、[scikit-learnのコード](https://gist.github.com/fumiakiy/b8018b441d51639d3690ad003bebce0a)にかけてみた。\n\n```\n$ python sk2.py\n0.49504950495049505\n[6 1 0 1 6 0 6 5 0 1 6 4]\n[6, 0, 2, 0, 6, 3, 3, 7, 2, 0, 5, 4]\n```\n\nそこそこ? [Tensorflowでやってみる](https://gist.github.com/fumiakiy/c1f8fe23b36b0a8984a12cd2bb54cd0e)とどうだろう。\n\n```\n$ python tf2.py\n```\n\n```\nTest set accuracy: 0.195\n```\n\n```\nPrediction is “4” (15.8%), expected “2”\n```\n\n```\nPrediction is “4” (15.8%), expected “0”\n```\n\n```\nPrediction is “4” (15.8%), expected “1”\n```\n\nガーン。ものの見事に全然区別できてない。ニューラルネットワーク様にはもっとたくさんデータを食わせないとダメなのかあとか思いながら、アルゴリズムを変えてみたりあーだこーだしていてふと気づいた。DNNClassifierに渡している「hidden_units」ってなんだ。\n\nニューラルネットワークでディープラーニングってのは、[それっぽい絵](https://www.mapleprimes.com/maplesoftblog/209354-A-Beginners-Guide-To-Using-The-DNN)を見てみると、入力から出力の間に通る場所をいくつも作っていくものらしい。どこかのサンプルから写経した[10, 10]を渡しているけど、これはニューラルネットワークのinputとoutputの間にあるよくわからん神経ノードの数と層を表しているのだから、ここを増やせば精度が上がるんじゃないか? データも少ないし、適当に[20, 40, 20]などと渡してみた（前掲リンク先のを真似した）。\n\n```\n$ python tf2.py\nTest set accuracy: 0.929\n```\n\n```\nPrediction is “2” (100.0%), expected “2”\n```\n\n```\nPrediction is “0” (100.0%), expected “0”\n```\n\n```\nPrediction is “1” (48.4%), expected “1”\n```\n\n…精度が急上昇しました。\n\nちょっと本当かなと思って、[20, 40, 80, 40, 20]とかも渡してみたら精度が1.000のオール100%なんてことになったこともあったんだけど、こんな程度のデータ量だと(?)試行するたびに全然違う数値が出てくるので、一喜一憂しても仕方ない。このままデータを増やしていっても、aとdをより精度高く判別できるようになるとは正直思えないし。\n\nだが、まあとりあえず小文字のアルファベットのデータを全部作って、できたモデルをTensorflow Liteに乗せてAndroidで実行してみるモチベーションが湧くくらいの精度ではあるので、次に進もうと思う。\n\n[Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る](/blog/2019-07-04_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------Android----------------a4747b4edca5)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-06-01_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google------------8b56fb4732d7","date":"Sat, 01 Jun 2019 15:09:41 GMT","title":"Levi’s Commuter Trucker Jacket with Jacquard by Googleを操る自作アプリを作る","epoch":"1559401781","excerpt":"スマートジャケットからの情報を得て何かするAndroidアプリを作ってみる"},"markdownBody":"\n### Levi’s Commuter Trucker Jacket with Jacquard by Googleを操る自作アプリを作る\n\n### LEDを思いどおりに光らせる\n\n[前回](/blog/2019-05-12_Android----------Bluetooth------------5844e20b5b98)までで書いたとおり、公式アプリとsnap tagとの通信をWiresharkでのぞいて見て、同じ値を自作アプリから同じBluetooth serviceの同じcharacteristicに送りつけてみたら同じように光ったので、同様の手順で公式アプリがサポートしている3種類の光らせ方とその値をメモってみた。すると、一部だけ異なる値を送っていることがわかった。何度も同じ光らせ方をさせてみると、その度に変わる部分と、光らせ方が同じなら変わらない部分があることもわかった。そういうわけで、こういう単純なコードで、3種類のうちから希望の光らせ方をする値を作って送りつけることはできるようになった。\n\n```\nfun valueToWrite(command: Command): ByteArray {\n  val b = UByteArray(19)\n  \n  // It seems these two bytes varies among snap tags\n  b[0] = 0xc0U\n  b[1] = 0x11U\n  \n  // Together with these 5 bytes, the first 7 bytes could be a tag's id?\n  b[2] = 0x08U\n  b[3] = 0x00U\n  b[4] = 0x10U\n  b[5] = 0x08U\n  b[6] = 0x18U\n  \n  // The 8th byte seems like an id of the command.\n  b[7] = sequenceId\n  \n  // The next 4 bytes don't seem to change\n  b[8] = 0xdaU\n  b[9] = 0x06U\n  b[10] = 0x08U\n  b[11] = 0x08U\n  \n  // The 13th byte seems like the command\n  b[12] = command.value\n  \n  // The following 6 bytes don't seem to change\n  b[13] = 0x10U\n  b[14] = 0x78U\n  b[15] = 0x30U\n  b[16] = 0x01U\n  b[17] = 0x38U\n  b[18] = 0x01U\n  \n  return b.toByteArray()\n}\n```\n\n```\n...\ncharacteristic.setValue(valueToWrite(command))\ngatt?.writeCharacteristic(characteristic)\n```\n\n### ジェスチャーを取得する\n\n次に、これも公式アプリでサポートされている、Jacquardの糸を触ることで通知を受け取る機能を実装してみる。LEDを光らせるのは自作のアプリからsnap tagへ向けてデータをwriteすればよかったのだが、これをするにはBluetoothの通知機能を利用しないといけない。具体的には、snap tagが出しているserviceのうちどのcharactersticがこの通知を発信しているのかを探し出して、それに対して通知を有効にした上で、callbackのメソッドで値を受け取る。\n\n```\nval CLIENT_CONFIG_DESCRIPTOR_UUID = \"00002902-0000-1000-8000-00805f9b34fb\"\n```\n\n```\njacquardService?.characteristics\n  ?.forEach { characteristic -&gt;\n    characteristic.descriptors?.filter {\n      it.uuid == UUID.fromString(CLIENT_CONFIG_DESCRIPTOR_UUID)\n    }\n    ?.lastOrNull()?.let { desc -&gt;\n      desc.setValue(BluetoothGattDescriptor.ENABLE_NOTIFICATION_VALUE);\n      gatt?.writeDescriptor(desc);\n      gatt?.setCharacteristicNotification(desc.characteristic, true)\n    }\n  }\n```\n\n```\n...\n  override fun onCharacteristicChanged(gatt: BluetoothGatt?, c: BluetoothGattCharacteristic?) {\n    Timber.d(\"changed ${c?.uuid}: ${c?.value?.size} == ${c?.value?.map { b -&gt; String.format(\"%d\", b) }?.joinToString(\":\")}\")\n  }\n```\n\nとりあえず通知をしてくるcharacteristicすべてをlistenしてログを見ようとやってみたのだが、どうもそれらしき通知がこない。あるcharacteristicからの通知しかこなくて、その値は明らかに別の用途のものだった。\n\nひとしきりうなってからググると答えがすぐに見つかった。通知を受け取るには、characteristicのdescriptorに対して通知を有効にするように値を書き込む必要があるのだが、ある書き込み要求が完了するまで次の書き込みはできないらしい。つまり、descriptorWriteが完了したというcallbackのメソッドの中で次の書き込みをしないとならない。連続ですべてのcharacteristicに対して通知を設定しようとしていたので、リストの先頭のcharacteristicのものしか設定されていなかった。\n\nちょいちょいとテキトーにコードを書き直したところ、他の通知も来るようになり、ジェスチャーが発生したときの通知は「d45c2030–4270-a125-a25d-ee458c085001」のcharacteristicであること、ジェスチャーそれぞれに以下のような値が割り当てられていることがわかった。\n\n```\nenum class GESTURE(val raw: ByteArray) {\n  DOUBLE_TAP(ByteArray(1, { i -&gt; 0x1 })),\n  BRUSH_IN(ByteArray(1, { i -&gt; 0x2 })),\n  BRUSH_OUT(ByteArray(1, { i -&gt; 0x3 })),\n  COVER(ByteArray(1, { i -&gt; 0x7 })),\n  BRUSH_OUT_IN(ByteArray(1, { i -&gt; 0x8 })),\n  UNKNOWN(ByteArray(1, { i -&gt; 0x0 }))\n}\n```\n\nおもしろいことに、公式アプリでは使われていないもう1つのジェスチャー（BRUSH_OUT_IN）が存在した。たまたまBRUSH_OUTした後に惰性で少し触り続けていたら違う値が飛んできたのでわかったのだが、OUTした後INする動作を連続で行うとこのジェスチャーが認識される。公式アプリで使われていないのは、多分他のジェスチャーと混同しちゃうからだと思う。BRUSH_OUTはジェスチャーと認識されるまで少しラグがあるっぽいのもこれのせいか。\n\n### 「生」データを取得する\n\nさっきジェスチャーの通知を取得しようとしていて別の用途の通知が来たと書いたが、それがなんで別の用途だとわかったかというと、とにかくJacquardの「糸」に触れると何発も飛んでくるからだった。これは明らかに糸を触ったときのデータが来てる。つまり、決められたジェスチャーについてのデジタルなデータだけではなく、触った糸についてのある種アナログな情報も通知が来るということ。これも公式アプリにそれっぽい画面があるので、そういう情報も取れることは想像できるのだけども。\n\nこの生データ、18バイトのデータが来るのだが、先頭の1バイトは明らかにシーケンス番号で、次の2バイトは割と固定のデータで、残りの15バイトの値が触り方によってずいぶん変化する。糸を数えてみるとやっぱり15本。というわけで、3から18までのインデックスの値が糸の触り方を示すっぽい。\n\nそこで公式アプリと同じような、[こんな感じの見た目のViewを作ってみた](https://www.youtube.com/watch?v=aBNK_TGtYTUところそれっぽい動きが見えるようになった。\n\nこの値をどう使うかについてちょっとアイデアが浮かんだのでやってみたいんだけど、時間と手間が掛かりそうでうーん。とりあえずは、ここまでの調査でできたものを、コードをきちんと書き直して公開するのを先にやろうかな。とはいえこのジャケットを持っている人が少ないしコードを書く人はもっと少なそうだから公開しても誰の役にも立たなそうだけども。\n\n[Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖を手書き認識デバイスにする](/blog/2019-06-15_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------1ae6347c67fc)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-05-12_Android----------Bluetooth------------5844e20b5b98","date":"Sun, 12 May 2019 15:43:30 GMT","title":"AndroidアプリとデバイスとのBluetoothでの通信内容を解析する","epoch":"1557675810","excerpt":"Levi’s Commuter Trucker Jacket with Jacquard by Google を手に入れたの続き。"},"markdownBody":"\n### AndroidアプリとデバイスとのBluetoothでの通信内容を解析する\n\n[Levi’s Commuter Trucker Jacket with Jacquard by Google を手に入れた](/blog/2019-05-05_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google--------f37ae5a4cde5)の続き。\n\nJacquardのジャケットとsnap tagの機能を利用するAndroidアプリを作るために、まずは公式アプリでは何をどうやっているのかを調べてみた。前回の記事で触れたとおり、JacquardのサービスのUUIDといくつかのcharacteristicのUUIDはわかったので、それらに向けていつどんなデータが飛んでいるのか、特にLEDを光らせるコマンドはどうやって送られているのかを調べる。\n\n### AndroidでBluetoothログを取る\n\nまずは、公式アプリで3つのうちのどれかのアクションにLightを割り当てておく。\n\nまた、Androidの設定で[Developer Optionsの中にある「Enable Bluetooth Host Controller Interface (HCI](https://developer.android.com/studio/debug/dev-options) snoop log」をオンにして)、Bluetoothを切って再度オンにしてやる。これでBluetoothでの通信内容がログに保存される。\n\nそうしてから、Jacquardの3種類のLEDモード（白く光らせるFlash Light、赤く点滅させるSignal、7色?に回転するParty?）を順番に割り当てて、実際に動作させてみる。するとそれぞれの動作は、どれも起動してから30秒で自動終了すること、また実行中にもう一度同じことをする（Brush Inするとか）と停止できることがわかった。\n\nそれぞれの動作を行った時間をなんとなく記録して、adbでデバイスからBluetoothログを取得する。[ドキュメント](https://developer.android.com/studio/debug/dev-options)には「Captures all Bluetooth HCI packets in a file stored at “/sdcard/btsnoop_hci.log”」って書いてあるんだけども、最近のAndroidではこのファイルが作られるわけではなく、ログを見るにはbugreportするのが正解らしい。\n\n`adb bugreport hoge`とやると、hoge.zipが落ちてくるので、`unzip hoge.zip FS/data/misc/bluetooth/logs/btsnoop_hci.log`してログを手に入れた。\n\n### WiresharkでBluetoothログ解析\n\n`btsnoop_hci.log`はそのまま人間が読むためのテキストログではないのだけど、幸いなことに[Wireshark](https://www.wireshark.org/)がいい感じにやってくれるので、Wiresharkでファイルを開く。だいたいこんな時間帯にこれをやったよな、ってあたりのログを見て、それらしき通信を探す…\n\n探す…んだけど、[前回あたりをつけた](/blog/2019-05-05_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google--------f37ae5a4cde5)サービスやcharacteristicのUUIDが全然見つからなかった。あたりはハズレでした。\n\nしかたないので、それらしき時間帯で共通に発生しているらしくて、他の時間帯には発生していないと思われる、ような気がするかもしれない感じのログエントリーを探してみる。すると、snap tag上のサービス「d2f2bf0d-d165–445c-b0e1–2d6b642ec57b」、characteristic「d2f2eabb-d165–445c-b0e1–2d6b642ec57b」に対して謎の値をwriteしていることがわかった。\n\nさらに、その謎の値をwriteした後すぐに、同じサービスの同じcharacteristicから別の値をreadしていること、その30秒後にまた同じサービスの同じcharacteristicから別の値をreadしていることが判明する。しかも3種類のLEDの光らせ方それぞれを行った時間帯にそれぞれ、writeとreadの値だけが異なる通信をしている。これは怪しい。\n\n何度かLEDを光らせてはログを取ってみると、これらの値は毎回若干異なるもののある種のパターンがあり、全然変わらない部分、毎回変わる部分、光らせ方によって変わる部分があることがわかった。例えば赤点滅モードの場合の値は「c0110800100818**06**da060808**20**107830013801」だったり「c0110800100818**1a**da060808**20**107830013801」だったりするが、Partyモードの値は「c0110800100818**05**da060808**10**107830013801」だったり「c0110800100818**1b**da060808**10**107830013801」だったりする。\n\nそこで、とりあえず自作Androidアプリから同じUUIDsに対してログと同じような値を送りつけてみたらどうなるか、やってみることにした。\n\n### Bluetooth Low Energyのperipheralにコマンドを送る\n\nこんな感じのコードを書いて、BLEデバイスを見つけて、接続して、characteristicを取得して、それに値を書きこんでみる。これが動けば、`startScan`してすぐにsnap tagが光るはず…。\n\n```\nprivate val blinkInRed: ByteArray get() { ... }\n```\n\n```\nprivate val bluetoothAdapter: BluetoothAdapter? by lazy(LazyThreadSafetyMode.NONE) {\n  val bluetoothManager = getSystemService(Context.BLUETOOTH_SERVICE) as BluetoothManager\n  bluetoothManager.adapter\n}\n```\n\n```\nprivate var scanner: BluetoothLeScanner? = null\nprivate var gatt: BluetoothGatt? = null\n```\n\n```\nfun startScan() {\n  scanner = bluetoothAdapter?.bluetoothLeScanner\n```\n\n```\n  val filter = ScanFilter.Builder()\n    .setServiceUuid(ParcelUuid(SERVICE_UUID))\n    .build()\n  val option = ScanSettings.Builder()\n    .setScanMode(ScanSettings.SCAN_MODE_LOW_POWER)\n    .setCallbackType(ScanSettings.CALLBACK_TYPE_ALL_MATCHES)\n    .build()\n```\n\n```\n  // Start searching the service advertised from a device\n  // Get the result in `scanCallback` below\n  scanner?.startScan(listOf(filter), option, scanCallback)\n}\n```\n\n```\nprivate val scanCallback = object: ScanCallback() {\n  override fun onScanResult(\n    callbackType: Int,\n    result: ScanResult?)\n  {\n    result?.let {\n      // Found the service. Connect to the device\n      // Get the connection result in `gattCallback` below\n      gatt = it.device.connectGatt(context, false, gattCallback)\n    }\n  }\n}\n```\n\n```\nprivate val gattCallback = object: BluetoothGattCallback() {\n  override fun onConnectionStateChange(\n    gatt: BluetoothGatt?,\n    status: Int,\n    newState: Int)\n  {\n    if (status == BluetoothGatt.GATT_SUCCESS) {\n      when (newState) {\n        BluetoothProfile.STATE_CONNECTED -&gt; {\n          gatt?.let {\n           // Connected to the device. Lookup the service\n           // Results come in to `onServicesDiscovered` below\n           it.discoverServices()\n          }\n        }\n      }\n    }\n  }\n```\n\n```\n  override fun onServicesDiscovered(\n    gatt: BluetoothGatt,\n    status: Int)\n  {\n    when (status) {\n      BluetoothGatt.GATT_SUCCESS -&gt; {\n        gatt.getService(SERVICE_UUID)?.let { s -&gt;\n          // Found the service. Lookup the characteristic\n          s.getCharacteristic(CHARACTERISTIC_UUID)?.let { c -&gt;\n          // Set the binary value to the characteristic\n            c.setValue(blinkInRed)\n            // Write the value to the characteristic\n            // This should light up the LED on the snap tag\n            gatt?.writeCharacteristic(c)\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nまあ光りませんよね。\n\nそもそもscanに成功しなくなった。それもそのはずで、公式アプリとsnap tagを接続してしまっていたので、snap tagはadvertisementをやめているぽかった。なので、いったん公式アプリでsnap tagをforgetし、Androidの設定からBluetoothデバイス「Jacquard」もforgetする。すると、scanに成功して、connectからcharacteristicを見つけるところまで進むようになった。\n\nが、やはり光らない。`writeCharacteristic`はtrueを返してきたりするんだけど、光らない。このアプリの通信をBluetoothのログをWiresharkで見てみたけど、送っている値や何かが間違っている気配はない。\n\nここで気になったのは2点。まず、Androidが執拗に「Jacquardからparing requestが来てるけどpairする?」というポップアップが出続けること。まあこれは無視していたんだけど、もう1点、もしこのコードが動作するようなら、そこいら辺にある他人のsnap tagを光らせ放題じゃないか?\n\n### Bluetooth Low Energyにおけるconnectとpairとbond\n\n当然なんだけど、BLEにはちゃんとセキュリティ、特に飛び交うデータの暗号化に関する仕様があって、それを理解しておけって話なんだけども、簡単にいうとググってこの[connecting, pairing and bondingに関する説明](https://devzone.nordicsemi.com/f/nordic-q-a/11939/connecting-bonding-pairing-and-whitelists/45217#45217)を発見して納得した。connectしただけで見られる情報だけでは（おそらく多くの場合で）デバイスの機能を利用することはできず、それをするには鍵交換を行った上でデータを暗号化してやりとりしなければならないと。\n\nとなると、Wiresharkで見た情報は鍵が変われば変わってしまうのでは? 鍵はperipheralとclientが同じでも毎回変わるのか?\n\nどうやってそれを調べられるのかわからないので、とりあえず公式アプリを一度アンインストールして、再度インストールして、接続してログを見てみた。値は変わっていなかった。これは多分違うAndroidデバイスだと違う値になるんだろうなあ、当然snap tagが変わればまた違う値になるんだろうなあ、となると、Wiresharkで見た値そのものを公開したところで、他の人はその値を使えないだろうなあ。でも「俺専用の俺アプリ」を作る分にはまあ動くのかも。\n\n### Let there be light\n\nというところまで考えたところで、Androidのドキュメントに戻って、[BluetoothDevice#setPin(byte[] pin](https://developer.android.com/reference/android/bluetooth/BluetoothDevice.html#setPin%28byte%5B%5D%29))と[BluetoothDevice#createBond(](https://developer.android.com/reference/android/bluetooth/BluetoothDevice.html#createBond%28%29))を見つけた。pinの値は、ジャケットを持っていれば知っているはずのアレで、ジャケットにくっついている。公式アプリはこれを入力しなくても値を表示してくれるので、snap tagはこの値もadvertiseしているはずなんだけど、それを探すのは後回し。\n\nそういうわけで、`discoverServices()`を呼び出す前に`setPin`と`createBond`を呼び出してしまう。\n\n```\ngatt?.let {\n  // Connected to the device. Pair, bond\n  it.device.setPin(myJacketPin)\n  it.device.createBond()\n  // And then lookup the service\n  // Results come in to `onServicesDiscovered` below\n  it.discoverServices()\n}\n```\n\n光った！光ったよ！Wiresharkで見た値をそのまま送りつけただけですが、無事光りました。\n\n本当は`createBond`の成否をBroadcastReceiverで受け取ってから操作しないといけないのだが、いったんbondしてしまえば次からはまあ速いので、こんなコードでもちゃんと動いた。\n\n[Levi’s Commuter Trucker Jacket with Jacquard by Googleを操る自作アプリを作る](/blog/2019-06-01_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google------------8b56fb4732d7)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-05-05_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google--------f37ae5a4cde5","date":"Sun, 05 May 2019 22:57:12 GMT","title":"Levi’s Commuter Trucker Jacket with Jacquard by Google を手に入れた","epoch":"1557097032","excerpt":"苦節4年、ついにスマート繊維を組み込んだ例のアレを手に入れて、早速少しコードを書いてみました。"},"markdownBody":"\n### Levi’s Commuter Trucker Jacket with Jacquard by Google を手に入れた\n\n苦節4年、ついにスマート繊維を組み込んだ例のアレを手に入れて、早速少しコードを書いてみました。\n\n![Levi’s Commuter Trucker Jacket with Jacquard by Google](/images/1*Ox5jDQ5gm-Bmf7YJH8TBkQ.jpeg)\n\n### Levi’s Commuter Trucker Jacket with Jacquard by Googleとは\n\nあれはもう4年前。Google I/O 2015で発表されたJacquardプロジェクトで「スマート繊維」というコンセプトが発表された。\n\nGoogle I/O 2015の後にたまたま[Rebuild.fm](https://rebuild.fm)に呼んでいただいて、[ほんの少しだけこれの話をして](https://rebuild.fm/94/)いて、このときからずーっと気になっていた。もっともこのときは、ジーパンに組み込まれるとばっかり思ってて、「太ももにこれが入っててもガサガサ触っちゃう」みたいなことを言っていたけど。\n\n1年経って、Google I/0 2016でLevi’sとのコラボで作るのはパンツじゃなくてジャケットであることが明らかになって、なるほど！となった。\n\nGoogle ATAPは（少なくとも当初は）2年で市販品を作ることがプロジェクトの要件になっていることが他のmoon shot的なR&amp;Dとは違う点で、それゆえにこのジャケットも、Levi’sが2017年の春に出荷することになっていた。結局2017年の春は過ぎたものの秋になってようやく、でも本当にこのジャケットは出荷された。2019年の今でもLevi’sの（ディスコンになった）Commuterラインで唯一残っている製品として[オンラインでもお店でも買うことができる](https://www.levi.com/US/en_US/clothing/men/outerwear/levis-commuter-x-jacquard-by-google/p/286600000)。\n\n買うことができる…んだけど、$350って高すぎる！どうせちっとも使わない機能がついているからって$250余計に払うほどお財布に余裕はなかった…。\n\n### ジャケットとsnap tagを別々に中古で入手\n\n自転車通勤時のパンツを買いたいなあと思ってて、でも上記のとおりLevi’sのCommuterラインはディスコンになっていて、うーんと思ってたときにメルカリで1本見つけて以来、（スマホで）古着屋巡りを時間つぶしにやっていた。こちらNew Yorkでは[Poshmark](https://poshmark.com)というサービスがそこそこに流行っているらしく、それ以外に[Grailed](https://www.grailed.com/)とか、あともちろんeBayやMercariもある。\n\nある日、どういうわけか女性ものを検索する設定で、”Levi’s Commuter”をPoshmarkで検索してしまったときになんと、Jacquard Tracker Jacketを$50で売っている人を発見。スマホと通信するためのsnap tagはなくなっているものの、$50??\n\n実はそれ以前にMercariでsnap tagだけを$15で売っている人を見つけていて、ただ本体がなければ意味がないので放置していた。さっそくMercariを見に行って、まだ売っていることを確認。Poshmarkでジャケットを無言即買いした。ジャケットが出荷されたのを確認して、Mercariでsnap tagもお買い上げ。しめて$65+送料トータル$20くらいで、$350だったあのジャケットを手に入れた。\n\n中古といっても、ジャケットは色落ちもほとんどなくてめちゃくちゃ綺麗だし、snap tagは多分開封してない箱のままで来たし、新古品と言ってよいクオリティ。\n\nジャケットの作りも、全部のポケットにファスナーがついていたりとか、袖の内側に隠してリブがあって風が入らないようになっていたりとか、Levi’sのCommuterラインのTracker Jacketの作りはそのまま、まあこれが$50ならお買い得なのでは、という感覚。\n\nだがこれを買ったことの意味は当然、Jacquardを試すことにあるのだ。まずはさっそく、公式アプリをダウンロードして機能を確認してみる。\n\n### Jacquard公式アプリ\n\n[Jacquardの公式アプリ](https://play.google.com/store/apps/details?id=com.google.android.apps.jacquard)を起動して、ジャケットとsnap tagとアプリを接続したら、ジャケットに対する3種類の動作について対応するアクティビティを設定できる。3種類の動作とは、袖を上から下へこするBrush in、袖を下から上へこするBrush out、袖を2回叩くDouble tap。このそれぞれに、現時点でアプリが提供しているアクティビティを割り当てる。例えばナビゲーション機能（次に曲がる場所を音声で教えてくれる）、現在地をアプリに保存する、単純にsnap tagのLEDを光らせるなど。また、アプリからsnap tagへ通知を送って、snap tagを振動させることもできる。電話やテキストが来たときとか。\n\n…と、かなりできることは限られていて、まあそれも当たり前で、自転車に乗っている最中にサッと行いたいことってそんなもんでしょう、ということなんだろう。音楽を聞き始めるとか、次の曲へ進むとか、止めるとかいうこともできるんだけど、自転車乗ってるときに耳栓しないでーって思うし（けっこうな数いらっしゃいますけども）。\n\nとはいえ、ここまで何もできない（というか自分がやりたいことが揃っていない）と、そりゃー「なければ作る」しかないねと。\n\nだが、Jacquard SDKは存在しない。以上。\n\n仕方ないので、適当にあたりをつけながらtry and errorでジャケット+snap tagとアプリとの通信を解析してみることにした。\n\n### Bluetooth Low Energy (BLE)でscan\n\nまあ当然予想されることだし、スマホのBluetoothを切ったらアプリが動かないことがわかるので、snap tagはBLEのPeripheralなんだろうなと想像して、単にBLEでデバイスをscanして、deviceを見つけて、serviceを見つけて、characteristicを見つけて、という一連の流れでデバッグログを書くだけのものを作ってジャケットを探してみた。\n\n最初は全然それらしきデバイスが見つからなくて困ったんだけど、snap tagがadvertiseしてなかっただけっぽくて、ジャケットの袖に刺激を与えたら出てくるようになった。[ScanRecord#getDeviceName(](https://developer.android.com/reference/android/bluetooth/le/ScanRecord.html#getDeviceName%28%29))が”Jacquard”という文字列を返してくるのですぐわかってよかった。結果、このデバイスは「d45c2000–4270-a125-a25d-ee458c085001」というserviceを持っていることがわかった。\n\nさらにこのserviceは17のcharacteristicを持っていて、そのうち12のUUIDが上記のserviceのUUIDと関連する値を持っていた。具体的には「d45c2000」の部分が「d45c2010」「d45c2030」となっていてあとは同一。これが多分Jacquard特有のcharacteristicなんだろうなあと推察した。\n\nそこでこれらのcharacteristicからもう少し情報を取得してみる。[BluetoothGattCharacteristic.html#getProperties(](https://developer.android.com/reference/android/bluetooth/BluetoothGattCharacteristic.html#getProperties%28%29))を見ればcharacteristicがREADだけなのかWRITEもできるのかがわかって、WRITEできるcharacteristicはアプリからメッセージを送れる、つまりLEDを光らせるものかまたは振動させるものということになる。予想に反してWRITEできるものが4つ見つかったんだが、そのうち2つはREADとWRITEが両方できるので、なんとなくこの2つ「d45c2080–4270-a125-a25d-ee458c085001」と「d45c20f1–4270-a125-a25d-ee458c085001」から見てみることに。\n\nとりあえず、公式アプリから使える「七色に光らせる」機能を自作アプリから呼び出せるようにしてみようと目標を設定した。\n\n[AndroidアプリとデバイスとのBluetoothでの通信内容を解析する](/blog/2019-05-12_Android----------Bluetooth------------5844e20b5b98)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2019-02-12_---------e0304804cb7b","date":"Tue, 12 Feb 2019 15:26:17 GMT","title":"翻訳有害論有害論","epoch":"1549985177","excerpt":"プログラミング自体を学ぶのか特定のソフトウェアの使い方を学ぶのか。さらに例えばExcelの使い方みたいな意味でソフトウェアの使い方を学ぶのか、Javascriptのフレームワークやライブラリの使い方を学ぶのか。目的に応じて最短距離は変わると思うけど、どの場合でも「母語でない自然言…"},"markdownBody":"\n### 翻訳有害論有害論\n\nプログラミング自体を学ぶのか特定のソフトウェアの使い方を学ぶのか。さらに例えばExcelの使い方みたいな意味でソフトウェアの使い方を学ぶのか、Javascriptのフレームワークやライブラリの使い方を学ぶのか。目的に応じて最短距離は変わると思うけど、どの場合でも「母語でない自然言語を体系的に学ぶ」が最短距離であることはない。ただこの人が論じているのは短期的な目的を達するための最適手段ではなく、長期的な利益を見据えた投資として最適である何か（英語を学ぶこと）が明らかであるのに、短期的に利益が見込めるのが明らかであるという理由で「すでに長期的な投資をある程度まで行っていてその利息を享受し始めている人々」が、「まだその長期的な投資をしていない、するかどうか決めかねている人々」に、安易な選択肢を与えてしまうことで、（見返りを短期的には実感できないかもしれない）長期的な投資から人々を遠ざけるように仕向けるのは、「優しい」のか、という問題提起なんだろう。\n\nで、その問題設定？に「えーそれちがうでしょー」と感じるのは、長期の投資によって将来得られるであろう利益が大きいことが確定的であったとしても、その長い期間生きていく糧は得つづけなければならず、その糧が「このJavaascriptフレームワーク」を使えることで得られる場合、そのドキュメントが翻訳されていたおかげで日々の糧を得ることが可能になるのだから、それは「優しい」でしょう、って思うから、かなあ。そういう、当座は困らないっていう前提があれば、次に使うことになる他人製のライブラリは英語でしかドキュメントがないかもしれないから、当座困っていない今のうちに英語も学んでおこう、っていうことにもなるし。元の論者がいう翻訳有害論はこういう入り方を許してくれないので、よくないなーとなる。「糧」っていうと食い扶持という意味に聞こえるけど、「今日もプログラミング楽しかった」「今日もやりたかったXをやることができた」って思うのも糧だよね、って意味で。\n\n個人の経験はすべての人に当てはまらない。勉強という、誰にでも許されている普遍的な権利というわけではないものでは特に、というそれだけ、なんだけども。\n\n```\n<a href=\"https://mobile.twitter.com/miyagawa/status/1095184749973319681\"></a>\n```\n\n"},{"frontmatter":{"slug":"/blog/2018-06-02_Sony--6000---------Essential-PH-1-----de81334408d","date":"Sat, 02 Jun 2018 15:32:11 GMT","title":"Sony α6000 を買った、またはEssential PH-1はいいぞ","epoch":"1527953531","excerpt":"Essential PH-1は買い"},"markdownBody":"\n### Sony α6000 を買った、またはEssential PH-1はいいぞ\n\n### 買った\n\n昨年は転職やらなんやらでバタバタで夏休み的なもので旅行をできなかったので、5月に10日休んでスペインはアンダルシア地方に行ってきた。で、当然写真を撮るわけなんだが、せっかくなのでスマホじゃないカメラを持っていきたいと思った。\n\nこれまで使っていたのは Pentax K10D で、重いし一番よく使うレンズに曇りがある（落っことしたせい）しで、持っていくのやだなあと思って、もっと軽いカメラを探すことにした。小さくて軽くてもマニュアル撮影とかバルブとかできるってことになるとやっぱりミラーレスってことになるらしく、ミラーレスということになるとPentaxはもちろん、CanonやNikonもいまいちらしく、Panasonic/OlympusかSonyかって選択になるらしい。\n\n$500とかくらいでどうにかなりませんかねえと、Google Shopping Searchしてみたところ、[Tri-State Camera](http://tristatecamera.com/)がSony α6000を$500未満で売っているのを発見。出発日が5/16で買ったのが5/11というギリギリの注文だったけど、余計に払って2 day shippingを選択して、会社の所在がニューヨーク近郊ぽいから大丈夫だろうと思ってエイやで発注したらちゃんと届いた。\n\n### 調べた\n\n届いたのでさっそく使い方をいろいろ調べた。おまかせモードが2つあって、i+のほうがファンシーな写真を撮れるらしい印象。あとはシャッタースピード優先、絞り優先、マニュアルという感じか。パノラマとかシーンとかもあるけど使わなそうな気がした。\n\nそれより面白そうなのは、カメラに[ダウンロードする「アプリ」](https://www.playmemoriescameraapps.com/portal/)があって、カメラの機能を使って変わった写真を撮れるらしいこと。有料アプリがたくさんある中、とりあえず無料のアプリ「Picture Effect+」と「Sync to Smartphone」を落としておいた。\n\n### 使った\n\n実際にスペインで使ったんだけど、まあ軽いので持ち運びが苦ではないのが一番よい。ほぼ歩いて回って観光していたので、K10Dを持って行ったら辛かっただろうと思う。\n\niとi+の2つのオートモードの違いは、iのほうが見たままのを撮れる感じで、i+のほうがもう少し写真としてどうにかしてくれるという感じか。\n\n![これiだっけi+だっけ忘れた](/images/1*eSEd6CEVSysOXm9oNDfoIQ.jpeg)\n\nK10Dの機能で一番missしたのは、グリーンボタン。絞り優先とかオートじゃないモードにしているときに、今の状況をカメラはどう思っているのか（つまりグリーンモードで撮ると絞りやシャッタースピードはどうなるのか）をサクッと知ることができて、下手くその僕には必要な機能だったんだけど、それがないこともあって、ついつい前回のマニュアル設定で全然違うシーンを撮ってしまうことが多くて、あとで見ると暗すぎたり。明るくて白飛びしているのはファインダーでわかるんだけど、あとで見たら暗すぎるっていうのは、ファインダーで見ると意外にカッコよく見えることが多くて、お、これでいいじゃん、って思ってしまう。\n\n![く、くらい](/images/1*_cyKFCRi_G7DTzG5zrsJKw.jpeg)\n\ni+と、マニュアル系のモードでは機能選択すれば、Auto HDR（3枚撮り）してくれる。\n\n![日差しが強いですね](/images/1*GDRnsheEvCgOrD04oZgrsw.jpeg)\n\nPicture Effect+ アプリを使うと、強いHDR写真を撮ることもできて、これはこれで面白い。\n\n![花の小径](/images/1*CxlBvUY4RRQmwWVucukp-w.jpeg)\n\nToy CameraとかMinitureとかのエフェクトもあって使ってみたけどよくわからなかった。\n\nそれと、Sync to Smartphoneアプリが便利だった。古い4GBのSDカードしかなくて2日で満タンになってしまっていたんだけど、adhoc wifi接続したスマホに「その日の分」を全転送しておいて、あとはまた「その日の分」を全消去してしのいだ。スマホが勝手にGoogle Photosに上げてくれるのでその後のことはあまり考えなくてよい。\n\n### レンズ欲しい\n\n標準キットのレンズが16–50mm/f3.5–5.6っていうズームレンズで、使ってみた結果別にいいんだけど、暗いところで明るい写真を撮るのが好きなので、f/1.xのレンズが欲しい。いくつかあるけど高い。がんばって働こう。\n\n### ところでEssential PH-1\n\nカメラがクソだカスだと未だにレビューで書かれるPH-1ですが、昔の記事はともかく、最近のレビューでもそうやって書かれているのを見ると、はてこの人は一体どんな写真を撮りたいんだろうと思ってしまう。PH-1のカメラ、今ではだいぶいいと思うんだけど。フツーよりもだいぶ良いという意味で。\n\n![ ](/images/1*xjA0jyO_Zoul3vEIFEgv-g.jpeg)\n\n![HDRですけどね](/images/1*M2_cAE_sXw38v6O29rzIPA.jpeg)\n\nα6000と比較するとこう。いやあ悪くないと思うんだけどなあ。\n\n![](/images/1*Tl2JAcAjHX7U5Dn_U4IaLg.jpeg)\n\n![左α6000 右PH-1](/images/1*fLfAJiaqxVN86mq9JDToqA.jpeg)\n\nSnapdragon 835の128GBでこの程度の写真を撮れて、[お値段$500未満](https://www.amazon.com/dp/B074BWGRKH/)ですよ。お買い得じゃないですか。セキュリティアップデートはPixelより早いし、今すぐAndroid Pも入れられますよ。[今すぐ買う](https://shop.essential.com/products/phone)\n\n### 買ってよかった\n\nα6000は買ってよかったなと思った。僕が撮るような下手くそ写真はスマホカメラで十分なんだけども、まあ撮ってて楽しかったし。今回は三脚を持っていくのを忘れたり、夜早々に寝ていたこともあって夜をほとんど撮れなかったので、次どこかに行くときは夜の写真を撮ろうと思う。\n\n"},{"frontmatter":{"slug":"/blog/2018-03-25_Uber-Visa----------a9728070d622","date":"Sun, 25 Mar 2018 19:15:49 GMT","title":"Uber Visaカードをゲットした","epoch":"1522005349","excerpt":"追記: 2019–11–32020年2月以降、ここで書いているようなrewardsはUber Visaでは得られません。Uber関連のサービスに毎日お金を払っている方以外にはおすすめできないカードになりました。念のため追記しておきます。詳細はこちらの記事などをご参照ください。"},"markdownBody":"\n### Uber Visaカードをゲットした\n\n**追記: 2019–11–3**\n2020年2月以降、ここで書いているようなrewardsはUber Visaでは得られません。Uber関連のサービスに毎日お金を払っている方以外にはおすすめできないカードになりました。念のため追記しておきます。詳細は[こちらの記事](https://www.businessinsider.com/uber-credit-card-review)などをご参照ください。\n\n### アメリカでクレジットカードを作る\n\nアメリカ合衆国に来て、Social Security Numberを取得したあと最初にしたことの1つがクレジットカードを作ること。SSNを取得したてで当然Credit Historyは皆無なので、それでも作れるカードの代表ということで、[ANA Card USA](http://www.anacardusa.com/en/)に申し込んで、3週間くらいで無事カードを手に入れたのが2013年6月のことだった。\n\n### ANA Card USA時代\n\nカードをゲットしてから、地道に支払い可能なすべてのものをカードで支払い、遅滞なく毎月全額を収め続けてスコアをあげたつもりになっていたが、なにせ、ANA Card USAはなにしろANAのマイルがつくだけ。こっちはANAに乗るとはいっても大した回数でもないしマイルの使い道にとぼしい。\n\nそろそろそれなりのスコアになったかなあと思った10ヶ月後、なにげなくクレジットカード比較サイトを眺めていたら、新しくアメリカでサービスを始めたBarclaycardの紹介を目にした。旅行関係の支払いで1$2ポイント、2500ポイントを$25として使えて、初回ボーナス50,000ポイント。\n\nこっちならANA以外の場所でもポイントを使えるし。$89の年会費があるものの初年度は無料だしANA Card USAも年会費はかかっていたから、まあいっかということでダメ元で申し込んでみたのが2014年3月。なんとapproveされてカードが届いた。ANA Card USAは速攻キャンセル。\n\n余談。ANA Card USAをキャンセルした直後に、Amazonで買った商品がなんだか理由は忘れたけどキャンセルされて、カードにrefundされた。されたんだけどカードはもうキャンセルしたので戻ってこない。困ってANA Card USAを運営するFirst Bankcardに電話してみて事情を説明したら、refundされた分を小切手で郵送してくれた。後にも先にも小切手を持って銀行に行って口座に振り込んだのはこのときだけ。\n\n### Barclaycard arrival+時代\n\nBarclaycard arrival+カードは、当初はクレジットカード比較サイトでの評判も上々だったし、クレジットラインも地道に増えたし、Mastercardのランクも少し上がったし、とそれなりにrewardingだったんだけども、2015年後半頃からポイント使用の下限が10,000に跳ね上がったり、Android Payがいつまでたっても使えなかったりと不満を溜め込み始めた。\n\n2017年初にも、年会費を払う前に解約していよいよ他に移るかあ、と思って他のカードをいろいろ検討し始めた。Barclaycard arrival+は、この頃にはクレジットカード比較サイトでも全然評判がよろしくなくて、他のカードにない唯一の利点がヨーロッパでChip and Pinできるよってこと。いやまあヨーロッパ旅行もする予定はあるけど、そんだけ？\n\nとはいえ移る先のカードにも対して魅力的なものがない。周囲を見るとChase Sapphire Preferredが一番人気のようだけど、年会費が今よりあがる上にメリットは大して変わらないように見える。初回ボーナスをもらえるのは魅力だけど。\n\nそうやって結局2017年の年会費も払い、さらに使うこと1年。Android Payもようやく使えるようになり、まあこれでいいのかなあと思っていた矢先に、Barclaycardのサイトで[Uber Visa](https://www.uber.com/c/uber-credit-card/)の紹介を見つける。\n\nUber Visaは年会費無料、Foreign Transaction feeゼロ、んでもってカテゴリーによるが最大4%バック。まじで？APRとかバランストランスファーのレートとか俺には関係ないし、これ最高じゃね？\n\n### Uber Visa却下\n\n引っ越しで家賃を下げてケーブルTVを切ってといろいろ断捨離したので、ついでにカードも断捨離するかということで、さっそくUber Visaを申し込んだ。Ficoでのスコアは700をコンスタントに超えているし、問題はないだろうと。\n\nところがその2週間後、そっけない手紙とともに申し込みは却下された。\n\n手紙に書いてある理由は「too few accounts with sufficient satisfactory performance」。いや確かにaccountは今のカードの分しかないけど、それあんたたち自身（Barclaycard USA）なんだから、俺がどんだけloyalでtrustworthyな客か知ってるだろう？！\n\nんーまじかこれってどうにかなるんだろうかと、その理由の文言をネットで検索してみると、これはBarclaycardが使う文言なのか、同じのに当たった人がわりと多いみたい。掲示板を見てみたら、スコア800でこれをくらったという人に対して、回答者が「definitely recon」と言っている。reconnaissance？どこを？なんで？と思って「Barclaycard recon」で検索したら、reconsiderationのことだった。どうやらあまり表には出していない電話番号があるらしい。\n\n### Uber Visa reconsideration\n\nさっそく翌日に電話する。手紙に書いてあるApplication番号を告げると、向こうから返ってきた返事は「当社からあなたに渡せるcredit lineは現在arrival+で利用可能な分だけです。arrival+のほうのcredit lineを減らす気はありますか？」なるほどー。\n\n「もちろんです。Uber Visaをもらったらarrival+のほうはキャンセルするつもりでした」\n\n「それなら、Uber Visaのほうに大部分を移して、arrival+のbalanceを全部払ったらあらためて電話してくれれば残りを全部移動させてarrival+をキャンセルするという流れにできますよ」\n\n「それでおなしゃす！」\n\nそして無事approveされた。5日後にカードが無事手元に到着する。めんどくさがらずになんでも電話してみるもんである。\n\n### Final thoughts\n\n思うに（完全に想像だが）、僕が今持っているカードがBarclaycard発行のものでなければすんなり行ったのかもしれない。逆にUber Visaではなく、例えばChase Sapphireを申し込んでいれば、やはりすんなり行ったのかもしれない。これらは異なるissuerだから新しくcredit lineを作ることになるのだろうというのがその理由。副作用として、最初に使える枠はだいぶ下がっただろうけど。また一つアメリカでのサバイバル知識を得た。\n\n知ってのとおりUberという会社自体の評判は非常によろしくないので、Uber Visaを持ってますとか自慢するようなことじゃないんだけど、このカードのreward内容は年会費無料のグループの中では群を抜いていると思う。Amazon PrimeのメンバーならPrime VisaでWholefoodsでの買い物が5%バックになるっていうので一瞬考えたんだけど、結局Primeの年会費を払いたくなくてやめた。\n\n以上、クレジットカードを変えた話でした。\n\n"},{"frontmatter":{"slug":"/blog/2018-03-25_------------------c7e99608f41e","date":"Sun, 25 Mar 2018 14:13:47 GMT","title":"ニューヨーク市内で引っ越し先へ入居","epoch":"1521987227","excerpt":"ニューヨーク市内で引っ越しの当日からの続き"},"markdownBody":"\n### ニューヨーク市内で引っ越し先へ入居\n\n[ニューヨーク市内で引っ越しの当日](/blog/2018-03-22_-----------------bd1b22641946)からの続き\n\n今度の部屋は結構広いウォークインクローゼットがあって収納は問題ないなーと思っていた。持ってきた服をぶら下げ、とりあえず使わないものを棚の上に押しやり、お皿やなんかをキッチンにしまって…と荷ほどき作業をしていて気づいたのは、キッチン周りの収納スペースが狭いというか使いづらいこと。\n\n前の部屋はキッチンカウンターとシンクが別々にあってシンク側にもカウンターがあって、さらにそれぞれのカウンターの上に収納スペースがあって引き出しもあったから、食器類を入れる場所には不自由していなくて、というかだいぶ余っていたんだが、今回の部屋のキッチンの収納は、お箸やスプーンをしまったり、オタマや包丁をしまったりする引き出し的な収納がない。キッチンカウンターはそれなりの広さがあるんだが、冷蔵庫の配置の関係で、デッドスペースも多い。\n\nバスルームの手洗いの下の収納にものをしまおうとしたら、これもありがちな、水漏れ中。パイプから滴ってて、収納したら濡れてしまう。\n\n### 鍵交換\n\n結局部屋の鍵は見つからず、すこし落ち着いたので1階に戻ってフロントマンに聞いてみたところ、ちょうど鍵職人がくるところだった。ドアのシリンダーごと交換することになったらしい。\n\n鍵交換職人が作業しているときに、ここの管理のheadだという若干殺し屋っぽい風情の人もきたので、ついでに水漏れの件も聞いてみた。そしたら鍵をつけた後にそのまま鍵職人が見てくれて、あーだこーだやって直してくれた。仕事はやい。このシンクは部屋の内覧をしたときはまだ設置されていなくて、殺し屋曰くなんだかあわてて外部の職人が付けたようなので、うまく締まってなかったんだな、とかなんとか。\n\n鍵交換も終わり、改めて1階へ戻って新しい鍵のセットを受け取ってサインをして、ようやく部屋が自分のものになった感。自分のものじゃないけど。ただ、このときもまだドアは開放モードのままで鍵を使わなくても出入りできるようにしていた…\n\n### ベッド組み立て\n\n我々は日本人なので、家では靴を脱ぐ。そこでお約束の、だいたい片付け終わった部屋のフロアを雑巾掛けして、裸足で歩けるようにする。\n\nそして寝床を確保するべく、新しく買ったベッドフレームを組み立てる。ドライバー1本でできるので、これは簡単。ベッドの脚の底に、家具なんかがぶつかっても傷がつかないようにするためのフェルトみたいなやつ（日本で100円ショップで買ってきたあった）を切り貼りする。すべらせて移動してもまあ床にも傷がつかないし。\n\nそしてベッドの上でこれも新しく買ったマットレスを開放する。[ATP](http://atp.fm/)のディスカウントコードを入れて買った[Casperのマットレス](https://casper.com/friends/fumiakiy)は、真空パック状態で丸まってやってくるので、傷つけないように梱包を開いていくと、プシューっと空気が入ってマットレスが立ち上がる。枕もついでにCasperで買ったので、これも開放する。\n\nCasperの枕は、なんだかかわいらしい円筒形の容器に1つずつ入ってくる。この容器をゴミ箱として利用することにした。まあまあいい感じ。\n\n### Casper vs Nectar\n\n実はマットレスは、当初はCasperではない別のメーカーのものを購入していた。買わなきゃならないってことで探していたら、[Nectar](https://www.nectarsleep.com/mattress)というメーカーがCasperと似たような製品をCasperの2/3の値段で売っていたので、それを買ったのだ。Nectarはキャンペーン中で枕2つが無料で付いてくるってのもあって。Nectarは365 nights trialで1年寝てから返品もアリって言ってて。\n\nだがしかし、Nectarは2週間経っても発送される気配がない。おまけにNectarは発送していないのにカードに課金をしていることに気づいて不信感も募る。とうとう引っ越し1週間前になったので、これは新居で寝る場所を確保できない懸念がピークに達して、結局Casperでオーダーした。Casperはすぐ出荷するって書いてあったし、評判は悪くないし。\n\nNectarのサイトのLive Chatでオーダーをキャンセルした。\n\n「もう予定の期日を過ぎたから、これ以上発送されなかったら当日に寝場所がなくなっちゃうから、もう他で買っちゃったんだよ」\n\n「でも明日発送されるかもしれないよ」\n\n「発送されても、他で買っちゃったし、おそらくNectarのほうを返品することになると思う。そしたらそのマットレスはゴミになってしまう。だったら最初から他の人に送ったほうがいいと思う」\n\nとかいうやりとりを経てキャンセル。カードへの返金を確認した。もう少し余裕を見て注文しておけばよかったんだが、なんせ引っ越し自体のスケジュールに余裕がなかったから。\n\nCasperの寝心地は今のところとてもよい。今からNectarを発注して、Casperは100日以内に返品するっていうのも仕組み上はアリだけど、今のところそうする予定はない。\n\n### 鍵が開かない\n\n引っ越し当日の最後のタスクは、Verizonの技術者が来てネットを開通すること。午後3時から5時っていう枠で頼んでいて、4時くらいに電話が来てあと30分くらいで行くっていうから待っていたんだが、案の定5時になってもこない。ありがち。5時半になってもこないので、Verizonに確認すべく共用フロアのネットを使うために部屋を出る。フロントでWifiのパスワードを聞いて、Verizonとチャットを始めて「技術者がこないんだけど」とか打っていたら、妻が走ってきて「なにやってんの！もう来たよ？そしたらなんだか出て行っちゃって、そいでもって部屋に入れなくなっちゃった！」\n\nえ？\n\n急いでチャットをぶったぎってそのままに部屋に戻ると、ドアが閉まっていて開かない。ドアはオートロックというか、外からはドアノブを下げられなくて、開く方向に鍵を回すことでドアノブを下げるのと同じ動きをする仕組みなのだが、鍵を回しきってもドアの金具というか爪というかが完全には引っ込まず、引っかかったままなので開かないという状況だった。\n\nこれって…泥棒の道具でどうにかなるんだろうか…？ええーどうしようと思いながら全身全霊を注入してエイやって回したら開いた！\n\nホッとして、1階に電話をかけて、鍵が開かない状態ですと説明をして再度handymanを派遣してもらう。さっき鍵を付けたのとは違う人が来て、状態を確認して、「作業するにはウォデュコディが必要だからそう言ってくれ」と電話を渡される。え？え？なにそれ？と思いながら電話でいやだからその鍵が開かなくてえっと、といってまた代わってもらい「自分で言ってくれない？」と言ったら「いやそれはできない」とかえ？え？となってもう1回代わったら、電話の先でフロントマンが「Work Orderを出すんでしょ？」と言われてあーそういうことかーはいお願いします、と。要するに、今は確認しに来ただけで実際に手をつけるには手続きとしてWork Orderを発行してもらわないといけないということだった。\n\n鍵の調整をしてもらっていたらやたら陽気なVerizonの技術者が戻ってきた。「Hey wazzaup」遅れてすみません的なことは特になく。ありがち。部屋にある機械は動いているが地下にある大元が切れているのであーだこーだみたいな説明をして、口笛を吹きながらなんだかごそごそやっている。その後も地下と部屋を何度か往復し、本部らしきところに電話をかけて状態を確認したりしていた。\n\n### トイレの水が止まらない\n\n鍵の調整中にまた殺し屋風情のheadがやってきたので、彼にもう1つお願いした。実はトイレを何度か使ったら、それっきりトイレの水が流れるのが止まらなくなってしまっていた。前のアパートでも1回あって、タンクの中の栓の部分のゴムが弱っていたっぽいんだけど、今回はそんな感じではなく、お手上げだった。\n\n鍵の設置をしたときもこのheadはその場にいたので、しきりにhandymanに「ほんとに開かなかったのか？全然問題ないじゃないか」「いや今俺が直したから問題がなくなったんだよ」「んーどうなってたんだ？」「いやだからこれがこの辺で止まっちゃってたんだよ」みたいなやりとりをしていた。\n\n鍵の調整は完了したので、一応妻の力でも十分開けられるかどうかを外から確認して、OKとなった。殺し屋headは上がる時間だったみたいで、鍵の調整をしている人に引き続きトイレも作業するように支持して、Good nightといって去っていった。さっきから殺し屋などと書いているが、見た目がそうなだけで、普段から挨拶も交わすし、仕事きっちりな、いい人である。\n\nトイレの調整もなんだかネジを閉めたりなんだりしたらしく、キュッと止まるようになった。\n\n### My voice is my password\n\nVerizonの技術者が陽気に口笛を吹きながらごそごそやり続けて1時間。どうやら地下で切れているのはテレビのデータだったらしく、インターネットのほうは生きていたようで結局ネットはつながったらしい。最後まで電話でその点をしきりに「この契約はネットだけだから、ごにょごにょの電波がゼロでも問題ないだろ？な？」と確認していた。\n\n配線盤はクローゼットにあるんだけど、Wifiルーターは室内におきたかったので、壁のアウトレット側の作業もしてもらい、ルーターの接続を確認した。そこでネット開通のためのユーザーアカウントの引き継ぎみたいな作業を行うらしく、なんか専用の端末にログインしたんだが、そのとき彼が口で電話に向かって、自分の技術者IDらしき番号に続けて「My voice is my Verizon password」と言ったので仰天した。\n\n僕が大好きな映画[Sneakers](http://www.imdb.com/title/tt0105435/)の中で、ビルの中のセキュリティレベルの高いエリアに入るために音声認証が必要というシーンがあって、そのときのフレーズが「[My voice is my passport, verify me](https://www.imdb.com/title/tt0105435/quotes/qt0448965)」なのだ。\n\n作業が終わったところで技術者にSneakersを見たことがあるか聞いてみたら、あるようなないような返事だったのでいまいち盛り上がれなかったんだけど、この人とは代わりにゴジラの話題で盛り上がった。子供の頃にゴジラを映画館で見てそれ以来大好きだと。日本の輸出品で一番優れているものがゴジラなんだと力説されて、あー、おー、そうかそうか、最近のやつはみた？みたらいいよ？あーアメリカのじゃなくて日本のやつ、いやアニメもあるらしいけどrealのほうね、あれ、アメリカでは見られないのかなあ、みたいなやりとりをした。\n\nWifiルーターを一度外して、壁のアウトレットにパネルをはめ込んで、再度繋げて、技術者のおっちゃんは意気揚々と帰っていった。帰りしなに、この近辺のおすすめバーを3つ4つ教わった。\n\nそして…ネットは繋がらない。は？と思ってルーターを再起動したりしたんだけど、ルーターには繋がるけど外にいけない。は？は？だってさっきはつながったじゃん、と思いながらふと、ルーターを壁のアウトレットから外して、クローゼット内にある機械のコネクタに直結したらつながった。ありがち。たぶんアウトレットのパネルをはめ込んでいるときに断線したかなにかだろう。\n\n結局Wifiルーターは今もクローゼットの中にある。別に電波強度も問題ないので、まあかえってよかったかも。\n\n### Settled\n\nこの日の夕食は、さっそく繋がったばかりのネットを使って食事のデリバリーを頼んだ。前のアパートでは、デリバリーが到着するとフロントから電話がかかってきて、それに応答しないとデリバリーが中に入れないことがあったので、デリバリーを頼んだときは（[前記のとおり](/blog/2018-03-11_------------------1-35e3f49ceed1)）PCを開いてGoogle Voiceで電話を受けられるようにしておかないといけなかったのだが、このアパートにはフロントから一方通行の連絡が受けられるインターホンが付いている。わりとけたたましいピーピー音がなって向こうからの声が聞こえる仕組み。なので、やっぱり電話は不要になりそうだ。\n\n初めての引っ越しは一応の落ち着きをみた。\n\n続く。。。かも\n\n"},{"frontmatter":{"slug":"/blog/2018-03-22_-----------------bd1b22641946","date":"Thu, 22 Mar 2018 11:59:05 GMT","title":"ニューヨーク市内で引っ越しの当日","epoch":"1521719945","excerpt":"ニューヨーク市内で引っ越しの準備その3からの続き"},"markdownBody":"\n### ニューヨーク市内で引っ越しの当日\n\n[ニューヨーク市内で引っ越しの準備その3](/blog/2018-03-17_------------------3-72cd85892fd1)からの続き\n\n### Verizon Fios…\n\n金曜日の朝9時指定で引っ越しなので、木曜日の夜は冷蔵庫も空で外食し、慣れ親しんだ街で最後の夜を過ごす。\n\n金曜日の朝。起きたらすでにネットが繋がらない。今日移動することは指定していたが、まさか0時で切れるとは。なんでそこだけきっちりやるんだ。向こうが開通したタイミングで切れればいいんじゃないか。\n\n[Ting](https://zql8fe716fn.ting.com/)でネットを使えるので助かった。[電話を契約しておいてよかった](/blog/2018-03-11_------------------1-35e3f49ceed1)ーと思った瞬間だった。\n\n### Men arrived with a van\n\nほぼ時間通りに引っ越しManから電話が入り、引っ越し作業開始。まずは5年間住んでて初めて足を踏み入れた裏の荷物搬入口へ行って、導線を確認。バンから出てきた明らかに現場のボスぽいおっちゃんと握手をし、もう1人の背の高い屈強な男と一緒に部屋へ向かって荷物の量などを確認する。こちらの作戦、つまり大物を運んでいる間に箱詰めを完了する予定を伝える。\n\n予定と実際の内容に違いがないことを確認して、引っ越しguysがバンから青いプラスティックの梱包用の箱やダンボール製のワードローブ、それに映画でおなじみのダクトテープなどを取り出してくる。\n\n自転車、ソファ、テーブル、テレビを運び出している間にその青い箱に細かいものをバンバン入れて…と思ったが、実際はソファやテーブルを運ぶ前の準備を引っ越しguysがいろいろしている間に梱包作業はあらかた終わってしまい、ソファは後回しになった。背の高い男は「やー少ないし早いから大丈夫とかってみんな言うんだけど実際はそうじゃないことが多いんだよねー。今回は本当に早かったな。」とお褒めの言葉？をいただいた。\n\nこの背の高い引っ越しManは、話すタイミングがあるごとに「俺たちは仕事を早くして余計な金を使わせないことを大事にしてる」「なるべく早く終わらせるから心配するな」「任せとけちゃんとやるから」と言っていて、好感度大賞を狙っているんだろうか？という感じ。実際に好感度は高い。\n\n結局正味40分程度で搬出は終わった。\n\n### さよならマンハッタン\n\n部屋に忘れ物はないか確認して退出。大家さんとの約束通り、アパートのフロントデスクに鍵を預ける。まだ部屋の契約は残っているので部屋に入る権利はあるのだが、新しいrenterを探すために部屋を見せることが必要で、不動産屋さんが入るときに使うのだ。ちなみに、大家さんもビルのフロントもそれぞれ鍵を持っているので、彼らはいつでも勝手に入れるっちゃ入れる。もっともそんな心配をしたことは5年間で一度もなかったが。\n\n5年前に初めて来たときにもやりとりしたフロントのマネージャーと最後の挨拶をし、外に出て引っ越しguysと引っ越し先で落ち合うことを確認して、地下鉄で移動する。\n\n### 来たぜブルックリン\n\nそういうわけでアパートを見にきたとき以来のブルックリンへ降り立つ。朝から何も食べてないのでお腹が空いたということで、アパートの近所のスーパーを下見がてら訪問して、すぐ食べられそうなパンみたいなやつを探していたら…電話が。「おい、着いたぜ、どこにいる？」まじか早いなMan With A Van。道空いてたのかな。\n\nあわてて走ってアパートへ行き、フロントの人と「今日引っ越してきたんですけど」「ああ聞いてます。Nice to meet you」「Nice to meet you too. えーと鍵をください」「あー、んー、ん？」\n\n### 部屋の鍵がない\n\n引っ越しの段取り確認の中で、部屋の鍵を前日に受け取れないか聞いてみたんだが、鍵は引っ越し当日にしか受け取れないけどフロントに置いてあるから心配すんな的なやりとりを不動産屋さんとフロントのマネージャーと三つ巴でやってあったんだが、この始末である。\n\nそのマネージャーとのメールを見せて、\n\n「ほらフロントに置いてあるって書いてあるでしょ」「うん…ここにあるメモにもこのフロントの所定の置き場にあるって書いてあるけど、でもない」「このマネージャーはどこにいる？」「オフィスにいるから電話してみよう」\n\nフロントマンがマネージャーに電話をする…「え？不動産屋が持ってるかも？あ、そう、電話してみる」\n\nさらに電話…「ハーイ、うちのマネージャーが部屋の鍵をあんたが持って行ったって言ってるけど？え、持っていかない？あー、大家さんが持って行ったかも？違う？いやーそんな記録ないけど？えー？あー？」\n\n再度マネージャーへ電話「いや持って行ってないってよ、いや探したけどないんだよ。記録は昨日の午後に返したことになってるんだけど？…んー、バーイ」\n\n要するに所在不明。結局どうなったんだか今もわからない。\n\n結局30分くらいこのやりとりで無駄になり、引っ越しguysも荷物を運べる状態にしてしびれを切らして電話してきて「おい、まだか」「鍵がないって言っててさー入れないんだよ」「なんだそりゃ困ったな」的なやりとりをした挙句、とりあえず管理会社が持っているマスターキーを使って部屋を開けて引っ越し作業だけさせてもらうことに。\n\nやっと部屋の階へ行くと、荷物を部屋の前まで持ってきて待っていた好感度guyが笑顔で「おお、やっとか。じゃあがんばって早く終わらせよう」的な。好感度さらに倍。\n\n### アンパッキング\n\n何もない部屋に荷物を適当に運び入れる。引っ越し屋さんが持ってきた青い箱を開けて中身を適当に部屋に出して、青い箱はたたんでいると「お、これってもしかしてもう持って帰ってもいい？」\n\n青い箱は[totobox](https://totoboxes.com/)という有料の梱包資材で、Man With A Vanがオススメしてたからそれにしたんだけど、再利用可能なのであとで回収される。荷ほどきが終わらなければ置いていってくれてあとで連絡すれば取りに来てくれるんだけど、それももちろん手数料がかかるし、なんせ20箱程度で中身はとりあえず部屋にぶちまけてしまえばいいものなので、その場で回収していってもらうことにした。\n\nそれと、新しく買った[ベッドフレーム](https://www.amazon.com/gp/product/B01M24RQTM/)と[マットレス](https://casper.com/friends/fumiakiy)が配達で届いていたので、これも好感度guyに1階から運んでもらった。一応と思って、事前にメールでMan With A Vanに頼んでみたらあっさりOKで作業目録にいれておいてくれて、好感度guyも「おお、話は聞いてるぜ」と。Man With A Van、事務所と現場との連絡もちゃんとしていた。よかったよかった。\n\n荷ほどきを終えて、That’s itとなった。3時間で時計を止める。\n\n### 請求\n\n最後に現場監督の方が部屋に来て、3時間だから$360+車$25（だったと思う）と言われて、あれ、見積もりより安いなと思ったんだけどそのままOKした。支払いはクレジットカードでと言ってあったので、Square…ではなく電話を始めた。電話に対して「Hey boss, it’s John. Yeah, finished. 3 hours.」んで僕に「カードをよこせ」渡すと、電話口にカード番号、MM/YY、裏っ返してCVVも、口頭でお大声で。はあーまじかーそうだよねえー。たぶん一番手数料が安い仕組みなんだろうな。小切手とか最近流行らないし、かといって引っ越しのドタバタのときに現金で数百ドル持って来いとは言いづらいし、後日請求と振込とかだとトンズラが怖いし、これが安くて確実な方法なんだろうな。\n\nBillingも完了して、チップを$20ずつのつもりで$40渡して、ありがとうありがとうさようなら元気でねと挨拶して、2人は帰って行った。考えたら最後にバンの中が空かどうか確認するとか荷物のチェックとか一切しなかったけど、まあ正直金目のものもないし、覚えているものはすべて出したし、問題はなし。\n\nと思ったら20分後くらいに電話がかかってきた。「へい、梱包資材のtotoboxとワードローブの使用料を請求するのを忘れてた。すまんがさっきのカードに$80足させてもらうぜ」「ああーそれでなんか安かったのかー。はい問題ないです」「オッケー、バーイ」\n\nというわけで、Man With A Vanの請求は総額$478でした。ほぼ見積もりどおり。\n\n### Man With A Vanについて\n\nここまで読んでいただいた方には僕がMan With A Vanを気に入っている様子が伝わったと思うのですが、後日譚がありまして。\n\n液晶テレビを毛布みたいなやつでくるんだりそれなりに梱包して運んでくれていたんですが、おそらく車内でなにかにぶつかったか圧迫されたかで、画面左下の一部がなんというか半円形にぼやけるようになっていました。ど真ん中じゃなくてよかった。映像のその部分がたまたま白っぽいと、黒い影のようになって目立ちます。\n\nフロアランプはほぼそのままの状態で、長細いダンボール箱につっこんで、その上にほかにもいろいろいれて運んだんですが、一番底の台の部分がへっこんでいて、これもおそらく車内で何かに強くぶつかった跡だと思われます。\n\n傷がついたかなと思ったのはこの2点だけで、ソファと自転車は問題なし。お皿やなんかも、きちんと梱包してダンボールに入れておいたので、特に問題なし。\n\n総じて、丁寧さには欠けるけど、まじめで親しみやすくて働き者guysでした。Oz MovingやFlatrate Movingの見積もりは（いろいろディスカウントして）$700以上だったので、今回の引っ越しの規模にちょうどあったサービスと値段だったなと思っています。それに他の引っ越し業者なら丁寧だったのか？ってのはわからないし。\n\nそういうわけで、よっぽど潔癖な方でない限り、僕はMan With A Vanをオススメできます。\n\n[ニューヨーク市内で引っ越し先へ入居](/blog/2018-03-25_------------------c7e99608f41e)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-03-17_------------------3-72cd85892fd1","date":"Sat, 17 Mar 2018 15:40:41 GMT","title":"ニューヨーク市内で引っ越しの準備その3","epoch":"1521301241","excerpt":"ニューヨーク市内で引っ越しの準備その2からの続き。"},"markdownBody":"\n### ニューヨーク市内で引っ越しの準備その3\n\n[ニューヨーク市内で引っ越しの準備その2](/blog/2018-03-11_------------------2-ae04f126a97f)からの続き。\n\n### 家具付きアパートから引っ越す\n\nこれまで5年間住んできたアパートは、会社経由で紹介してもらった[住友不動産の柏原さん](https://www.sumitomo-ny.com/agent_detail.aspx?agentid=2118)が、こちらから連絡した条件を元に探してくれていたアパートたちを2、3日かけて内覧しまくって決めた。当時連絡した条件は、$2,000/mo、ちゃんとしたキッチン、できれば自転車置き場、くらいだったと思う。\n\nマンハッタンで$2,000/moで探すのは実際かなり厳しく、まあそれでも10部屋くらいは見たんだが、ここを見て$2,500/moだったけど決めたのは、家具付きだったことが大きな理由 [1]。クイーンサイズのベッド、一人がけカウチ、テーブルと椅子、間仕切りにも使える棚と、おまけに炊飯器からお皿からとにかく生活に必要なものがすべてそこにあった。僕はNYにはスーツケースとダッフルバッグ1つずつに服とバリカンと髭剃りだけ詰め込んで来たので、最初に家具や日用品への出費を迫られなくて本当に助かった。\n\n[1] あとで気づいたんだけど、あのロケーションとビルの設備とを考えると、$2,500/moはかなりお得だったと言わざるを得ない。\n\nそういうわけで、この5年の間自分で買って引っ越し先にも持っていきたい大物といえば、テレビとソファとダイニングテーブルくらいしかない。とはいえ自分たちで運べる量ではないので、引っ越し屋さんを探す必要があった。\n\n### Man With A Van\n\n日本でもそうだけど、引っ越し屋のトラックは目立つので日常生活をしているだけでなんとなくそういう会社があることくらいはわかる。よく目にしていたのはOz MovingとかFlatRate Movingとか。先述の理由で普通の引っ越しより規模が小さいと思われるので、日本でいう赤帽みたいなサービスないのかなあと思ってsmall local movingとかって検索して、見つけたのがこの[Man With A Van](https://www.manwithavan.com/)というサービス。引っ越し以外にたとえばIKEAに一緒に行って、買った荷物をバンで運んでくれたりとかもするらしい。\n\nさっそく見積もりを依頼してみると、ものすごい速攻で返事がきた。相場がわからないので安いのか高いのかよくわからないが、人間2人の時給とバンの値段という計算自体はリーズナブルに思えた。ただ相場がわからないと交渉もしづらいということで、Oz MovingとFlatRate Movingにも見積もりを依頼した。\n\nありがちな話だが、両者とも返事の代わりに電話をよこしてきて詳しい話をさせてくれと。んで、電話してみるとこっちが見積もり依頼にすでに書いたことと同じ質問をしてきて、ふむふむなるほどーとか言って、それで最初の値段を掲示してくる。そのとき「今上司に確認してこれこれの分をWaiveしてもらったよ！」とかいうのを忘れない。\n\n両者とも出てきた金額はMan With A Vanにくらべて2割増しくらいだったかなと思う。まあ赤帽とヤマトやアートとを比較するのに意味がないのと同じことなので金額の多寡を比較しても仕方ない。ただこのときすでにMan With A Vanにすることに決めていた理由は、何か質問すると2時間程度でメールで返事が来ること。いちいち電話とかしてこないし、一度聞いたことをまた聞いてきたりしない。うざくないのだ。んでもって安いし自分たちの用途にあてはまっているし、小さいローカル会社ががんばってる風なのでそういうのを応援するという意味もまあまああるし、ってことで決めた。\n\nその後Quoteをもらった他数社には「もう他社に決めたからすまん」というメールを送り、それに対して電話がかかってきてますますうざくて嫌いになるというお定まりの流れもあった。\n\n### パッキング\n\nさて引っ越しを2週間後に決行することに決まった。その後のやりとりで、事前にダンボールを送ってもらうのはやめて、当日に再利用可能な梱包箱を持ってきてもらい、その場で梱包して出すことにした。事前に送ってもらうとその分金がかかるから。\n\n引っ越し作業の方々は時間計算でお金を取るので、その場での梱包は高くつくかもよって言われたんだけど、いやいやこっちは荷物なんてほとんどないんだから大丈夫任せとけと言った手前、準備は万全にしなくては。そこで必要になるのがダンボール箱。買うこともできるが、ここは日本でやったのと同様に、ほうぼうから集めてくることにした。日本だとスーパーとかでダンボールをもらえることが多いから、Wholefoodsとかにないかなーと。細かいものをなるべく箱にまとめておいて、当日は作業する方々が大物を移動させる間に、当日持ってくるはずの大きな箱に小さい箱をバンバンいれていって、30分程度でパッキング完了、という作戦を立案していた。\n\nWholefoodsでは使えそうなダンボールは見つけられなかったんだけど、会社に毎日なにかしら届くAmazonその他の箱を回収することには成功した。Office Managerの協力を得て、あとその週たまたまゴミ捨て当番だったこともあり、普段ならリサイクルの日に出すものとしてキープしてある箱を毎日せっせと家に持って帰ることを続けて、昼間に妻が細々したものをそれらの箱にまとめる。あと本とかDVDとかの当座は必要なくて細かいけどそれなりに大事に扱いたいものは、リュックに詰めて会社に持って行って一時的に会社に置いておくという荒技も使った。\n\nということで、2週間後には大小様々な箱と、空になった棚とクロゼットができあがった。洋服など入りきらないものもあったが、当日持ってくるはずの梱包キットに入れられるはず。布団やなんかも旅行用真空袋に詰め込むことにして、準備万端。\n\n### 引っ越し手続き\n\n東京23区内での引っ越しでは聞いたことがなかったが、こちらでは引っ越しや大型家具の搬入などでエレベーターを使うときには、エレベーターを事前に予約することと、搬入出業者の損害保険加入証明書（Certificate of Insurance, COI）をビルの管理会社に提出すること、セキュリティデポジットや諸経費が必要な場合がほとんど。これは引っ越し作業中に建物を傷つけたり作業員が怪我したりした場合に、お互いがお互いを訴えたりしないように、だと思う。今住んでいるところも次に住むところもそれらが必要で、まずは引っ越し業者にCOIを提出してもらう必要がある。\n\nMan With A Vanはこれもやっぱり速攻で、2時間程度で2つのビル宛のCOIをメールで送ってくれた。あとはそれぞれの管理会社のフォームに必要事項を記入して、引っ越しの日と時間帯を書いてエレベーターの予約をリクエストする。今いるビルの方はPDFに書いたものをスキャンしてメールで送るのだが、引っ越し先のビルはWebサイトでエレベーターの仮予約をして、申込書とCOIをメールで送って、それが承認されればエレベーターも予約完了という流れだった。\n\n引っ越し先のビルはあっさり申し込みできたんだが、今いる方のビルから手続きの返事がちっとも来ない。4日待ってしびれを切らして、催促のメールを入れたらその返事でようやくエレベーターの予約が完了した。引っ越し予定日の4日前。\n\nビルの側からは「エレベーターの予約が確定するまで引っ越し業者と日時を確定しないように」と言われるが、引っ越し業者は人員の配置などから当然のように「日時はいつですか」から契約作業をスタートするわけで、完全に鶏と卵。引っ越し業者を決めなければCOIはもらえないが、COIがなければ申し込みができず、エレベーターの予約もできない。もちろん多くの場合で引っ越し業者側が日時の変更に応じてくれるのだろうが、この2フェーズコミット問題は本当になんとかならないものだろうか。\n\nあ、そうそうそれとNYCのビルの多くは週末の引っ越しを許可しないのでご注意を。僕の引っ越しは金曜日の朝一に設定しました。会社には休みをもらって。\n\n[ニューヨーク市内で引っ越しの当日](/blog/2018-03-22_-----------------bd1b22641946)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-03-11_------------------2-ae04f126a97f","date":"Sun, 11 Mar 2018 16:02:02 GMT","title":"ニューヨーク市内で引っ越しの準備その2","epoch":"1520784122","excerpt":"ニューヨーク市内で引っ越しの準備その1からの続き…なんだけど引っ越した後の話になってしまった。"},"markdownBody":"\n### ニューヨーク市内で引っ越しの準備その2\n\n[ニューヨーク市内で引っ越しの準備その1](/blog/2018-03-11_------------------1-35e3f49ceed1)からの続き…なんだけど引っ越した後の話になってしまった。\n\n### Cutting cord\n\n2013年に来てからずっと変えていないものがVerizon Fiosのケーブルテレビ契約。入ったアパートではTime Warner Cable（現Spectrum）かFiosを選べたのだが、2週間入っていたAirbnbの部屋がTWCでまあ遅かったのと、Fiosは初年度契約がTWCより安い上に光ファイバーなので速い。Fiosにしない理由が見当たらず、Double playというやつで契約した。$79.99/moとかだったと思う。\n\nしかし初年度が終わると割引が消え、その後はテレビのチャンネル数を減らしたりなんだり頑張ってどうにか$120/moに抑える感じで、たけーなー変えたいなーと思っていた。引っ越し先のアパートを見にいったときも、必ずケーブル会社を確認していたが、ブルックリンだとOptimumが選択肢に入ることもわかった。Optimumの初年度プランはインターネットのみで$39.99。一応割引を受けているためVerizonとの契約を途中で切るとEarly Termination Feeを取られるが、それを払っても安上がりになる。Optimumの品質は不明だけど、まあダメなら乗り換えればいいさ。\n\nそういうことで、今月引っ越すので解約したいという連絡をVerizonのWebサイトのチャットで連絡する。この手の話でチャットで済むのはありがたい。\n\n当然といえば当然だがチャット先のサポートの人は、解約せずにVerizonのインターネットのみ契約への切り替えを提案してきた。全然脅すとかなんとかそんなつもりはなかったんだが、解約したいといったら向こうから初回契約のみのはずの割引プランを掲示してきて、結局Fiosの100/100のプランを$39.99で契約できることになった。アカウント自体は継続するから契約作業とかめんどくさいことはなくなるし、Verizon Fiosの品質はわかってるしってことで、月$10のWifiルーターのレンタルが入って$49.99/moということで。\n\nこれで今までに比べて$50以上浮いた。Cordをcutした以上、この浮いたお金を使う先は決まっている。\n\n### Netflix vs Hulu\n\n特にスポーツシーズンに結構ライブテレビを観ていたしニュース番組もそれなりに観ていたので、インターネット契約のみになってもライブTVは必要だろうなと思って検討を始めた。対象は、YouTube TV、Hulu with Live TV、DIRECTV NOW、PlayStation VUE、Sling TV。\n\nそれに、いわゆる動画サブスクリプションサービスで念願のあれやこれやも観たい。これまで$120も払っている上にさらに月$10で見る暇もないドラマを見るサービスとかないわーと思ってNetflixにもHuluにも入ったことがなかったのだ。Amazon Video、Netflix、Hulu。Amazon Primeにも入っていないので。\n\nAmazon vs Google vs Apple的な現状を考えると、YouTube TVはないかなあ。PS VUEはラインアップが弱いし、Sling TVとDIRECTV NOWは観たいチャンネルが高い。Hulu with Live TVはHuluも含まれての価格だから、まあお得っちゃお得かも。とは言いつつ、せっかく減らした月額費用が$40/moで吹っ飛ぶなあ。\n\nと、いろいろもやもやした結果、とりあえずNetflixを始めてみることにした。最初にNetflixにした理由は3つ。1つは[Icarus](https://www.netflix.com/title/80168079)を観たかったこと。[Lance Armstrongのポッドキャストに監督がゲストで出ている](https://lancearmstrong.com/podcast/the-forward-episode-80-bryan-fogel-part-1)のを聞いて、これは観たいと思った。ロシアのドーピングの話なんだけどそこに至る経緯がすごい。2つ目はBlack Mirror。[Rebuild.fm](https://rebuild.fm)でたびたび話題に上るので観てみたかった。そして、これらNetflixオリジナルのものにはどうやら日本語字幕があるらしいことが決め手になった。\n\nテレビにNetflixアプリが入っているので、何もつけなくてもテレビをWifiにつなげばリモコンのNetflixボタンを押して視聴できる。さっそくIcarusを観て、サン・ジュニペロを観て、Hang the DJを観て…\n\n… House of Cardsを最初から見始めた。どハマりした。平日に夕食を終えてから2、3エピソード、休日はもっとたくさん観続けて、Season 4の途中まで来た。でもまだStranger Thingsもあるし、ドキュメンタリーも観たいのが結構あるし、それなりに映画もあるし。\n\nLive TV観てる時間なくね？\n\n[ニューヨーク市内で引っ越しの準備その3](/blog/2018-03-17_------------------3-72cd85892fd1)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-03-11_------------------1-35e3f49ceed1","date":"Sun, 11 Mar 2018 15:20:50 GMT","title":"ニューヨーク市内で引っ越しの準備その1","epoch":"1520781650","excerpt":"ニューヨーク市内で引っ越しを決めたからの続き"},"markdownBody":"\n### ニューヨーク市内で引っ越しの準備その1\n\n[ニューヨーク市内で引っ越しを決めた](/blog/2018-03-03_------------------c07c6b753d71)からの続き\n\n### My wireless history\n\nこの時点で僕は携帯電話の契約をしておらず、スマホはWifiでしか使っていなかった。ここでNYCにきてからの携帯電話の契約を振り返ってみよう。\n\nこの時代になっても電話番号は何かと必要なIDである。2013年にNYCに来て結構楽しみだったのも、AT&amp;TとVerizonどっちにしようかなーとかいうことだった。どっちのショップにも行ったんだけど、決めかねたのでとりあえずVerizonでPrepaidのフィーチャーフォンを買ってまずは電話番号をとった。この電話には結局3、4回チャージしてたぶん$150くらい払ったと思う。最初のアパートの契約作業や、その他諸々で電話がかかってくることもかけることも度々で、携帯電話があってよかったなあと思った。\n\n本物の電話番号があればGoogle Voiceのアカウントを作れる。携帯電話の番号をゲットしてすぐにGoogle Voiceのアカウントを作った。今でもこの番号が僕の電話番号だ。\n\n2013年に渡米したときに持っていた端末はSoftbankの201Mで、まだこれをSIMアンロックできていなかったので、とりあえずWifiで使いつつNexus 5が出るのを待っていた。11月にNexus 5をゲットして、Straight TalkというAT&amp;T系MVNOでとりあえず使い始め、その後Walmartで買えるT-Mobileの$30/moプランに変えて使っていた。\n\nPrepaidなのでたまに日本に1ヶ月ほど戻るときは、Refillせずに放置してまた帰宅したらRefillするようにしていたんだが、あるときふと気づいた。俺、電話もSMSも全然してなくね？\n\n朝起きて会社に行って夜帰って寝るという生活で、おまけに電話やSMSをくれる相手も全然いないとなると、携帯電話の契約を持っている理由が見当たらない。スタバはもちろん、マンハッタンではそこいらじゅうにNYCの無料Wifiのタワーがあるし、地下鉄の駅に降りれば地下鉄のWifiを使えるし、データを使えなくて困るということもない。\n\nそこで2014年の中頃に日本から戻ってきてから、Refillをせずに携帯契約をしないことにしてみた。Google Voiceには今は使えない携帯の番号が連携されていることになっているが、それでもPCブラウザーのGmail経由で電話をかけることも受けることもできる。受けることができるので、電話がかかってくることがわかっていれば、PCを開いて待機していればよい。いつかかってくるかわからない電話で、どうしても出なければならないものなどないので、それで十分だった。\n\nそれから3年ほど経過し、たまに両親がこっちにくるとかで連絡できないと困るときにStraight TalkにRechargeして1ヶ月使うくらいで、携帯の契約はないままWifiのみで生きてきた。友だちが訪ねてきたのに友だちの携帯で店を探させたりして。\n\n一番困ったのは[自転車事故で救急車で運ばれた](/blog/2017-10-02_-------------------------2d10b02652db)ときで、このときは会社にも家族にも連絡できずあーどうしようと思っていたんだけど、やっぱりERに無料Wifiがあって、結局そこまでは困らなかった。\n\n今どき手元のスマホでデータを使えない瞬間があるって最初は不安だけど、生活リズムができてしまえば、月に$50とか無駄に払っているのはバカバカしい。無駄じゃないことが多いんだろうけど、僕の場合は。\n\n閑話休題。\n\n### Tingで携帯電話再開\n\nそういうわけで携帯の契約がなかったんだが、引っ越し作業でふと気づく。前回同様、なにかと電話がかかってくるのでは？\n\n月に$50とか払うのはバカバカしいが、最近では使った分だけ払うMVNOがあって安く済ませようとがんばれば$20以内に収まるかもしれない。さっそく[宮川さん](https://rebuild.fm/)に[Tingのリファラルコード](https://z69tpj6ndek.ting.com/)を送ってもらって、TingのSIMをゲットし、Google VoiceにTingの番号を連携した。あっと[僕のコードも貼っておこう。ライトユーザーなら特にTingおすすめですよ](https://zql8fe716fn.ting.com/)。\n\n引っ越し作業が終わって結論を書くと、電話の契約は正解だった。特に後述の引っ越し屋さんとの連絡で役に立った。部屋を探しているときも電話がかかってきていることが何度かあったので、もっと早く気づいていればよかったなと思った。\n\n[ニューヨーク市内で引っ越しの準備その2](/blog/2018-03-11_------------------2-ae04f126a97f)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-03-03_------------------c07c6b753d71","date":"Sat, 03 Mar 2018 15:26:52 GMT","title":"ニューヨーク市内で引っ越しを決めた","epoch":"1520090812","excerpt":"ニューヨーク市内で引っ越し先を探したの続き"},"markdownBody":"\n### ニューヨーク市内で引っ越しを決めた\n\n[ニューヨーク市内で引っ越し先を探した](/blog/2018-03-03_-------------------6d634505e7cb)の続き\n\n### 書類クエスト\n\n数日うんうん悩んだが、まあそうそうこの金額でこの程度の物件が出てくるとも思えないし、ブルックリンに住んでみたかったし、えいどうにかなるわってことでエージェントに連絡してアプリケーションを開始することにした。\n\n今回の物件はいわゆるCondo、分譲アパートの賃貸で、部屋の持ち主が僕を受け入れてくれること以外に、ビルの管理委員会がその賃貸を承認してくれることが必要になる。部屋の持ち主との暫定的な合意ができたら、ビルの管理会社へ書類を提出する（アプリケーション）必要がある。申込書やパスポートのコピー、給与証明、直近のbank statement、昨年の確定申告書類、直近の給与明細などの通常書類以外に、3種類のreferral letterが必須で、困った。\n\n1つはbusiness referralで、現雇用主からの推薦状的なもの。会社のFounding Partnerに頼んでみたら、ふたつ返事で引き受けてくれてほっとした。\n\n2つ目はpersonal (character) referenceで、雇用以外の関係者、ようするに友人からの推薦状。友だちのいない身としては誰に頼めばいいのか困ったあげく、前社のCFOにおそるおそる（僕たちまだ友達ですよね…）お願いしてみたら、これもふたつ返事で引き受けてくれた。ああよかった。ありがとうございます。\n\n3つ目がcurrent landlordからの推薦状。てことは今の大家さんに引っ越すよって言わないといけないのか。でもアプリケーションがうまく行かなかったら残留の可能性もあって、そうなると「へっへっへ家が見つからないんだろう、家賃をあげてやるぞ」とか言われないかとか、（今となってみれば）若干余計な心配をした。おそるおそるメールしてみたら、これもふたつ返事で引き受けてくれた。わりとよくあることなんですねたぶん。\n\nそういうわけで心配したわりに数日であっという間に書類は集まった。次はお金。\n\n登録手数料および引っ越し諸経費（これらはビルの管理会社へ）と、security deposit（敷金）1ヶ月分および最初の月の家賃（これらは大家さんへ）を、銀行小切手で用意する。（この時点での）現在の部屋を契約するときにこれが大変だったのだが [1]、5年住んでそれなりに慎ましく生きている今となっては何の問題もなく、銀行へ出かけてお金をおろす感覚で窓口に立ち「銀行小切手でちょうだい」っていうだけだった。\n\n小切手を郵送するのはなんとなくためらわれるし、通勤経路の途中だったこともあって、不動産屋さんに直に出向いて書類一切を手渡しして、アプリケーションがはじまった。\n\n[1] 前回はSSNもないし銀行口座は開いたばかりだし、その上口座の中身を急いで作るためにATMで日本の口座から下ろした金を、同じATMでこっちの口座に入れるということを（しかも限度額があるので3日にわたって）したため、口座にフラグが立ったらしく、残高を作って銀行小切手をもらいにいっても「審査が必要」って言われて、残高はあるのにしばらく小切手を切ってもらえなくて、Airbnbの終わりも迫っていて、ホームレスの危機に立ったのだった。銀行窓口のマネージャー的な人に、ホームレスになっちゃうんですよマジで、お願いしますよ、俺H-1Bゲットして日本から来たプログラマーだしマネーロンダリングとかいう額でもないですよね、とか半泣きで訴えて、やっと3日経った夕方にもらえたのでした。\n\n### 契約打ち切り交渉\n\nそして数日後、アプリケーションは無事受け入れられ、晴れて引っ越しすることに決定した。契約は2月15日から。つまり、このままだと2月から5月までの3ヶ月間、あのNYCに部屋を2つ賃貸していることになる。何このセレブ感。\n\n前述のとおり、基本的には契約期間中に家を出ることはできないので、大家さんとの合意を形成する必要がある。ググってみても、1ヶ月かぶるのは当たり前、あとはうまく時期を選んで探せよGood luck的な書き込みしか見つからず。どうせだめなんだろうなあと思いながら、おそるおそるメールで、referralのお礼と2月に引っ越すので契約を打ち切れないかと打診してみた。\n\nそしたら返事は思いの外前向きで、1ヶ月短縮に同意、さらに次の人が見つかればもっと短縮してもよいということに。この大家さんほんとにのんびりしてるというかガツガツ感が全然ない人で、なんだかとても助かったなあ。こっちの思い込みかもしれないけど、こんなにさらっとうまくいくことはめったにないんだろうなあと思った。Confrontationがとにかくイヤなので、言わないで済むなら言わないで済ませたいなあってつい思ってしまうんだけど、何でも言ってみるもんだなあというか、僕はもっと人を信じるべきなんじゃないかと思った。まあ次回もこうなるかはわからないし、次回引っ越しするときはもう少し期間を考慮しようとも思った。\n\n[ニューヨーク市内で引っ越しの準備その1](/blog/2018-03-11_------------------1-35e3f49ceed1)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-03-03_-------------------6d634505e7cb","date":"Sat, 03 Mar 2018 14:29:44 GMT","title":"ニューヨーク市内で引っ越し先を探した","epoch":"1520087384","excerpt":"ニューヨーク市内で引っ越しの続き"},"markdownBody":"\n### ニューヨーク市内で引っ越し先を探した\n\n[ニューヨーク市内で引っ越し](/blog/2018-02-11_--------------6324113a2d34)の続き。\n\n### 南に向いてる窓を\n\n最初の物件を見に行ったのが土曜日で、そこで自分たちの中で固めた必須条件を満たす物件をさらに探してみた。もちろんそれなりによさげな物件はそこそこ見つかる。\n\nだがしかし、写真を見ただけでは安心できないのがStreet EasyやTrulia。同じビルの違う部屋の写真が平気で載っているし、説明に書いてある内容が実は古かったり単に間違っていたりすることは当然のようにある。\n\nそこでまずはFloor planから部屋の向きを確認する。ところがFloor planには方角が記されていないことも多いし、Floor planが違う部屋のものであることも往往にしてある。なのでFloor planがあってもうのみにせず、次に住所をGoogle Mapsとストリートビューで確認する。小さいアパートやタウンハウスなら、窓が南に向いて付いているかどうか、ストリートビューが撮影された時点で目の前に大きな建物はないか、くらいはわかる。\n\nStreet Easyには過去にその部屋がRentされたときの情報も載っていることがあって、過去のリスティングでは違う写真やFloor planを使っていることもあるので、そこらへんも調べる。また、大きな建物の場合は建物が全方向を向いていたりするので、目当ての部屋が南に向いているかどうか調べるのが難しいこともある。大きい建物の場合、ビル自体のWebサイトがあることが多いので、そこで部屋番号と向きを照らし合わせたり、自分の目当ての部屋にFloor planがない場合は違う階の同じ部屋番号のFloor planを探したりする。\n\n東京23区あたりでは説明に南向き日当たり良好って書くのは（ほんとにそうなら）当たり前だったと思うが、こっちでは必ずしもそこがウリのポイントではないのか、Southern exposure, lots of sunlightと書いてあるものもあれば（うのみにはできない）、実際はそうなのに書いてないこともある。特にマンハッタンでは日当たりは贅沢品なので、日当たり重視で探すならこういう手間暇をかけることになる。もちろん出かけていって見てみるのが一番いいのだが、ハズレ物件をいくつも見るのはしんどい。\n\n### 近い物件発見\n\n同じ土曜日の夜に1つ物件をStreet Easyで発見した。立地は以前に日本から同僚がNYCに来たときに彼らが使ったAirbnb物件のすぐ近くで、少しだけ土地勘がある場所。アパートは、写真を見る限り部屋は明るそう。\n\nさっそくFloor planを調べ、ビルのWebサイトで部屋の場所や向きを調べる。幸いビルのサイトに階ごとの部屋の配置と向きが出ていたので、それとFloor planを照らし合わせて南に向いて窓があること、南側は道ではなく中庭なのだが、7階建ての6階で中庭部分は抜けているのでおそらく日はだいぶ入るだろうことを確認して、内覧のアポをStreet Easyから送った。\n\n夜中に送って翌日の日曜日を希望したので、無理かなあと思っていたんだが、日曜日の午前に返事がきて、夕方4時45分に待ち合わせて見せてもらうことに。1月の夕方5時だともうだいぶ暗いので、日当たりは見られないかなあと思いながら、街並み的なものも確認するために2時過ぎに出かける。\n\n最寄り駅まで地下鉄で行って、そこからぶらっと歩きつつ街の様子を確認する。Wholefoodsは今のとこより近いし、さらに近いところにもう1件スーパーがある。カフェだのバーだのレストランだのも結構あるし、人通りも多い。アパートのある建物の周辺は日本語でいうタワマンがにょきっと生えていて、再開発だなあって感じ。\n\n…とかほんとはもっとぶらっとしたかったのだが、なんせ3時過ぎると寒さで辛くなるので、4時半ごろにはビルに入ってロビーのソファで不動産エージェントを待つ。\n\n5時過ぎに現れたので、部屋を見る。ギリ太陽光がある状態でこんだけの明るさならまあいいかなあ。部屋も今より広いなあ。窓は今より大きい。中庭経由でお向かいさんの窓と向かい合っているのが気になるが、洗濯機が部屋に付いてる！外にも各階に大型の洗濯機がある。ゴミ捨ても各階でできる。ジムやコンシェルジェなどのありがちなアメニティもある。15分ほどで見て回って内覧は終え、アプリケーションのための書類をとりあえず送ってもらうことにした。\n\n### 家賃交渉\n\n場所もいいしアパートもきれいだしほぼ完璧に南向きで日が入るしということで条件はクリアしていた上に、すでにめんどくさくなってきており、いやーこれでいいっしょーって気分になっていたんだけど、妻にとっては初のブルックリンで、中庭を挟んでいるとはいえ目の前に他人の目がある感じで、えーどうかなあ、うーんうーんとか言いながらその後一週間、その間も部屋をいろいろ探して1件は見てみたものの、特によいものにはあたらない。\n\nそんな中不動産エージェントからメールが来て、「こないだ送ったメール見てくれた？」あ、見てなかった。だいぶめんどくさくなってきていたし、ここはいっちょダメ元でという気分で、$100下げてくれと家賃交渉をしてみることにした。ダメなら断る理由にもなるし。\n\nエージェントからの返事は「交渉するなら、こっちも何か、たとえば契約期間を長くするとかしたら？」ということで、当初12ヶ月のところを18ヶ月コミットするから、$100下げてくれと言ってみる。これは、14とか16とか言えばよかったと後悔した。\n\nそしたらなんと、家賃下げOKの返事が。やばい。どうしよう。\n\nこちらの賃貸契約は、東京23区あたりの賃貸では当たり前の、別に契約期間に関係なく2ヶ月前連絡でいつでも出られるというものではなく、どんな事情であれ双方の合意がなければ契約期間中の家賃支払いは必須なので、出なきゃならない場合は例えば自力で代わりに住む人を見つけてこいとか、住まなくても構わないけど家賃は払えよとかそういうことになるので、コミットメントなのだ。\n\n（その時点で）現在住んでいるアパートの契約が5月半ばまであるのに今回のこのアパートは2月半ばからの契約になってしまい、3ヶ月分も重複してしまう。どうしよう。\n\n[ニューヨーク市内で引っ越しを決めた](/blog/2018-03-03_------------------c07c6b753d71)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2018-02-11_--------------6324113a2d34","date":"Sun, 11 Feb 2018 15:18:37 GMT","title":"ニューヨーク市内で引っ越し","epoch":"1518362317","excerpt":"ニューヨークであれそれをしたシリーズ。"},"markdownBody":"\n### ニューヨーク市内で引っ越し\n\nニューヨークであれそれをしたシリーズ。\n\n### 序\n\n春が近づくといつも憂鬱になるのが、大家さんと家賃の話をしなければならないこと。5年前にニューヨークに到着して以来住み続けている今のアパートは、アメニティもロケーションもよくて、5年も住んでれば周囲の土地勘みたいなものも定まることもあって居心地がとてもよいんだけど、毎年毎年少しずつあがっていく家賃がもはや限界だなあ、と思い始めていた。\n\nそういうわけで、年初から重い腰をあげて部屋探しを始めた。当初は、とはいえ5月まで部屋の契約があって原則的に突然契約を終わらせることはできないことになっているので、まあ物件を見る練習ってことでいいかなあくらいの軽いノリ、のつもりで。\n\n### 諸条件\n\n今のアパートはマンハッタンのFinancial Districtにあるのだが、引っ越しをする以上家賃を下げることは必須の条件。そうなると、ほぼ必然的にBrooklynやQueensなどへの移転ということになる。オフィスがBrooklynのGowanusにあるので、Queensからだと遠いかなということで、主にBrooklynで探すことに。\n\n場所以外の条件としては、買い物するスーパーが徒歩圏にあること、夜はともかく日中はそれなりに人の往来があること、でもうるさくないこと、洗濯をアパート内で（部屋内でとは言っていない）できること、部屋に自然光がそれなりの時間入ること、くらいかなあ。ジムとかドアマンとか配達の集積とかのアメニティはあきらめるつもり。\n\n### 手段\n\n住みたい場所が決まっているなら、東京23区あたりと同様に不動産屋さんに直接でかけて、大まかな条件を言って物件をいくつか見せてもらう…なんてこともできなくはないんだろうけど、東京23区あたりとは違って、同じ道でも数ブロック移動すればがらっと雰囲気が変わってしまうのがこちらの街。距離的に千駄ヶ谷と錦糸町くらいの違いかな？みたいなつもりで行くと、実は千駄ヶ谷と千葉県流山市くらい違う。そういう土地勘は全然ないので、まずは物件を探してみて、そこを見に行くことを続けて土地勘を養いつつ、と思っていた。\n\n物件探しのツールは[Street Easy](https://streeteasy.com/rentals)と[Trulia](https://www.trulia.com/)。Street Easyのほうがサイトやアプリが使いやすいが、TruliaにはCrime MapとかStreet Easyにはない機能がある。ちょっとした時間にアプリやサイトで、大体の金額や場所で検索して、写真で見た目が気に入ったらお気に入りに保存して、夜に家でその中身を検討する。\n\n当たり前だが、今検索して出ているのは、現在市場に出ているか近々市場に出る物件であって、1ヶ月後にはまた違う物件が出るわけだが、手続きだなんだを考慮すると、そんなに練習時間もないよなあ、とか思って、ある週末にさっそく見つけた物件を1件見に行った。場所はGreenwoodというPark SlopeとGowanusに隣接したエリア。通勤至近、お家賃安め。Wholefoodsには…自転車で行く感じ。[Industry CityというモールにSunrise Mart（日本食スーパー）が入るらしい](https://ny.eater.com/2017/10/20/16504268/japan-village-industry-city)、などの情報からそこそこいいんじゃないか？的な。\n\n### 物件内覧その1\n\nStreet EasyでOpen Houseの予定が出ていたので、そこから行きたい時間を指定してメールを送る。エージェントが割とすぐに返信してきて、現地で待ち合わせをすることに。\n\n当日、結構寒い中外で待っていると、中からエージェントが出てきてさっそく部屋へ。ビルは新築で、部屋も過去の入居者なし。1ベッド1バス、お家賃$2,400。バルコニーがあってバーベキューグリルが付いてる！屋上にもバーベキューエリアがあり、駐輪場完備、エアコンは三菱、などなど、まーちょっと狭いかな？っていうのを除けば、部屋自体は相当良い。\n\nだがしかし、難点が2点。まず部屋が北東向きで日があまり入らない感じ。暗くはないんだが、日が入ってくる感じでもない。バルコニーがあるのはいいんだけどねえ。日中部屋にいることが多いとやっぱり部屋の居心地は大事。そもそも今の部屋も当初の予定より高かったけど、どうせひきこもるんだし日が当たって外を見渡せるのはいいよなあと思って選んだんだった。\n\n2点目は街。とにかく古いタウンハウスが並んでいる昔の住宅地で、昔ながらの商店とちょっとしたスーパーがあるものの、日中でもあんまり人は歩いていない。[Industry City](https://industrycity.com/)まで歩いていけっていうのも、距離というよりは近所の不安感という意味で、辛そう。おそらくあと2年もすれば開発されて人口が増えるんだろうし、自分一人ならまあ問題はないんだけど、結構な重点項目にバツがついた。というか、ここを見てみてほんとに重視する項目はなんなのかがはっきりした感じ。もちろんこれらの難点があるからこそ、家賃がそれなりの範囲だってことなんだけども。\n\nというわけで、パス。まあ、まだ始めたばかりだし、物件はまだあるさ。[ニューヨーク市内で引っ越し先を探した](/blog/2018-03-03_-------------------6d634505e7cb)に続く。\n\n"},{"frontmatter":{"slug":"/blog/2017-12-17_----------13490d2fdef1","date":"Sun, 17 Dec 2017 17:00:57 GMT","title":"自転車事故のその後","epoch":"1513530057","excerpt":"ニューヨークで自転車事故にあって、鎖骨を折って、手術を受けてから2ヶ月。術後の経過は良好で、2度の通院でも特に問題なし。ただ、骨をつないでいる金属板の存在を日々感じるので、違和感が常に左肩にまとわりついている。"},"markdownBody":"\n### 自転車事故のその後\n\n[ニューヨークで自転車事故にあって](/blog/2017-10-02_-------------------------2d10b02652db)、[鎖骨を折って](/blog/2017-10-02_-----------------------39bae6cabb2c)、[手術を受けてから](/blog/2017-10-02_-------------------------d8cb3ae7d810)2ヶ月。術後の経過は良好で、2度の通院でも特に問題なし。ただ、骨をつないでいる金属板の存在を日々感じるので、違和感が常に左肩にまとわりついている。\n\n最終的に自分の保険のOut of pocket maxである$6,000に達したので、（この1年については）$6,000以上の自腹負担はない、らしい。この国のこと、まだ油断はできないが、ひとまずそういうこと。\n\nこれについては保険会社のカスタマーサポートに連絡を入れて、2度確かめた。というのも、外科手術の費用として病院に払う分だけで$5,300で、外科医師の施術費用$1,400と合わせただけでOut of pocket maxに達するのは明らかで、でもどういうわけか保険会社のWebサイトを見ても、いつまでたってもこの$5,300のClaimが確定しない。これが確定しないので、Out of pocket maxに達していないという状態のまま、やれX-Ray$25だ、やれ医師の診察$60だと細かい金額の請求書が積み上がっていって、いやいや、これらは払わなくていいはずだろ、俺はもう$5,300も病院に払っていて、$1,400の外科医師費用を払ったらオーバーしちゃうんだから。\n\n保険会社からの最初の返答は、まだ処理中ってことだったので、いや通常2週間かかるって設定になっててすでに3週間を過ぎていること、この金額が確定すればOut of pocket maxを越えることが確実なのに、他の細かいのは最近の日にちのものも含めてどんどん確定して請求されているのに、一番大きい金額のものをいつまでも確定しないのはunfairであること、などを再度主張したら、その返答として突然の計算結果がやってきて、めでたく$6,000に収まるような金額に設定された。\n\nそれを受けてさらに1週間後、手術当日に先払いした$5,300のうちの$1,800くらいがrefundされたので、外科医の施術費用$1,400を支払った。手術からすでに2ヶ月以上経過していた。\n\nそうなんです、日本の方は疑問に思ったかもしれませんが、こっちでは外科医と外科医が手術を行う病院の設備利用料は別建ての会計です。病院にまとめて払うわけじゃないんです。麻酔科もレントゲンも全部別。もっとも支払う作業自体はNYUのWebsサイトでできますが。\n\nやっと一息ついたかなと思ったら、今度は救急隊から救急車の請求$700が2ヶ月以上経ってから来たんだけど、こちらは請求書に書かれていることに従って保険の番号などをWebで入力したら、数日後に$0に確定していました（すでにOut of pocket maxをオーバーしているから）。\n\n事故って以来、加害者側とかかった金額について都度連絡をメールでしていて返信もちゃんと来ていたので、$6,000が確定したので、それに上乗せして$7,000でどうだ、という請求をした。実は一番最初、まだ金額が確定していないときに、Out of pocket maxを信じて$7,000の請求を一度したんだけど、そのときはまあ確定してからにしませんかと言われて、まあそうするかってことにしていたので。\n\nまあ案の定というか、「ようやく奨学金を返し終えたばかりでやっといくばくかの貯金ができたばかりである」「実は春先に子供が生まれる予定」「そもそも事故を自分なりに検証したんだが、原因がどちらにあるかはambiguousだと思う」と。\n\n正直だいぶ迷った。彼はその場から逃げてしまうこともできたはずで、そうされていたら多分見つけられなかったと思う。しかし彼は逃げず、警察官（交通整理担当でたまたまそこにいただけで、事故の処理のために呼ばれた人ではない）に対してきちんと自分の身分を明かして調書にも名前と住所が載っている（はず。見てない）。メールを送ったら心配している旨の返信もよこしたし、そもそも連絡先をきちんと置いていった。この正直者を、その正直さゆえにpunishすることが正しいのか、と。\n\nただ、やっぱりどう思い返しても、あのとき彼が逆走さえしなければ俺の肩に異物が入ることはなかったし、彼だって無駄にお金を使うこともなかった。逆走されて「半々」というのは納得がいかない。\n\nということで、「わかった。$5,000でどうだ。別にこちらの責任を少し認めるわけじゃない。この不愉快な会話を早く終わらせていつもの生活に戻りたいだけだ。逆走しておいて半々というのはどう考えてもunfairだ。」という返信をした。\n\nするとこれも案の定、「$5,000で手を打とう。前のメールで書いた通り今手元にその金はないから、5回分割にさせてくれ。それと、この件でこの後法的措置をとってその結果がこちらの（注：加害者側の）不利に働いた場合、この$5,000は返金すると約束してほしい。」ま、そうですよね、ってことで今絶賛返金受け取り中。\n\n$1,000も出費することになったし、今後、例えば肩に入っている金属板をとる手術を受けるなら、その費用は完全に自腹だし、自転車だって見た目は平気だけど乗ってみたらどっか壊れているかもしれないし（まだ再開できてない）、弁護士さんが聞いたらアホって言うだろうけど、もう精神的に、なんというか、めんどくさい。ああ、正直者が罰を受けていいのかとか、ああでもじゃあ被害者たる自分は救われないのか、とかいうことを考え続けて毎日を過ごしたくない。\n\nそういうことで、この項は誰の参考にもならないと思いますが、一応記録として残しておくことにしました。おしまい。\n\n"},{"frontmatter":{"slug":"/blog/2017-12-09_Essential-PH-1-----1---5d3b0757a03c","date":"Sat, 09 Dec 2017 19:39:52 GMT","title":"Essential PH-1 を買って1週間","epoch":"1512848392","excerpt":"Essential PH-1を買って1週間たちました。$300未満（購入したときのディール）でSnapdragon 835なAndroidの使い勝手とは。"},"markdownBody":"\n### Essential PH-1 を買って1週間\n\n[Essential PH-1を買って](/blog/2017-12-09_Essential-PH-1------18f33b97acd2)1週間たちました。$300未満（購入したときのディール）でSnapdragon 835なAndroidの使い勝手とは。\n\nチタンの筐体は、プラスチック感満載のNexus 5Xと違って何か高級品を持っているような感覚。テーブルにもそっと置いてしまう。手に持つと重いのは重いんだけど、「手に収まっている」感じでまあ、長時間持ち上げて撮影とかでもない限り問題ないです。ケースは使っていませんが、えらいすべります。まあ早晩落としますねこりゃ。\n\nまー2年ぶりの新車だし、速く感じますよねそりゃ。カメラの起動が遅いって文句を見かけた気がするんだけど、普通の写真については、アイコンに触って撮影できるようになるまで1秒以内な気がします。最初はどうだったのか知らないけど、最初がそこまでひどかったんなら、すごく改善されたんだと思います。\n\n[Googleカメラ](https://www.xda-developers.com/google-camera-hdr-customization-raw-support/)も入れて、同じシーンを両方撮って見比べてるんだけど、うーん、何か違う？全然変わらない気がします。Essentialのカメラアプリは、HDRがデフォルトではオフになっていて、少しでも暗いときにオンにすると「明るいところで使う機能だよ」ってToastが出るんだけど、無視してHDRで撮ってこれ。特に問題ないと思うんだけど。\n\n![](/images/1*PhV2X0MgOJ4ashL1IG4nhw.jpeg)\n\n![左 Essenitial 右 Google](/images/1*t520QxeBgucs_5Guwo6kpA.jpeg)\n\n![](/images/1*pXDhLXoZdtpYKv8ULRGaQA.jpeg)\n\n![左Essential 右Google](/images/1*ga5PExdzMrVkb6toa08vwg.jpeg)\n\n光量が少ないところでたぶんGoogleカメラの方がよい。ということはソフトウェアでどうにかなるってことだから、アップデートに期待しつつ、まあもう少しいろいろ撮影してみないと。もともと写真のクオリティにそこまでこだわりのある人間ではないし、イラっとするような遅さもないので、今のところ問題なし。\n\n[今回のお得ディールで](/blog/2017-12-09_Essential-PH-1------18f33b97acd2)ついてきた360度カメラ。世界最小らしいです。360動画も撮れます。まあ面白いよねぇ…。\n\n11月のカメラアップデートで、iPhone 7 Plus以降のハイエンド機では必須になった感のある[ポートレートモード](https://www.engadget.com/2017/11/30/essential-phone-portrait-mode/)が増えました。\n\nうん、いいんじゃないでしょうか？あれやこれと比較してどうかはわからないけど、満足してます。\n\niPhone Xのあの部分のことは「のっち」と呼ぶそうなので、Essential PH-1のこの部分のことは「ゆかちゃん」と呼ぶことにします。\n\n見ている限り、アプリの「ゆかちゃん」対応は3段階あります。一番ダメなのが、ステータスバーの後ろに行ってしまってオプションメニューやナビゲーションドロワーを開けない（または開くのが難しい）アプリ。\n\n![ ](/images/1*0hFDnVStKxvJn2tLjo9Rlw.jpeg)\n\n![対応してないアプリの例。左Feedly。右Yahoo Fantasy Sports。我がチームの成績はひどい。](/images/1*l6qgEJUx-jaQIWSdtwrB7g.jpeg)\n\n松竹梅なら竹のものがたぶん最も多くて、プラットフォーム提供のウィジェットに変な手を入れていないとこうなるんだろうと想像。TwitterやInstagram。\n\n![Instagramは普通のアプリ](/images/1*IA42BLO9R-sjAc2rXZS6_A.jpeg)\n\n松レベルなのがGoogle Maps、City Mapper、Google Play 系（StoreとかMusicとか）。かしゆかの裏まできっちり利用しつつ、ツールバーはかしゆかの下に表示して操作できます。\n\n![Google Maps。ゆかちゃんの横にYankee Stadium。](/images/1*RC6IalFEwaB_WjnA8G5_xQ.jpeg)\n\n一番ダメなタイプのアプリがなぜああなるのかの原因はなんとなくわかっているので、別途コードを書いて検証しようと思います。\n\n地下鉄で横に座っている人のiPhone Xをガン見したときに、「のっち」は不自然に邪魔だろーこれーとかおもったんだけど、「ゆかちゃん」はまったく邪魔じゃないというか特に違和感ないです。「のっち」より細身だし。\n\n3.5mmヘッドホンジャックはありません。USB-Cからの変換ドングルが付属してます。僕はこれにあわせて[Refurbishedで$189だったSony WI-1000X](https://www.amazon.com/Sony-WI-1000X-Cancelling-Headphones-Refurbished/dp/B0774YYR2Y/)も買いました。この[SecondipityってSeller](https://www.amazon.com/sp?_encoding=UTF8&amp;asin=&amp;isAmazonFulfilled=&amp;isCBA=&amp;marketplaceID=ATVPDKIKX0DER&amp;orderID=&amp;seller=A3FG7YW6YCE8DR&amp;tab=&amp;vasStoreID=)は出荷も早いしSony製品を扱っていておすすめ。\n\nNFCペアリングをやってみたくて、でもNFCが本体最下部右側に配置されていることがようやく判明するまで手こずったけど、あとは問題なし。ただ、Oreoが来るまではLDACもaptXもないので、Oreo早くこないかなあ。\n\nで、そのOSアップデートなんですが、開封直後に4回のシステムアップデートを受けて、使い始めた翌日にさらにDecember Updateがきました。Pixelより2ヶ月連続で早く提供できたとかなんとか。\n\n```\n<a href=\"https://twitter.com/essential/status/938111279188787200\"></a>\n```\n\n```\n<a href=\"https://twitter.com/ryanminnick/status/938119280478052352\"></a>\n```\n\n少なくとも現状ではEssentialのソフトウェアチームはかなり積極的にアップデートを行っていて、Oreoのベータも始まってるし、この調子が続くならGoogle謹製でなくても問題ないなあ。\n\n```\n<a href=\"https://twitter.com/essential/status/936275149736763392\"></a>\n```\n\n[Essentialの社是](https://www.essential.com/blog/why-I-started-essential)?のようなところでAndy Rubinが書いている「Devices shouldn’t become outdated every year. They should evolve with you.」を信じるなら、この調子は続いていくはずなんだけど。\n\nAndy RubinがGoogleを辞めた理由みたいなことが今話題になっていて、それがどこまで事実なのかこの時点ではよくわかっていないんだけど、Andy RubinがいなくなるようなことがあるとEssential社の存在意義みたいなのは急に方向性が変わっていってしまうと思うので、綱渡り感はありますね。[Amazonからの出資を受けた](https://techcrunch.com/2017/08/09/essential-confirms-300m-funding-amazon-and-best-buy-retail-availability/)ので、次世代Fire Phoneになっちゃうんじゃないかみたいな話もあったし。\n\n追記：Andy Rubinは職場復帰してました。\n\n```\n<a href=\"https://twitter.com/atsushieno/status/939582678751260672\"></a>\n```\n\n### まとめ\n\n$499で十分お買い得な上に、[実際に入手した額](/blog/2017-12-09_Essential-PH-1------18f33b97acd2)のことを考えると、買ってよかったとしか言いようがない。もう2、3セット買って親兄弟に配ればよかった。まあまだ使い始めて一週間なので、その後どうなるかはまた書くかもしれないですが。ちなみにUSではSprintがキャリアで販売しているので、Softbankワンチャン。。ないのかな。\n\n"},{"frontmatter":{"slug":"/blog/2017-12-09_Essential-PH-1------18f33b97acd2","date":"Sat, 09 Dec 2017 15:55:49 GMT","title":"Essential PH-1 を買った","epoch":"1512834949","excerpt":"Cyber Monday のディールで Essential PH-1 を $300 未満で買いました。"},"markdownBody":"\n### Essential PH-1 を買った\n\n2016年の初代Pixelを（主に価格的な理由で）スキップして、その分2017年のPixel 2はXLを買っちゃうぞーと思いながら、Nexus 5Xを使い続けていた。その間、[Nexus 5Xのブートループ問題に遭遇](/blog/2017-07-17_Nexus-5X-------1bb1d9c07087)して交換してもらったけどもっさり感は特に改善されず、Black Fridayの何か（値下げとかオマケとか）を期待して11月末までがんばって、それでPixel 2 XLにしようと思っていた。\n\nところが先日の[自転車事故](/blog/2017-10-02_-------------------------2d10b02652db)のせいで計画にない多額の出費を迫られ、計画は完全に頓挫。事故の加害者側との示談交渉もはかどらず、Nexus 5Xをトレードインしても$800以上の出費になると思われる電話を買うような金はない！ってことに。ため息。\n\n交換後のNexus 5Xでもう1年ひっぱるのかなあ、と意気消沈していたところに、[EssentialがPH-1を$200値下げした](http://www.itmedia.co.jp/news/articles/1710/23/news057.html)というニュースが。ベゼルレスでチタン筐体でAndy Rubinという鳴り物入りで登場して、でもカメラが遅いとか写真の質が低いなどの理由でパッとしない感じだったあれだけど、Snapdragon 835で128GBでデュアルカメラってスペックで$499ってお買い得じゃね？その後11月に入り、Black Fridayまだかなーと思っていた矢先には、なんとさらに[$50値下げしたというニュース](https://www.droid-life.com/2017/11/06/deal-essential-phone-drops-another-50-best-buy-just-449/)が。\n\n調べてみると、カメラ問題はその後のソフトウェア アップデートでだいぶ改善されているらしく、さらに[勇者がGoogleのNexus/Pixelカメラアプリを移植していて](https://www.xda-developers.com/google-camera-hdr-customization-raw-support/)それを使えばだいぶよい写真が取れるという評判もあり、これは！\n\nBlack Fridayの週の水曜日。あーどうしようかなー買っちゃおうかなーなんて思いながらAmazon.comの商品ページを見ていてふと気づく。マーケットプレイスに中古が出品されていて、お値段なんと$389。見に行くと、出品者はAmazon Warehouse Dealsで、つまり返品か倉庫内での傷物であり、問題があったときの返品対応とかは、Amazon.comなので心配が少ない。これはお得！ポチってしまった。まあBlack Friday期間中のFree Shippingなんで、2週間後には出荷されるかなあ、くらいの感じで待ち始める。\n\n日付は進んで、翌週月曜日。Cyber Mondayってやつ。ニュースを眺めていたら[Essential PH-1が$399になった](https://www.theverge.com/2017/11/27/16704228/essential-phone-cyber-monday-deal-399-low-price)という記事に遭遇。ええっ、$10プラスで新品か。どうする、ランチ1回分だぞ？と思いながらAmazon.comのページを見に行くと表記は$499。あ、終わっちゃった？と思って記事を見直したら、360カメラ付きで$399と書いてある。で、そっちのBundle Dealを選んだら本当に$399。これは！\n\n先日の注文は幸いまだ出荷されていない。マーケットプレイスの出品物は、通常のAmazonとは違ってすぐに出荷準備中になってキャンセルするのがちょっと難しいんだけど、Amazon Warehouse DealsはAmazonなので、問題なし。キャンセルして、$399の360カメラ付きを改めて購入。つまり、2017年のハイエンド仕様のAndroid電話を$290で買ったってことですよ。カメラが多少ダメだろうと、会社の先行きが若干不透明だろうと、これはお得すぎるでしょ。\n\n![Yes they were.](/images/1*Gij1N5kDwkpSx5dhnVjM0A.png)\n\n1週間の待ち時間を経て、12月3日に無事届きました。[1週間後の使用感](/blog/2017-12-09_Essential-PH-1-----1---5d3b0757a03c)へ続く。\n\n```\n<a href=\"https://mobile.twitter.com/fumiakiy/status/937836294872420352\"></a>\n```\n\n"},{"frontmatter":{"slug":"/blog/2017-10-02_-------------------------d8cb3ae7d810","date":"Mon, 02 Oct 2017 03:57:32 GMT","title":"ニューヨークで自転車事故にあって外科手術を受けた","epoch":"1506916652","excerpt":"この話はニューヨークで自転車事故にあって鎖骨を折ったの続きです。"},"markdownBody":"\n### ニューヨークで自転車事故にあって外科手術を受けた\n\nこの話は[ニューヨークで自転車事故にあって鎖骨を折った](/blog/2017-10-02_-----------------------39bae6cabb2c)の続きです。\n\n### 手術当日\n\n前日午前0時以降は飲食一切禁止ということだし、朝6時には起きないとだしということで、前日はとっとと寝て金曜日の朝。6時半ごろ出発して地下鉄に乗って歩いて NYU Langone Tisch Hospital に到着。でかい。NYU すごい。NYU の病院施設だけでこんなにたくさんあるのに、NYU には他にもキャンパスがたくさんあるし、すごい。桁が違う。\n\n教わった通りに4階の受付へ進み、8時にはなっていなかったけど受付に通されて、またしても同意書などにサインをして、$5,300をクレジットカードで払う。これ、渡米1年目とかだったらもしかして信用枠がなかったかもしれないし、そうしたらどうしてたんだろう、昨日のうちに銀行で小切手を作ってもらっていたんだろうか。\n\n（後で見たんだけど）領収書をみると、手術の総額は$16,000を超えていた。Deductible $2,000 を引いた$ 14,000程度の CoPay 20% + $2,000（Deductibleは払わないといけないから）ということで、プラスSurchargeとかで$5,300的な値段でした。\n\nここで妻を待合室に置いて、診察室的なところへ通され、20分くらい待たされた後に、看護師から服を脱いで、身体を拭いて診察着に着替えて、着替えや靴をこの（NYU ロゴの）袋に入れてと指示があり、済ませる。済ませて、看護師が一連の質問（最後にものを食べたのはいつだとか、酒飲んでないかなど）をして、診察室へ妻が登場。\n\n次に今回手術を行う医師についているresidentだという2人の医師が登場し（2人とも初めて会う人）、手術のリスク事項について説明する。とはいえ、手術の内容は普通のもので、この医師の腕もよいから大丈夫、はははっと。それと、輸血が必要になったときに輸血を受けるつもりがあるかも聞かれた。もうぜひとも輸血してくれと答えた。そして、すべての説明を受けた旨を記したところにサインをするところで、ハタと気づく。\n\n### 通訳システムMartti\n\n前日の電話のとき、この説明時に通訳を必要とするかどうか聞かれて、医学用語がわからないことが多いのでお願いしたいがいくらかかるかと聞いたら「いや、無料ですよ」と言われたので、お願いしていたのだった。それが記録にあったので、よし通訳を呼べ、と。\n\nそして登場したのが、車輪付きで移動可能なデカい自撮り棒の先にiPadがくっついているスタイルの[Martti](https://www.martti.us/)というもの。とはいえただのSkypeではなく、お互いのコード番号やアクセスコードやらをやりとりして初めてつながるような、患者のプライバシーやその他セキュリティに配慮した特化型システムのようだった。\n\n日本人と思われる女性が音声越しに（人によってはビデオ通話なんだけど、日本人はほとんど音声だね、と看護師が言っていた）通訳をしてくれたのだが、結論としては僕には不要だった。というのは、医師はもちろんわかりやすい言葉で説明してくれるし、意味のわからない言葉は意味がわからないと言えばいいだけなので。手元にスマホがあれば、単語を入力したり入力してもらったりして、調べることもできるし。ただ、英語がほんとうにできない人にとっては安心だろうし、そもそも日本語だけじゃないいろんな言語に対応しているらしきシステムで、これはほんとにやっぱりニューヨークだなあと思った。英語を話せなかったために適切な医療を受けられなかったという状況を極力減らすための努力なんだなあと。システム自体は別の会社が提供しているぽいから、お金はかかっているはずなんだけど、通訳が必要という理由で医療費がさらに増えたりはしないという、通訳はluxuryではないということか。\n\n考えてみたら、救急車に乗って最初に隊員に聞かれたのも、救急病院で最初に聞かれたのも、「Do you speak English」だった。アメリカの他の街で病院に行ったことはないので比較はできないけれども、ニューヨークが多様性にあふれる都市であることを良いことだと思っていて、それを維持するために努力をしている街なんだなあと大げさながら思った。東京オリンピックに向けた国際化の取り組みは…とか少し思う。\n\n### 手術準備…\n\n外科手術の同意を終えて、次は麻酔の同意があるはずなんだが、看護師が「手術室の機器に問題があって、ちょっとおしてるから1時間くらいかかると思う」と。1時間くらいボーっと待っていると、木曜日にオフィスで会った執刀医が登場し、「やあやあ、ちょっと機械に問題があってねえ、あと2時間くらいかかりそう。待っててくれよ。同意書はOK？OKね。よしじゃあ印をつけて、と」とかいって、患部になにやら印をつけ、手の親指にはニコちゃんマークを書いて去って行った。\n\nこの部屋にはテレビがあって、なんでも見放題だったんだけどあまり見る気もしないし、CNNをぼーっと見て（このとき[この空軍学校での差別問題に関するスピーチ](https://www.youtube.com/watch?v=WfjZ1otkS3o)のことが流れていた）たりして、そして寝たりして、とにかく退屈で何もない時間がすぎる。あげくに看護師が交代する時間になって、外で引き継ぎをしている様子も聞こえたり。\n\n結局麻酔科の医師がやってきたのは、たぶん12時半くらいだったと思う。4時間以上何もせずに待っていたのだった。自分はともかく、部屋が結構寒くて同じ部屋にいた妻が風邪をひくんじゃないかと。\n\n麻酔科の先生も麻酔の内容とリスク説明をして、同意書にサインして、と日本語のページを開き、そこで看護師が「あ、日本語のページにサインするなら、通訳してもらわないと」「あ、そうなの？じゃあ英語のページで」「いや、外科手術の同意を日本語でしてるから、これも日本語じゃないと」「あ、そうなの、じゃあMarttiか。」ということで再度Martti登場。朝とは違う日本人の方が、通訳をしてくれた。必要なかったけど、でもありがとうございます。通訳コードを入れて、同意書にサインして、完了。\n\n### 手術へ\n\nようやく順番が回ってきた午後1時半。車椅子に乗って手術室へ。ここで妻とは別れ、一般とは完全に隔離されたエレベーターへ。車椅子を押している人が無線で「患者搬送、4階から6階」というとエレベーターが到着する。中にもエレベーター操作専門に乗ってる人がいて、その人がフロアのボタンを押す。とにかくエレベーターやボタンやその他に、患者や車椅子を押す人が一切触らない仕組み。院内感染防止ということらしい（書いてあったのをチラ見しただけ）。\n\n手術室のフロアーは騒然としていて、「この病院、まだこんなにたくさん医師と看護師がいるの？？」ってくらいたくさん人がいた。手術担当の看護師が僕を受け取って、名前や生年月日で本人確認をし、手術室へ。ドラマでおなじみの、目の前にでかいライトがあるベッドへ上がり、右手の甲に点滴の針を刺され、点滴で麻酔を入れますよー、もう1回お名前と生年月日を、とベッドの上で言わされ、もう一人の看護師が「私も担当ですよろしく」「ナイスツーミーチュー」と会話をした。部屋には10人くらいいた気がする。これなら$16,000はかかりますわそりゃ、と\n\n…\n\nここで記憶がぷっつり。\n\n…\n\n### リカバリールーム\n\n目がさめると、ベッドの上で、手術は終わっていた。傷口はだいぶ痛くて、喉もカラカラで、考えてみたら前日の午前0時からすでに14時間以上水すら飲んでいないことに気づいた。目が覚めたことに気づいたかわいい看護師さんが、今ご家族を呼びますからね、何かほしいものは、というので、「み、みず。。。」\n\n水をもらい、妻と会い、いったん妻はまた戻って行って、麻酔の先生とresidentの2人が様子を見に来て、手術は何の問題もなかったらしい。\n\nお隣の患者さんが覚醒後にだいぶ咳き込んでいて、なんか大変なことぽかったんだが、僕についているかわいい看護師さんはそっちも多少ケアしつつも、えらくテキパキと「ちょっと傷口が痛くて。。。」「痛み止めを入れましょう。」「今点滴から入れたからこれはすぐ効くけどすぐ切れるので、錠剤も飲みましょう。でも錠剤を飲むには何か食べないと。クラッカー、塩味と甘いのどっちがいい？」「あー、あ？どっちでもいい。。。」「わかった」「はいクラッカー」「はいさらにクラッカー」「はい水も飲んで。クラッカーってパサつくでしょ？」「はいさらにクラッカー、今度は違う味のやつ。もう少し胃の中に何か入れないと。」「はいまたクラッカー、あ、砕けてたごめん」「はい、じゃあこの薬を飲んで」\n\n痛み止めというものを初めて飲んだ気がするんだが、効きますねこれ。びっくりするくらい痛みがなくなった。他の感覚はあるのに。\n\nその後もしばらくじっとして、2時間ほど経過したところで、じゃあそろそろ帰りましょうか、となった。血圧や心拍をチェックして、機器を身体から取り外して、テキパキ看護師さんが最後に顔を覗き込みながら、にっこりと笑って「最後に例のバカバカしい質問するわね。お名前と生年月日は？」無事に答えられたので、ベッドから降りて、車椅子へ移り、医師や看護師とさようならして、来た時と同じ要領でエレベーターに乗り、診察室へ。妻も戻ってきて、そこで最後にまた別の看護師と対面した。\n\n### Discharge手続き\n\n麻酔付きの手術で日帰りする場合は、付き添いが必須であり、さらに帰っても良い状態であることを看護師が確認する必要があるらしい（というのは前日の電話で言われていた。誰が来るかもそのとき言う必要があった）。この最後の看護師さんが一番とっつきづらい人で辛かった。辛かったのはもう1つ理由があって、ベッドから降りて車椅子に移ったときに立ちくらみがあって、それは当たり前だからそのまま車椅子に乗ったんだけど、車椅子に乗っている間にどんどん貧血っぽい、頭から血の気が失せてすこし吐き気がするような状態になっていて、それを看護師に告げたら、ううーんその状態では家に帰せないが、血圧も正常だし、ちょっと休みましょう、と。んでオレンジジュースをもらい、クラッカーももらったんだけど食べられず、正味4、50分帰れなかったんだろうか。ただ座り、何度か血圧を測り、立った状態で測り、「問題ないんだけど」「やーでもさ、貧血気味なんだよね」と貧血という単語をGoogle Translateして見せても「いや、でも血圧は正常だし」と言われるだけで、いや貧血的な状態と血圧って関係なくね？と思いながら、うー、最後のこの人だけ話しづらいなあ、もう無理して帰っちゃおうかなあ、と思い始めた。\n\n結局、トイレに行って小さい方を出し、最高ではないものの気分もだいぶ治ったので、そういうことでじゃあ帰ります、ってことで、念のため1階まで車椅子で押してもらって、病院を後にした。看護師に「どうやって帰るの？」と聞かれて「地下鉄じゃないかな」と行ったら「はあ？地下鉄？その状態で？私ならUber呼ぶけどね」と言われ、「はあ、それもいいアイデアだね」なんつって、結局病院の外に止まっていたタクシーに手を上げて、帰宅する。8時過ぎ。12時間病院にいて、$5,300というか$16,000の外科手術が完了した。\n\n### 週末\n\nその夜は、傷口がだいぶ痛くて、明け方起き出して痛み止めを飲んでまた寝たりして、土曜日もほとんど動かずぼーっとしていたが、土曜日の夜はだいぶ痛みも引いていて、日曜日もあまり何もせずにぼーっとしていたんだけど、明日は普通に出社します。\n\n事故の相手とは手術前に一応連絡は取れていて、お金の件を少し話してあるので、$5,300のうちいくら払わせられるのか。弁護士にでも頼めば慰謝料やらなんやらでもっと取れって言うんだろうけど、どうしようかなあ。\n\n[続く](/blog/2017-12-17_----------13490d2fdef1)。\n\n"},{"frontmatter":{"slug":"/blog/2017-10-02_-----------------------39bae6cabb2c","date":"Mon, 02 Oct 2017 03:39:51 GMT","title":"ニューヨークで自転車事故にあって鎖骨を折った","epoch":"1506915591","excerpt":"この話はニューヨークで自転車事故にあって救急車で運ばれたの続きです。"},"markdownBody":"\n### ニューヨークで自転車事故にあって鎖骨を折った\n\nこの話は[ニューヨークで自転車事故にあって救急車で運ばれた](/blog/2017-10-02_-------------------------2d10b02652db)の続きです。\n\n### 外科医のオフィス訪問\n\n家に帰ってもろもろ落ち着かせて、それから言われた通りに外科医のオフィスへ電話…しようとおもったら、すでに向こうから電話が入っていた。すぐに折り返してくれと。電話して、秘書らしき人に用件を告げ、何時に来られるか聞かれて午後1時でアポをとる。場所は家から30分くらいのところだったんだが、いろいろおたおたしていたら出発時間になり、なるべく脱ぎやすい服をさがしてきて、出発。\n\nこのオフィスは NYU Langone Orthopaedic Surgery Associates というところで、普段外科の外来を受けるところなんじゃないかと思われる。普通のオフィスぽいとこで、受付でアポを告げて、「2つドアを通った先にある Room F」と言われ、2つドアを通って Room Fに行こうとしたら待ってる他の患者に「EよE」と言われ、「Fって言われたんだけど」「なんかFの先生今いないからEなんだって」「そうなの？ありがとう」みたいな会話をして Room E に行ったら、電話した秘書の人がいた。こういうの、日本の病院の待合とかであまり想像できない。こっちの人って、右往左往してそうな人を見るとすかさずなんかヘルプしようとするよなあ、と思う。\n\n簡単に今日の訪問の用件を聞かれ、「CoPayがあるわね」と、すかさずカード読み取り機を掲示され、$60をお支払い。後で保険会社のWebページで見たんだが、この先生のOffice Visitの定価は$850だそうだ。それが保険会社との契約によるdiscountで大幅に減額され、それの20%をCoPayしたということのようだった。\n\n先生が来るまで別室で待つように言われ、最初に看護師が入ってきて、身長体重から薬の使用歴やら病歴やらを聞かれはじめ、そこでハタと「もしかして今日 Emergency からきた？」「はい」「あーどうりで、全部入力されてたわ。レントゲンも撮った？」「はい」「あーほんとだ、じゃあこれを表示して、ふーん、先生が来るからお待ちくださーい」ってことで看護師の仕事は終了。\n\n次に、アポを取った先生についている研修医的な若手が登場し、レントゲン写真を見て、ふむふむ、OK、これはこうやって骨をまっすぐにしてプレートで固定するって手術になりますよ。職業は何です？プログラマー？じゃあ大丈夫だね。鎖骨ってのは、腕を肩から90度より上に上げたときに動く骨なんだよね。プログラマーがキーボードを打つときの腕はこうだろう？上には上がらないから、手術後は仕事も普通にできるね。お酒は飲む？うん、治るまで飲んじゃダメ。タバコは？うん、治るまで禁止ね。じゃあちょっと待っててね。\n\n最後にアポを取った先生が登場。さっきの若手も一緒についてくる。先生は若手とほぼ同じ質問をして、それに同じように答えると、後ろの若手が満足そうな顔をするのが微笑ましかった。\n\n先生はリスク説明もした。縫った後の部分はちょっと他の部分より鈍感に（numbに）なるよ、麻酔をするから麻酔にかかわるリスクもある（もっと詳細な説明だったけど省略）、それから（レントゲン写真を指差して）ここの黒いところ、肺なんだよね。んで、骨にボルトをこうやって入れるから、ここんとこで刺しすぎると肺に穴があいちゃって空気が入っちゃうことがあるんだよ。そうなったら入院してお腹の横から空気を抜かないといけないんだよね。いやーめったに起こらないから。あとここら辺には神経もあるから、それに触れたり切ったりすることもあるよ。いやーめったに起こらないから。\n\nと、ほんとにこういうノリの説明で、こっちとしてはもうお任せするしかないので、はあ、はあ、はあ、と聞いていた。最後に、「手術は2日後の金曜日にする。手術は午前中に行われるが詳細な日時は追って連絡が来るから。」そしてさっきの秘書のところに戻る。\n\n秘書のところでは、外科手術同意書にサインをし、改めて病歴などのチェックボックスたちにチェックを入れ、コピーをもらって、この日は終了。14時くらいだったかな。\n\n朝から何も食べていないので、なんか食べようとUnion Squareのあたりをうろうろしたけど何かピンとくるものがなく、仕方なくAstor Placeまで歩いて一風堂でラーメンを食べて家に帰ってきた。\n\n### 会社からのサプライズ\n\n家からSlackで会社に連絡を入れ、そういうわけなので、片腕しかないので生産性が下がりますが、今日は家で仕事しますということにして、Pull Requestのレビューをしたり、準備していたPull Request（するためのコミット）を見直したりして、少し働いた。痛みは多少あるものの、動かさなければそうでもない感じだった。\n\n夜の8時ごろ、突然電話が鳴って、出たらアパートのフロントからで「配達が来てるけど」「え、配達？頼んでないけどな、部屋番号あってる？」「うん、とりあえず行かせるよ」「あ、OK」なんだろう？請求書がバイク便で来たか？とか思ったらなんと、会社のみんなから「早く元気になってね」カードとポテチなどが！Office ManagerのSandyがわざわざ家まで持ってきてくれたのでした。俺まだ入社1ヶ月とかっすけど、みんなちゃんと俺のこと意識したメッセージを書いてくれてて泣きそうだった。今書いててもちょっと泣きそう。いい会社に入れたなぁ、おれ。がんばらないと。ありがとうありがとう。\n\n### 木曜日\n\n事故って救急に行き、改めて医師のオフィス訪問をしたのが水曜日で、手術は金曜日なので、木曜日は自宅で仕事をそれなりにやった。骨は折れたままだし、腫れも引かないので、そんなに集中できないんだけども。\n\nそんな中、救急から電話が入る。その後の容体の確認だそうだ。オフィスへ行って、手術が明日になったと告げたら、「早いわね！よかったわね！」と。やっぱりそこまで普通な感じではないんだよねこのスピード感は、と思った。救急からは金曜日にも「その後どう？」という電話が入る。ルーティンなんだろうけども、ちゃんとしてるなあと思った。\n\nさらに手術をする病院から手術費用の連絡が入る。$5,300。「それって保険効いてるの？」「保険のdeductibleやらcopayやらをあーだこーだして出した額です」「ん、でその額を保険が払ってくれるってこと？」「ん、は？いいえあなたが払います」「んあ、えと、手術後に払うってこと？」「いいえまず払ってください」「え、この電話で？」「はー、受付で払ってください。」「あ、そういうことか。」こっちは月$10のAmazon Primeを無駄だと思って入らない人なのに。$5,300とか。Pixel 2は買えないわ。買おうと思ってたものはすべてナシだ。あーあ。\n\n最後にまた手術をする病院から連絡が入り、金曜日の朝8時に来るように、場所はNYU Langone Tisch Hospital。玄関を入ったら左に曲がってエレベーターで4階に来てくれと。そこで払うのね。はい。\n\n[手術の日](/blog/2017-10-02_-------------------------d8cb3ae7d810)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2017-10-02_-------------------------2d10b02652db","date":"Mon, 02 Oct 2017 03:32:02 GMT","title":"ニューヨークで自転車事故にあって救急車で運ばれた","epoch":"1506915122","excerpt":"序"},"markdownBody":"\n### ニューヨークで自転車事故にあって救急車で運ばれた\n\n### 序\n\nいつもの水曜日の朝。いつものように愛車にまたがり、歩行者の予測不能な動きに細心の注意を払いながらブルックリンブリッジを越え、車線のある道に出てほっと一安心し、前方に2台の自転車が並走してこちらに向かってくるのを見て、「まあ完全に車線があるところだし周囲にも余裕はあるし、あっちが避けるに決まってる」と思い込んでそのまま自分の車線を直進し、…\n\nえ？避けないの？？？は？なに？してんのお前？\n\nまったく回避行動をしないでこちらの車線上で逆走を続けた1台と、隣の車線を走る2台との間をすりぬけるような感じになって、すり抜けられた！とおもった次の瞬間、こちらの車線を逆走していたライダーと接触してふっとばされ、左肩から道路に落ちた。\n\nしばらく呆然として、それから何気なく左肩から首にかけて触ってみて、変な出っ張りがあるのに気づいて、「あ、鎖骨折れた」。自転車ロードレースを見ていておなじみになった、fracture、collarbone、という英単語が頭をめぐる。\n\nたまたま交通警官が毎朝立っている場所で起きた事故で、警官が様子を見にきて、事情を聞き、救急車を呼んでくれた。さらにぶつかった相手もすぐにこちらに来て警官と話をして、さらに最初から「アイムソーリー」を連呼していて、あ、やっぱりアメリカ人は謝らないとかいうのもウソなんだなぁとか、もやーっと考えた。\n\n10分くらいで救急車が到着し、警官から「84 precinct で injury report を出してあるから必要なら取りに行け」と言われ、この「precinct」がわからなくて、でもまあいいかと、救急車に乗って、包帯で左腕を曲げた状態で固定し、「あ、これ自転車レースでよく見る姿勢」とか思いながら、あー、救急車で ER か、一体いくらかかるんだろう…。\n\n### 救急車から救急病院\n\n救急車の2人はプロフェッショナルなフレンドリーさで不安感は特になかった。救急車内で住所や電話番号、身長体重なんかを聞かれた。身長体重は、フィートやポンドでは言えなかったんだけど、KgとCmの数値で大丈夫で、入力しているタブレット端末で勝手に変換されていた。\n\n救急隊員が警官から Go をもらって、おそらく一番近いだろう NYU Langone Cobble Hill という Emergency Care 施設へ。20分くらいだったかな。救急車を降りて、歩いて処置室まで行き、そこで看護師が救急隊員と情報を（タブレット端末で）交換し、交換したはずなのに再度身長体重を聞かれ、SSN を聞かれ、言ったら変な顔をされ、「その番号変？」と聞いたら「なんかちょっと変」と。えーなんか違うのかな。\n\n救急隊員は「feel better, man!」と言って去り（周りからいろいろ聞かれていてろくにお礼も言えなかった）、看護師が情報入力を終えたところで、着ていた服を激痛に耐えて脱ぎ、看護服を着たら医師が2人登場し、どういう状況だったのか、身体がどうなっていると思うのかを聞かれ、レントゲン撮影をしましょうとなる。この間、Wifiがあるのを発見して、会社と妻へ事故った旨を連絡。\n\nたぶん撮影準備を待っている間、医療事務の方がタブレットを持って登場し、10箇所くらいサインする必要がある各種同意書の説明をしてくれた。他の病院の医師から紹介があったときに履歴を見せてもいいか、保険会社には見せてもいいか、とかそういうこと。あと臓器提供の意思（するしないではなく、そのどちらかの意思）を示したか、とか。それとパスポートと保険証を渡す。\n\n同意書の説明がまだ続いている間にレントゲン技師が現れて、撮影へ。7–8枚撮影して、ああーこれはいくらになるんだろう…。\n\nレントゲンから戻るとさっきの同意手続きの続き。そうこうしているうちに身体が冷えてきて、それから患部が急速に腫れて痛くなってきて、それを見た看護師が痛み止めを出しましょうかと言ってくれたんだけど、ここでまた「自転車レースでは最近痛み止めも問題視されていたな」とか思って「いいえ結構です。耐えます。」とか言ってびっくりされ、代わりにアイスバッグを持ってきてくれて、さらに毛布もくれた。この毛布が温めてあるんですよ。ほんわかしてホッとするんですよほんとに。\n\nその後2人の医師の1人（すぐ目の前の席でコンピュータを見てた）に呼ばれて、レントゲン写真の説明をされ、要するに1本の骨が3つに別れてしまっていて、しかもそれぞれが微妙に重なり合ってしまっているので、手術することを「おすすめする」と。そこで思わず「えっ、他にオプションがあるんですか？」と聞いてしまったんだけど、もちろん医師は真剣な表情で「放っておいて治るのを待つってこともあるし、実際私はそれで治したんだけど、あなたのとは違って私の場合は折れた場所が山型に盛り上がったような状態だったのと、あなたはアクティブな人のようだから、腕が90度より上に上がらないというような後遺症は避けたいでしょう？それなら手術をした方がいい。」と。\n\nそうか、あくまでも、こんな場合でも患者の意思が尊重されて、患者の意思によって医療行為が行われるのだ。こんなときでも受動的になすがままではいられないんだこの地では、と、ちょっと生きていくのが辛くなる。\n\n「そういうわけで、すでに外科の先生に連絡をつけてあって、先生はあなたに今日中に電話してきてほしい、今日中に会って、なるべく早く手術をしたいとおっしゃっている」と説明され、After Visit Summaryというものを渡されて、「とにかくここに電話して」となって、診察などは終了。\n\nこの間に、さっき渡しておいたパスポートと保険証を使って医療事務の人がいろいろ手続きをしてくれていて、看護師からも最後の説明、とにかく先生に電話しろ、あと家でなにをするかはAfter Visit Summaryに書いてあるから、ということを聞かされ、最後にパスポートを返してもらいに事務の人のところへ行って、返してもらったら、それで終了した。\n\nあれ？ここの料金はどこで払うの？って聞いたら、「んんー？支払いはないみたいよ。」Emergency Care の CoPay は Waive される場合があると保険の説明に書いてあるけど、そういう場合だったんだろうか。よくわからん。まだ請求は来ていない。\n\n会社に改めて詳細を連絡し、事故現場に停めてある自転車を回収して会社に行くと言ったら仰天され、今日は来なくていいからと言われて、考えてみたら外科医にあわないといけないかもしれないしと思って、ありがたくお休みをいただき、妻に連絡して近くの地下鉄駅まで来てもらって、2人で自転車をかついで家に帰った。自転車は外見上は壊れていないように見える。地下鉄に（ラッシュ時以外なら）そのまま自転車を載せられるのがありがたい。\n\nこのとき午前11時ちょっと過ぎ。[外科医訪問](/blog/2017-10-02_-----------------------39bae6cabb2c)へ続く。\n\n"},{"frontmatter":{"slug":"/blog/2017-08-08_Achievement-unlocked----------H-1B-----------2-2--6609e6aa7d68","date":"Tue, 08 Aug 2017 01:35:14 GMT","title":"Achievement unlocked: アメリカ国内で H-1B のまま転職した（その2/2）","epoch":"1502156114","excerpt":"その1からの続きです。"},"markdownBody":"\n### Achievement unlocked: アメリカ国内で H-1B のまま転職した（その2/2）\n\n[その1](/blog/2017-08-08_Achievement-unlocked----------H-1B---------1-2--488f6217c832)からの続きです。\n\n### 11. Visa Transfer\n\nH-1Bビザ保有者は、新しいスポンサーが「transfer」の手続きをしてくれれば、会社を移ることができます。「transfer」と通称されているものの、実質はほとんど新規の手続きと変わらないようで、例えば本当にできたばかりのスタートアップとかだと、却下される可能性も高いそうです（弁護士談）。\n\n弁護士から連絡が来て、必要な書類を至急送ってくれるように連絡がありました。\n\n* パスポート（空白ページも含む全ページ）\n* これまでに得たすべてのI-797\n* 出入国履歴\n* 最新のI-94（入国記録）\n* 直近の給与明細\n* H-1B取得時に提出した、学歴やその評価記録\n\nまた、配偶者のH-4も同時に申請するので、配偶者の分のパスポートや出入国履歴なども。\n\nアメリカで働いているような方々は、日本に一時帰国したり、ヨーロッパへ出張したり、バケーションで海外へ行ったりすることが多いでしょうから、出入国履歴をhandyにしておくことは結構大事だと思いました。かくいう私はパスポートのスタンプから日付を判別し、Google Photosで記録を照らし合わせ、日付を正確にするめんどくさい作業をする羽目になりました。\n\nそれと、最初にH-1Bを取得したときに弁護士が提出したはずの資料のコピーを手元に置いておくことも重要です。私の場合は特に、CSメジャーでない（BA, Liberal Arts, Linguistics）せいで私の学歴及びその後の職歴をSoftware engineerとしての特殊技能を保持していると認定できるかどうかの精査を行ってもらっていたので、その書類が手元にあったことは手続きを早く進める上で役に立ったと思われます。これまでに得たすべてのI-797も同様。\n\n配偶者のH-4も手続きする場合はさらに、婚姻証明や戸籍謄本も（当初の申請時に）取得、作成したはずですから、それも手元に置いておく必要があるでしょう。これらの日付は当然古いわけですが、そこは問題にはならなかったです（これを読んでいる方は弁護士に確認してください。あくまで、2017年8月時点の私の場合はそうだったというだけです）。配偶者の出入国履歴も必要です。記録しておきましょう。\n\nこれらはほぼすべて、スキャンしてPDFをメールで送ればすみます。サインが必要な場合でも、サインしてスキャンしたり、EchoSignのようなオンラインのサインツール（電子署名って書くと誤解がありそうなので…）を使ってオンラインで行うことができます。\n\n実際にFedExで弁護士に原本を送らないといけなかったのは、H-4に関する委任状だけでした。これは私たちの場合、当初のH-4は弁護士に頼まずにすべて自分たちで作業を行って取得したためだと思われます（想像です。必要なら理由を確認してください）。\n\n### 12. transfer手続き待ち…\n\n私の場合、H-1Bを移す先の会社をどうするか問題（詳しくは書きませんが先述のスタートアップだと厳しいかもって話）があって少し遅れ、7月第三週からやりとりが始まり、8月に入ってもまだ完了していなかったので、7月末の退社はどのみち無理筋でした。H-1Bのtransferを伴う場合、1ヶ月は見たほうがよさそうです。\n\nただし、transferが完全に行われるまで待つ必要はなく、USCISがtransferの手続き書類を受理して、その受理番号を受け取れたら、その時点で新しい会社で働き始めることができます（これを読んでいる方はこの件も確認してください。あくまで、2017年8月時点の私の場合はそうだったというだけです）（H-1B portability rule）。\n\nしかしそうなると当然浮かんでくる次の疑問は、「働き始めてからtransfer申請が却下されたらどうなるの」です。ググってみると、「却下されてもビザは無効にはならないので、元の会社では働ける」という説明が出てきます。確かに、transfer申請の事実や、必要書類などについて、現職（つまり前職）の会社に知らせる必要は、手続き上は一切ありません。ですから、失敗したらしれっと働き続けるというのは不可能ではないのでしょう。ですが、オファーを受けて入社日を決めなければtransferの手続きも始まらず、入社日を決めた以上は退社日について通告しなければならないわけですから、これは鶏と卵です。入社日を決めずにtransfer手続きだけ開始してその成り行きで退社日と入社日を決められればよいのでしょうが、自分の場合そこまで頭を回せませんでした。\n\n却下されて前職も退社した場合、残る選択肢は出国以外にはありません。転職が決まったはずでも、多くの場合ここでの雇用はお互いの「At will」ですから、入社数日で解雇という結末になるのでしょう。\n\n自分の話に戻ると、入社予定日（月曜日）の直前の金曜日に「transferの手続きはcomplete」という連絡が来て、晴れて月曜日から（とりあえず却下されるまでは）新しい会社で働けることになりました。\n\n### 12. 残務整理\n\n結果的に新規プロジェクトのリリース直前での離職となることもあって、残務整理というよりはかかえていた仕事を終わらせるので手一杯でしたが、それまでに頭の中にだけあったもろもろのドキュメント化も並行して行いました。狭い業界だし、その会社や同僚が憎くてやめるわけでもないし、クビになったわけでもないので、自分の退社が周囲に対するpunishmentになってしまうようなことはできるだけ避けようと思っていました。\n\nとはいえ、「ちくしょう、転職だっ」と思ったのは初めてではなくて、その度にドキュメント化などの作業は行っていたので、あわててあっぷあっぷで書いたというわけでもないです。もっとも、そういう理由ではなくて常日頃からできるだけドキュメントやワークログを残したり、情報共有をしておくほうがいいに決まっています。全然やっていなかったわけではありませんが、いかんせん目の前の作業の方が多かったもので。結果、ろくに休みもとれずに会社を移ることになってしまいました。\n\n### 13. まとめと反省\n\n転職先に余裕があって入社日をオファー受諾から1ヶ月以上先にできるような場合には、transferをとにかく開始してもらって、進捗を見て最終出社日を通告するのが、気持ち的に一番楽だろうと思います。\n\nですが多くの場合、転職先に余裕がない（今すぐにでも来てほしい）か、現職がそこまでゆるふわっとしていなくて2週間前通告で即退社というわけにもいかない（プロジェクトがどんどんアサインされるとか）、またはその両方でしょう。できるだけコトを円滑に進めて自分の気持ちに余裕を持たせるためにも、「ちくしょう転職だっ」から実際の転職活動に踏み切るまでに、1ヶ月くらいは助走期間を設けるのがいいんじゃないかと思いました。現職からスムーズにフェードアウトできる準備や、必要書類を調べてできるだけ（紙とスキャンした電子データの両方を）手元に用意して、大まかな最終出社日と新しい会社での初日を描いて、それから転職サイトへの登録なんかをやるとよいのかなと思いました。transfer手続きは現職には完全に伏せたまま行うことができるはずですので、transferのめどがたった段階で、最終出社日を通告するのが、気持ち的にはよいかなあと思いました。\n\nとはいえ転職するとかしないとかは自分の気持ちだけで決められるものでもないので、ということは、そういう書類関係は常日頃から用意しておくこと、現職においていつ辞めても周囲が困らないように情報共有して属人化しているものを切り離していくこと、などをしておかないといけないということで、だけどこういう心構えや準備って、当然だといわれるけど実際には業務に追われていたり周囲にやる気がなかったりで難しいことも多いので、やっぱり「転職だっ」とかならないとなかなか難しく、結局鶏と卵なのかなあと。\n\nなお、これを書いている2017年という年は、後から振り返ったときに特異な年として記憶されているのかもしれません。言うまでもなく現政権の「America First」のおかげです。H-1Bの審査を厳正にする話やポイント制度の導入などが聞こえてきます。\n\nですから、もしかするとこれを読んでいるあなたには、私の体験談は の役にも立たないかもしれません。そうでなくてもこれはいわゆる「I am not a lawyer, but」な話ですし、すべての人に一様に当てはまるわけでもありません。そこんところはどうかご了承の上で、それでも参考になる部分があればよいのですが。\n\nおわり。\n\n"},{"frontmatter":{"slug":"/blog/2017-08-08_Achievement-unlocked----------H-1B---------1-2--488f6217c832","date":"Tue, 08 Aug 2017 01:32:47 GMT","title":"Achievement unlocked: アメリカ国内で H-1B のまま転職した（1/2）","epoch":"1502155967","excerpt":"前職でそのサービスを作り始めて7年半、2013年4月に渡米して4年と1/4が経ち、そろそろ新しいチャレンジをしたい、チャンスがまだあるうちに、というわけで転職活動をしておりました。8月から新しい職場で新しい仕事を始めます。 前職の会社、経営陣、社員の皆さまには、自分の書いた汚いコードをこんなに立派なサービスに仕立て上げて、世界4カ国で数十人を雇用し、200万以上のアカウントが作られるまでに育て…"},"markdownBody":"\n### Achievement unlocked: アメリカ国内で H-1B のまま転職した（その1/2）\n\n### 0. はじめに\n\n前職でそのサービスを作り始めて7年半、2013年4月に渡米して4年と1/4が経ち、そろそろ新しいチャレンジをしたい、チャンスがまだあるうちに、というわけで転職活動をしておりました。8月から新しい職場で新しい仕事を始めます。\n\n前職の会社、経営陣、社員の皆さまには、自分の書いた汚いコードをこんなに立派なサービスに仕立て上げて、世界4カ国で数十人を雇用し、200万以上のアカウントが作られるまでに育て上げてもらって、感謝しかありません。ありがとうございました。\n\nさて、せっかくなので忘れないうちに、H-1B ビザ保有者のアメリカ国内での転職はどういう流れで進んだのか、書き留めておこうと思います。検索したんですが、あまりこれだって例が見つからなかったので、誰かの役に立てばと思います。\n\nなお前半は私の転職体験談なので、H-1Bの転職で必要な書類とかの概略だけ知りたいという場合は主にその2から見ればよいとおもいます。\n\n### 1. 前提\n\nコードを書くことについては、転職時点で主に以下のような感じでした。\n* Perl 5 (10年以上)\n* JavaScript (AngularJS, 少々Backbone.js、1–2年)\n* Android (2.xで少々、4.0から本格的に、3年)\n* iOS (Objective Cのみ、2012から2014くらいまで)\n  * インフラ周りの世話もほとんどやっていました(7年)。\n* AWS (EC2, S3, Elaticache, RDS (MySQL, Aurora), CloudWatch,&nbsp;…)\n* Elasticsearch (1.5, 1.7)\n* MongoDB\n* Kurado, xxForecast などのmetrics系\n  * AWSでclassicからVPCに移行するとか、MySQLをAuroraに移すとか、その手の作業も計画から実施まで大体1人でやってました。\n\nそもそもサービスをイチから作ったので、DB設計やサーバー構成やプログラムの設計なども（出来の良し悪しはともかくとして）やってました。7年前に行った設計に大した手直しもせずにここまで来ていて、根本から書き直さないとどうしようもないみたいな問題は7年後の今も起こしていないと思われるので、まあまあのレベルでできていると言ってもいいのかなあと思っています。\n\nこういう、いわゆる「full stack developer」的な履歴書（ただし自分でその言葉は使っていない）で活動を始めたのですが、今の仕事にだいぶ飽きていたということもあり、自分としてはAndroid Engineerで探せないかなあ、と思っていました。\n\n### 2. 転職サイト登録\n\nAngelList、Hired、Vetteryの3サイトにもともとアカウントを持っていたので、履歴書などの内容を更新し、「現在職探し中」のステータスにしました。よさげなところから連絡が来るのを気長に待つつもりでした。\n\nHiredは、職探しを始めると最初の2週間プロフィールをfeatureしてくれるらしく、Hired側のレビューが終わってfeature開始のメールが来た次の瞬間から、エラい勢いで「interview request」が来る結果に。\n\nVetteryも似たような雰囲気だったのですが、こちらはいつまでもreviewingのステータスのままで、結局今もそのままです。全然使いませんでした。\n\nAngelListでは向こうから連絡が来たことはなかったのですが、なんとなく募集中の会社を見ていたら1つ面白そうな会社を見つけたので、こちらからApplyしてみました。\n\n### 3. Phone screening\n\nHired のサイトで、「連絡したいから日時を指定してくれ」と言われたり、「カレンダーの空いているところにリクエストを送りました」と言われたりするので、日程を設定して電話を受けました。Hiredのカレンダーは真面目に更新しておいたほうがよいと思います。空いているところにいきなり予定を入れられることもあって、Google Calendarと同期できていないために見落としたり忘れそうになったことがありました。\n\nPhone screeningは基本的に先方の人事担当者がかけてくる15–30分くらいのもので、自社の紹介をしてくれた後、こちらの希望ポジションや勤務地を聞いてくるもので、技術的な話はほぼありませんでした。ただしここで大事なのが、「H-1B の transfer をする意思があるか」を確認することです。Hired にはそれが必要である旨を示す項目があるのですが、私の10社くらいの電話経験では、そこを見た上で連絡してきた人は少数でした。30分の電話の最後に、「ところで、…」と聞いたら「ああああー、それ無理だわーごめん」と言われて終了したところもありました。\n\n電話の後、24–48時間後には次の連絡があるのですが、この時点で「お祈り」されることも数社ありました。正直理由は全然わかりません。なんで電話してきたの？って感じでした。たぶん、HRの人は手当たりしだいに電話をし、なんとなくよさげだったらエンジニアや開発チームに履歴書を投げ、エンジニアや開発チームがそれを見て「イラネ」って言ってるんだろうと思います。\n\n### 4. Coding interview 1\n\nCSメジャーでなく、仕事以外のコードを公開しているわけでもない、ただの叩き上げプログラマーの自分にとって、Coding interviewは鬼門以外の何物でもなく、恐怖の対象でした。1ヶ月無給でいいから雇ってくれれば証明できるんだけどなあ、といつも思ってました。とはいえ恐れてばかりいても仕方ないので、leetcodeなんかで多少の練習はしつつも、基本的に最初のほうの会社のCoding interviewは、それ自体が練習なんだと思うようにしていました。出された問題を理解すること。書き始める前に問題に対して些細な、あるいは当然と思われることでも質問すること。その時間を通して自分自身のIce breakingをするような感覚でした。とはいえ、あの種の、これまで実践で書いたことのない種類のコードを30分でそれなり動くように仕上げるのは、非常に難しかったです。バカみたいなことでも、書いている最中に考えていることをブツブツ言うことも心がけました。\n\n1社だけ、interviewの前にinterviewerがメールを送ってきてくれて、「このinterviewで見るのはどういう風にコードを書く人なのかということで、問題に対する答えを書いたかどうかは重要ではないし、ググっても何しても全然構わない。」と強調してくれたところがありました。まあそうは言ってもねえ、と思ってましたが、その人のことをGitHubなどで調べられるし、そもそも変なところで緊張しなくてよいように気遣ってくれているのがありがたかったです。\n\n### 5. 履歴書の内容を少し変更\n\nここまでで数社coding interviewの予定ができて、phone screeningも数社入っている状況で、どれも基本的にはFull stackとかBackendとかArchitectとかいう肩書きの募集で、Android Engineerで先方から連絡があったのは1社しかない状況でした。考えてみたらそりゃそうで、履歴書にはAndroid EngineerとしてのAchievementsがあまりに目立たないので、あれを見てAndroidのポジションで雇おうと思う人は少ないだろうと。こちらとしては「なんでもできるから、なんなら2人雇うより安いよ」みたいなイメージでいたのですが、薄々分かっていたとおり、「なんでもできる」というのはポジションではないので雇われないんですね。\n\nそこで、履歴書の内容に少し手を加えて、Androidのアプリ開発で何をやっていたかを、他の項目以上の量で盛り込んでみました。それと、思い切ってfullstack/backend developerのチェックボックスを外して、Androidのポジションだけで職を探していることにしてみました。まあダメならあとで戻せばいいやと思って。先述のとおり、いいところがあったら…という感じでいたんです。\n\nそうしたら、Androidでの連絡もちらほらと来るようになりました。\n\n### 6. Coding interview 2\n\nある会社から、オフィスに呼ばれてホワイトボードコーディングをしました。データ構造を与えられて、RDBのスキーマを答え、続けてそのスキーマを使ってある種のデータを取り出すコードを書くという感じで、一見とっつきやすいが始めてみると普通に再帰してしまうと重複データが出てきてしまう、それを避けるためのコードを書くと、件数が多いときどうする？って話になる、と、よくできた問題でした。\n\nありがちかつ苦手な、「バイナリーツリーをバランスしろ」とか「文字列と別の文字列を比較してこれとこれの条件にあうやつを取り出した文字列の最大長を答えろ」みたいな問題ではなくて、もう少しその会社の実際の業務に近い内容とそこで直面する問題についてどう考えるかを聞かれた感じだったので、楽しめたしまあまあうまくいったんじゃないかなあという反応でした。\n\nここでも、とにかく問題が出たら質問する。考えすぎずに最初の思いつきで何かを書き始めて、途中で（普通なら自問自答するような内容を）聞く、あるいは議論するということを心がけました。あとあちゃー間違えたーと気づいたときも、「あちゃー間違えたー」と言いつつ、今やっていたことをそれなりに書き終えて、説明する中で、「ここはこういうつもりだったけど前提条件のこれを忘れてて、この条件がある以上これだとデータが重複しちゃいますね」みたいなことを、自分のバカさをさらけ出すようにしゃべって、反応をうかがったりしました。\n\n### 7. Coding interview more\n\nその他いろんな会社と電話でやったんですが、まあ失敗しますね。問題は解けたかなあと思っていても、その後の返事はお祈りだったってことがほとんどでした。負け惜しみですが、こちらからすごく興味を持ったのにお祈りをくらったわけではなく、そっちが興味あるっていうから話聞いてあげたのにさあ、とか思うようにして、あまり落ち込まないように心がけたりして。\n\n### 8. ここまで\n\n長々と書きましたが、じつはここまでで2週間経っていないくらいじゃないかと思います。なんというか、早い。Hiredでプロモートされる期間が2週間だけなので、それが終わったらどうなるんだろうなあと思っていましたが、3回目のinterviewまでいったところもあって、うーんどうしようかなあと。\n\nただそんな中、ここまでで「こちらからapplyした」唯一の会社でもinterviewは継続していて、この会社のことを知るほどに興味が湧いてこの時点ではここが第一志望ってことなんだろうな、と思うようになっていました。先述のinterviewerが事前に連絡をくれたのもこの会社で。印象もよいしやってることも面白そうだし、ポジションはAndroid Engineerだし。\n\n独立記念日の前の週になって、続けているのは2社という状況になっていました。前回オフィスに呼ばれた会社に、もう一回オフィスにきてCTOと他の開発メンバーとあってほしいということになり、ただ独立記念日の直前なので、みんな出払ってしまうのでどうしようかと相談され、結果Hangoutで木曜日にやることになりました。そしてその前の水曜日に、もう1社の「第一志望」的な会社からオフィスに来て経営陣とあってほしいということになり、休みをもらってオフィスへ行きました。\n\n素敵オフィスで4人と順番に話をし、自分が今までしてきたこと、今できること、今後やりたいことなどを話しました。好感触だったし、やろうとしていることも簡単ではないけど自分にできることもありそうだし、ここからオファーがあったら行こうと決めました。\n\n### 9. オファー\n\n結果、オファーをいただきました。先に電話でオファーの概要とこちらの気持ちや状況を確認され、問題なかったので即答しました。が、この電話のときにH-1Bの件を確認するのを（だいぶ舞い上がっていて）忘れたことにあとで気づき、あわててメールで連絡をしました。幸い、翌朝には「なんとかする」という返事をもらったので、ほっと一息ついたのですが、だいぶ胃が痛かったです。\n\nもう1社のほうにそういうわけでお断りとお礼の連絡をメールで送り、その他のこの間に始まってまだ残っていたinterview requestもすべてキャンセルしました。\n\nそしたらなんと、お断りしたほうのもう1社から「すぐオファーを出すからもう1回話させてくれ」という返信をもらい、うれしいやら困ったやら。この会社も印象はすごくよかったし、これでさらに好印象になったのですが、気持ちはもう決まってしまっていたので、「本当にごめん」と再度連絡して、「仕方ないねグッドラック」的な返事をいただきました。\n\n### 10. Two weeks notice\n\nオファーを受諾したので、前職（この時点で現職）のCEOに退職する旨を伝え、最終日の相談をしました。2週間という意味では7月末でよかったのですが、その時点で関わっていたプロジェクトの進捗との兼ね合いもあり、8月中から末を打診され、新しい会社のほうとも相談して、8月第一週ということで決まりました。\n\n想像ですが、おそらく私が転職活動をしていることはすでに知っていたのだろうと思います。会社のことよりも、ビザの件などこちらの心配をしてもらって、ありがたかったです。\n\n[その2](/blog/2017-08-08_Achievement-unlocked----------H-1B-----------2-2--6609e6aa7d68)へ続く\n\n"},{"frontmatter":{"slug":"/blog/2017-07-17_Nexus-5X-------1bb1d9c07087","date":"Mon, 17 Jul 2017 03:29:24 GMT","title":"Nexus 5X 交換顛末記","epoch":"1500262164","excerpt":"昨年のPixel をスキップしてもう1年同じ端末を使い続けようと決めて、「もっさり」も「フリーズ」も我慢してきたんだが、独立記念日の前日にとうとうブートループになってしまい、OSが起動しなくなってしまった。"},"markdownBody":"\n### Nexus 5X 交換顛末記\n\n昨年のPixel をスキップしてもう1年同じ端末を使い続けようと決めて、「もっさり」も「フリーズ」も我慢してきたんだが、独立記念日の前日にとうとうブートループになってしまい、OSが起動しなくなってしまった。\n\nfastbootには入れるんだが、そこでrecovery modeに入ろうとしてもまたブートループになってしまってお手上げ。Google Storeで買ったのでGoogleのサポートとチャットをした。独立記念日の前日（月曜日）に。\n\nブートループに入ったのを説明し、IMEIを教え、型通りの「これをやってみてくれ」という手順（fastbootからのrecovery mode）を教わり、やってみて、「やっぱり同じです」と言ったら、あっさりと「Warrantyは切れているけどspecialでreplacementします」と。\n\nReplace用のリンクを渡され、そこで「購入」手続きをして、出荷待ちになったのがJuly 4th。3日後の金曜日にもう交換品のNexus 5Xが届いた。アメリカなのにすごい。箱に貼ってあるシールを見ると、「Manufactured: 01/2017」と。え、2017年1月製造? てっきりrefurbishedのものが来ると思ってた。\n\n起動してみるとROMはAndroid 7.0。そこから今の時点までの月次更新と、7.1.1と7.1.2とその後の月次更新をひたすらやり続け、アプリをそれなりにインストールし、Googleアカウントを設定し、ほぼ復旧できた。\n\n復旧できなかったものが3つ。\n\n1つはLINEで、僕はcellular networkの契約をしていないので、SMSが受け取れないので、アカウント移行ができない。Google Voiceの番号には送ってくれない（Invalid Numberだって）ようで。\n\n2つ目はDropboxで、これはAuthenticatorアプリを失ったのに、emergency backup codeも紛失していたため。サポートリクエストを投げてみたがどうなるか。\n\n最後の、一番の大問題はAndroid Pay。機種変更したから復旧というか、新しくカードを追加すれば済むし、普段あまりつかっていないので「移行」という意味では問題ないんだが、壊れたほうの端末を送り返さないといけなくて（30日以内に送り返さないと、上述の「購入」手順で取られたAUTHをCaptureされてしまう）、送り返してどこぞの工場で復旧されてAndroid Payでお買い物されるの嫌だなあ、と。\n\nそういうわけで、どうかあと一度だけ起動してくださいお願いします、という願いをこめて、壊れた方の、2015年10月製造のNexus 5Xは今[冷凍庫に入っています](https://www.youtube.com/watch?v=SD25mdrgAys)。\n\nまあ指紋認証があるから、起動できないまま送り返しても大丈夫だとは思うんだけど。梱包して送る前に端末についてるだろう指紋をきれいに拭き取っておこう。\n\nそういうわけで、もしこの記事を見た人がいたら、今すぐ 2FA のバックアップコードのありかを確認しておくことをお勧めします。\n\n追記:\n\nDropbox サポートから連絡があり、1) 2FA のバックアップコードがない &amp;&amp; 2) 電話番号を登録していない、3) 他に登録されているデバイスがない、のトリプルコンボ達成済みのあなたのアカウントにはもうアクセスできません、と回答がありました。ま、そうですよね。\n\n"},{"frontmatter":{"slug":"/blog/android-horizontal-scroll-list-in-vertically","date":"Thu, 30 Apr 2015 16:07:59 GMT","title":"[Android] Horizontal scroll list in vertically scrolling list, and how to handle “ItemClick” event fired by each item","epoch":"1430410079"},"markdownBody":"\n\nOne thing I would have liked to implement in the all new Peatix Android app was this idea of “Home” screen. It shows what are relevant to you regardless of what type of objects they are. Very much like Google Now, which shows many different types of “cards” in one list.\n\nIn the list, I want to show as many “stuff” as possible in a small screen of a phone. The app could list things like tickets you have got, list of events the app recommends to you, tweets and FB posts related to an event you are going, etc etc. Lining up all of these items vertically in a scrolling view is an okay-idea; but it’s pretty obvious that the latter an item is listed, the latter it’s going to be actually seen (or to impress the user). As of this writing, Facebook and Twitter both put relevant items in vertically scrolling list, while some items (mostly Ads, some photos if there are many) occupies only a height of an item while showing many child items in horizontally scrolling list. I would have liked that in my app too.\n\nYou can see the guts of what I implemented in [one of my GitHub repositories](https://github.com/fumiakiy/HorizontalScrollSample). Here in [this commit I implemented very basic vertically scrolling list](https://github.com/fumiakiy/HorizontalScrollSample/commit/45af3bd8cbcad3fea1cf5ac256f6a07359a2a9cc) by using RecyclerView and its friends. Then I [added “ItemClick” event handler](https://github.com/fumiakiy/HorizontalScrollSample/commit/f8ff3871aa9ffa0684faf50c7febc4bcf08f6c4c) in the next commit. Simple, so far.\n\nNow I want to modify some of those items have its own child items that are horizontally scrolling. I added another layout XML that has a RecyclerView. So, the root RecyclerView has list of CardViews. Each CardView renders either [a text](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/res/layout/list_item_text.xml) or [another RecyclerView](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/res/layout/list_item_horizontal_parent.xml). Basically, many RecyclerViews are nested inside a RecyclerView. Each child RecyclerView has as many items its adapter gives it, and [renders each item using (happens to be) the same layout as is used in the parent RecyclerView to render a simple text](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/HorizontalItemsViewHolder.java#L22). So far so good.\n\nThe last but the hardest part to implement in a way that makes sense to me is to handle “ItemClick” event fired by each child items in a horizontally scrolling item. The very first idea that comes to my mind is to define OnItemClickListener in the [ViewHolder](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/HorizontalItemsViewHolder.java). If you think about it, each of the parent items in the vertically scrolling list fires the event to the handler that MainActivity defines; the same structure should work, so it’s natural for each ViewHolder to define OnItemClickListener for each of its own children.\n\nBut I dumped the idea because it didn’t make sense to me. ViewHolder should be a reusable utility class for a type of View/Layout rendered in a RecyclerView. It must not be tied to a particular item. But in order to implement OnItemClickListener, an instance of ViewHolder must know about either the list of items that its parent RecyclerView is rendering, or each item that is rendered. The latter makes the ViewHolder un-reusable. The former idea, well, having a reference in something that could be discarded at any point (RecyclerView aggressively recycles views) make hard the fighting NullPointerException.\n\nThese are what I have thought through, to conclude [my implementation of having both ItemClickListener (children and grandchildren)](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/MainActivity.java#L96-L114) in MainActivity.\n\nIt first detects if if’s a “touch-up” event. It then finds an item that must be under where the touch event occurs. If an item is a type of String, it fires the direct child’s OnItemClick (well, virtually said. It does what the event handler does inline in the example). If an item is not a type of String, it must be an array of String in this case, which means the item is a horizontally scrolling CardView. It [calculates offset to the CardView](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/MainActivity.java#L105) and [pass the calculated coordinates to the ViewHolder’s method to get a grand child item](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/HorizontalItemsViewHolder.java#L25). If the position of a grand child item is found, it looks for the grand child and fires OnChildItemClick (again, it writes [what it does in line](https://github.com/fumiakiy/HorizontalScrollSample/blob/master/app/src/main/java/com/luckypines/horizontalscrollsample/MainActivity.java#L108-L109)). Woot. Mission accomplished.\n\nI am not saying that this is the best way to handle these stuff though. I have a part of myself who doesn’t like this complicated implementation. Implementing ViewHolder pattern correctly is hard.\n\nPlease let me know what I am doing wrong if you see it. Thanks for reading thus far.\n\n"},{"frontmatter":{"slug":"/blog/butter-knife-vs-android-annotations","date":"Sat, 25 Apr 2015 23:04:24 GMT","title":"Butter Knife vs Android Annotations","epoch":"1430003064"},"markdownBody":"\n\nThere was a tech conference geared towards Android app developers, #DroidKaigi in Japan. I wasn’t able to attend unfortunately because it happened in Tokyo while I was in New York, but thanks to the folks who share most everything to the Internet, I could feel how it went. It sounds it went really well. Congratulations to the event organizers.\n\nOne thing that got me when I was reading those blogs and summaries about the conference was that [Butter Knife](http://jakewharton.github.io/butterknife/) sounded more popular and better loved by them than [Android Annotations](http://androidannotations.org/). I just sent a new Android app to our quality assurance process. The app was completely written from scratch by myself, which uses Android Annotations everywhere and I was a little concerned that I might have chosen the wrong library (again…?).\n\nBut it turned out that I wasn’t that much wrong. Butter Knife solves a problem that Android Annotations also solves, but it doesn’t have a critical part (for me) that Android Annotations helped me a lot while working on my app; Extra, FragmentArg and InstanceState.\n\nWithout these annotations, I have had to write \\_very correct\\_ code to handle saveInstanceState and onCreate and everything else. Now it might sound trivial - you just have to pass objects to an Activity you are starting and the code to handle them is well pattern-ed by now - but when it comes to handling orientation changes it gets incredibly hard to write “the correct” code. Fortunately, Android Annotations does the right thing for you with this tiny cost of you have to be careful that you are going to start Activity\\_ (with the trailing underscore), not the Activity you write (AndroidManifest.xml).\n\nTo be honest, we did have problems with Android Annotations in developing the previous versions of our app, especially with EFragment and AfterViews injections, but it turned out it was our fault - mostly because we wrote code that does “view initialization” in AfterViews methods, which turned out to be called every time the view is created. In Android, orientation changes destroy views and they are recreated. With some of those “gotchas” in mind, Android Annotations served really well for me this time.\n\nTo make it a little clear, I didn’t write code that use Butter Knife. I didn’t use EBean, EService, Rest or any other fancy attributes that Android Annotations offer either. EActivity, EFragment, ViewById, Click, OptionsMenu, OptionsItem, Extra, FragmentArg, and InstanceState are the ones I used and loved. I might try to use some more in the upcoming versions, but those only still make significant difference. I understand when someone say Android Annotations is simple overkill; it did concern me when I think of those unused features that Android Annotations offer, but hey, I managed to keep away from [Guava](https://github.com/google/guava) and the app still compiles without [multidex](https://developer.android.com/tools/building/multidex.html) enabled.\n\nI will write more about what I learned when I was writing the Android app soon.\n\n"},{"frontmatter":{"slug":"/blog/this-is-so-awesome-i-cant-help-but-posting-it","date":"Wed, 04 Mar 2015 16:34:15 GMT","title":"This is so awesome I can’t help but posting it here.","epoch":"1425486855"},"markdownBody":"\nThis is so awesome I can’t help but posting it here.\n\nFrom [http://www.mobileattack.com/android/another-wallpaper-apple-sliced-by-android-jedi/](http://www.mobileattack.com/android/another-wallpaper-apple-sliced-by-android-jedi/)\"\n\n![Image](https://64.media.tumblr.com/5e003a545527fe2f1e5349f6630cf612/tumblr_nkp4p3HvW21qz65w3o1_1280.jpg)"},{"frontmatter":{"slug":"/blog/unit-testing-android-app-using-robolectric-and","date":"Tue, 16 Dec 2014 20:16:00 GMT","title":"Unit testing Android app using Robolectric and Android Studio 1.0.1","epoch":"1418760960"},"markdownBody":"        \nI have been reluctant to write tests for my Android projects because…\n\n1.  I didn’t really know what to test. TextView.is.visible.and.hasText(“Hello World”). Er, um.\n2.  I did write some tests as experiments. It runs and start activities in my device. If I didn’t connect my device to my mac, it runs in the emulator. It is slow and it does destruct my focus on writing code.\n\n[At one of the greatest meetup experience of any kind that was held at Spotify NYC office by NY Android developers meetup group](http://www.meetup.com/android-developers-nyc/events/219023775/), I learned less painful ways of doing unit tests for Android dev. [Robolectric](http://robolectric.org/). It allows an Android project to run inside JUnit/IDE. Woot! I have got to try that.\n\nI thought that Robolectric was mostly a drop-in replacement of a standard Android testing framework that is installed when you create a new project in Android Studio. I was wrong. It took 8 hours worth of my spare time to get it finally working with \\`gradlew test\\` inside the Terminal window of Android Studio 1.0.1. It took another 4 hours to figure out running on JUnit test runner inside Android Studio 1.0.1 IDE.\n\nInstead of writing in a human readable language on how I made it work, I compiled my journey into a stream of commits in my GitHub repository. It should address many if not most of the issues you would be facing if you start doing the same stuff by yourself. I saw a lot of StackOverflow questions about it. Some of them are still applicable to Android Studio 1.0, but many of them were stale (don’t get me wrong. They were really helpful for me to figuring out where to look at. I appreciate it. That is why I list most of them in this writing later).\n\n[GitHub repository “AndroidAppWithRobolectricUnitTestsInAS101” by fumiakiy](https://github.com/fumiakiy/AndroidAppWithRobolectricUnitTestsInAS101)\n\nSo, try it by yourself (may be with some help from [my GitHub repository](https://github.com/fumiakiy/AndroidAppWithRobolectricUnitTestsInAS101)) and enjoy writing unit tests with [Robolectric](http://robolectric.org/) inside Android Studio 1.0.1!\n\nHere is the list of most helpful links that I found during my journey of making it all work.\n\n*   [https://github.com/robolectric/deckard-gradle](https://github.com/robolectric/deckard-gradle)\n    \n    This is the default “installation” suggested by Robolectric. It works. The contents of build.gradle are what I copied to the first half of my “MyApplication” project. But simply importing all gradle settings fails MyApplication because Robolectric currently supports API level 18 and the project Android Studio 1.0.1 creates targets 21.\n    \n*   [http://stackoverflow.com/questions/23867341/android-robolectric-does-not-support-api-level-1](http://stackoverflow.com/questions/23867341/android-robolectric-does-not-support-api-level-1)\n    \n    This SO is where I found a solution to API level issue. This one also reminded me that there are more than a few configuration you can specify in @Config annotation.\n    \n*   [https://github.com/square/assertj-android/issues/129#issuecomment-62205520](https://github.com/square/assertj-android/issues/129#issuecomment-62205520)\n    \n    After I added [AssertJ Android](http://square.github.io/assertj-android/) (that I also learned at the meetup), I started to see a build error again. This “temporary” solution is still required as of this writing to solve the issue.\n    \n*   [https://github.com/robolectric/robolectric/issues/1018](https://github.com/robolectric/robolectric/issues/1018)\n    \n    ResourceNotFoundException wasn’t there for the first time I started this “project”, but it was because I changed MainActivity extends from Activity. The default was ActionBarActivity that Android Studio 1.0.1 generates, which raises the ResourceNotFoundException. This thread helped me find “qualifiers” attribute. But the suggested value “v10” didn’t work because I was using suport library 21.0.3 which has “v11”.\n    \n*   [http://blog.blundell-apps.com/android-gradle-app-with-robolectric-junit-tests/](http://blog.blundell-apps.com/android-gradle-app-with-robolectric-junit-tests/) and [http://blog.blundell-apps.com/how-to-run-robolectric-junit-tests-in-android-studio/](http://blog.blundell-apps.com/how-to-run-robolectric-junit-tests-in-android-studio/)\n    \n    After I was satisified with running tests by \\`gradlew test\\`, I added JUnit configuration to the project so the tests can run inside IDE with some nice visual and debuggability. It failed in many ways. These tutorials by Blundell helped me a lot to figure out how to run it in the IDE. The document was not for Android Studio 1.0.1 and some information may be stale and not applicable by now (for example, I didn’t have to specify Alt JRE, and I didn’t have to create a “testClasses” gradle configuration), but I wasn’t able to figure out to do it by creating a Java library outside of the app project and refer to the app project from it, without these tutorials.\n    \n*   [https://github.com/robolectric/robolectric/issues/1334](https://github.com/robolectric/robolectric/issues/1334)\n    \n    I wasn’t able to “reproduce” this particular error when I was compiling my findings one step at a time, but I am sure I was hit by the error. This thread was where I found project.properties can specify libraries to look at. But later I found that you can also put library path to @Config attribute, which was handier for this project.\n    \n\nThanks to all of you who keeps records of what works and what doesn’t work. Hope this entry and [my GitHub repository](https://github.com/fumiakiy/AndroidAppWithRobolectricUnitTestsInAS101) also serve somebody in the future.\n\nLast but not the least, Thanks [Sean Kenny who did the presentation at the meetup](https://docs.google.com/presentation/d/1eF9B9Yi1SIGRxCrKQMhJbCFQQRf3RCuYNk0Ofos5FWE/edit). I also had to say thanks to [Erik Hellman who also did another great presentation at the meetup](https://docs.google.com/presentation/d/1e2jIRPc03BCfDi8wTWvvtipPsANru4KqbMkhYXfM1AI/edit). OMG, Spotify sounds like a great place to work. Thanks [New York Android Developers Meetup](http://www.meetup.com/android-developers-nyc/). I learned a lot from them.\n\n"},{"frontmatter":{"slug":"/blog/3-web-gihyojp","date":"Tue, 16 Dec 2014 18:16:21 GMT","title":"第3回　宮川達彦―最先端のWebエンジニアのキャリア：エンジニアの生存戦略｜gihyo.jp … 技術評論社","epoch":"1418753781"},"markdownBody":"\n\n[bulknews](http://weblog.bulknews.net/post/105334762130/3-web-gihyo-jp):\n\n> > 先を歩むエンジニアへのインタビューを通してエンジニアのキャリアについて考える本連載，今回は古くからPerlコミュニティで活躍し，最近ではWebテクノロジ情報発信のポッドキャスト「Rebuild」が話題の宮川達彦さんにお話を伺いました。\n> \n> WEB+DB Vol.83 で対談した記事がウェブで読めるようになってました。だらだらとしゃべっていましたが、うまいことまとまっていると思います :)\n> \n> [対談後に収録した Aftershow](http://rebuild.fm/64/) も合わせてどうぞ。\n\nhttp://gihyo.jp/lifestyle/serial/01/survival\\_strategy\\_engineer/0003"},{"frontmatter":{"slug":"/blog/twitter-login-using-twitterkit-but-without","date":"Fri, 12 Dec 2014 03:25:00 GMT","title":"Twitter Login using TwitterKit but without TwitterLoginButton","epoch":"1418354700"},"markdownBody":"        \n[Fabric](http://dev.twitter.com) is awesome. It was the happiest experience of my professional life when it was about to installing a SDK, when it was Fabric’s.\n\nThe app I was working on require Twitter login, and Fabric provides code that does heavy lifting for it. Unfortunately, [the documentation Twitter provides explains only about how to use the TwitterLoginButton](https://dev.twitter.com/twitter-kit/android/twitter-login) which I didn’t want to use, not because I didn’t like the design but the app I was writing required a smaller button.\n\nSo I dig a little bit of its code through Android Studio, another awesome software which made me love to write code in Java and for Android apps. I found a way to use my own button to go through what TwitterKit requires an app to do.\n\nYou just need to create a new TwitterAuthClient instance and call authorize method of it. It launches an activity and comes back to the calling activity (yours) by onActivityResult event. You handle the callback by sending the data again to authclient instance and that’s about it.\n\nSample code is on my github. \n\n[https://github.com/fumiakiy/CustomButtonTwitterLogin](https://github.com/fumiakiy/CustomButtonTwitterLogin)\n\n"},{"frontmatter":{"slug":"/blog/android-studio-v100-and-you-cant-build-your","date":"Tue, 09 Dec 2014 05:03:00 GMT","title":"Android Studio v1.0.0 and you can't build your project any more... here is how I fixed my project.","epoch":"1418101380"},"markdownBody":"\nIt’s v1.0 time! Android Studio is updated. My project which uses Android Annotations and Proguard failed to build after the update with pretty cryptic messages.\n\nAfter a few minutes digging, I was able to make it build again by applying the following patch.\n\n```\n--- a/app/build.gradle\n+++ b/app/build.gradle\n@@ -7,7 +7,7 @@ buildscript {\n     }\n     dependencies {\n         // replace with the current version of the Android plugin\n-        classpath 'com.android.tools.build:gradle:0.12.+'\n+        classpath 'com.android.tools.build:gradle:1.0.0'\n         // Since Android's Gradle plugin 0.11, you have to use android-apt >= 1.3\n         classpath 'com.neenbedankt.gradle.plugins:android-apt:1.4+'\n@@ -28,7 +28,7 @@ \n\n apt {\n     arguments {\n-        androidManifestFile variant.processResources.manifestFile\n+        androidManifestFile variant.outputs\\[0\\].processResources.manifestFile\n         resourcePackageName android.defaultConfig.applicationId\n\n         // If you're using Android NBS flavors you should use the following line\n         // instead of hard-coded packageName\n@@ -53,14 +53,14 @@ android {\n     }\n     buildTypes {\n         release {\n-            runProguard true\n+            minifyEnabled true\n             proguardFiles getDefaultProguardFile('proguard.txt'), 'proguard-rules.pro'\n         }\n         debug {\n-            runProguard false\n+            minifyEnabled false\n         }\n         staging {\n-            runProguard true\n+            minifyEnabled true\n             proguardFiles getDefaultProguardFile('proguard.txt'), 'proguard-rules.pro'\n         }\n```         \n\nJust postin' for someone including future myself who could struggle with it...\n\n"},{"frontmatter":{"slug":"/blog/on-not-following-my-passion","date":"Mon, 06 Oct 2014 19:43:50 GMT","title":"On not following my passion","epoch":"1412624630"},"markdownBody":"\nMike Rowe’s response to why he thought people should not follow his/her passion is getting a buzz. To simply summarize it, he argues that one should not follow one’s passion for too long because many if not most cannot reach there anyway; and he believes those people should see it earlier rather than later to make their lives happier.\n\nI am not rich. I am not even close to be rich at all. I am not what I thought I would want to be when I was younger. I wanted to be a journalist. I wanted to make my living by telling others stories from the rest of the world in “a journalistic way”. But I wasn’t able to even start my carrier as a journalist. I was employed by a systems integrator after I failed all of my applications to most of the newspaper publishers in Japan when I graduated school.\n\nBut I can safely say I am very fortunate to be where I am. Not because now those newspaper companies are struggling; but because I found what I would love during my tenure since then. It turned out I loved writing software to solve someone else’s problem.\n\nThe first batch of “programs” I wrote - or more like I typed - was games printed on magazines. Basic Magazine and later I/O. I had fun with it with my brother sitting next to me. I took basic computer programming classes of Natural Science division when I finished getting most of grade points to graduate college with my major. But it never got me until I was assigned to the R&D department at the second year at my first company when I found that I loved, and was very good at, learning something new about computer software and telling people about my learning by writing and speaking. That was when my carrier really has started.\n\nI could have followed my heart and have tried harder to be a journalist, by for example spend a whole year abroad and apply for the next year’s new hire opportunity. I was too scared of doing that. Instead I grabbed a job that wasn’t what I thought I would be - a programmer. Since then, like someone that Mike Rowe shared in his response, “I found a way to love it”. With a little bit of luck of department assignment.\n\nI am in no way eligible to telling someone else something to do or something not to do; I am nobody anyway. But I can say to myself now that I am doing what I wanted to do, by “telling people what I learned about the world”; That world may be smaller, or virtual world of computer programs. Bonus is that I am paid to do it and colleagues around me (I believe) thinks I am doing it good.\n\n"},{"frontmatter":{"slug":"/blog/mike-rowes-must-read-response-to-an-alabamian-who","date":"Mon, 06 Oct 2014 18:33:07 GMT","title":"Mike Rowe's must-read response to an Alabamian who asked why he shouldn't follow his passion - Yellowhammer News","epoch":"1412620387"},"markdownBody":"\n> When it comes to earning a living and being a productive member of society – I don’t think people should limit their options to those vocations they feel passionate towards.\\[…\\]\n\nhttp://yellowhammernews.com/faithandculture/alabamian-gets-schooled-mike-rowe-dirty-jobs/"},{"frontmatter":{"slug":"/blog/microservices-history-iterates-etc","date":"Fri, 29 Aug 2014 18:52:00 GMT","title":"Microservices, history iterates, etc","epoch":"1409338320"},"markdownBody":"\n\nIt was in an episode of [#rebuildfm](http://rebuild.fm) where I heard the term [microservices](http://martinfowler.com/articles/microservices.html) for the first time. It sounded nothing new to me, but it’s among the latest buzzwords.\n\nInter process communication\n---------------------------\n\nI started my carrier as an instructor/conference speaker at around 1995. Windows NT 3.51 finally made threads available to COM based applications through it’s intrduction of MTA (multi-threaded apartment). COM or Component Object Model was a technology that enabled inter-application communication through OLE2 (Object Linking and Embeddding). OLE(1) enabled an Excel speadsheet be embedded in an Word document. OLE2 made a step further and made the underlying technology available to 3rd party developers through COM. In other words, it was unix pipe for a Windows GUI application. Inter-process communication took place on top of COM’s intra-process, inter-thread communication.\n\nInter-machine communication\n---------------------------\n\nWith Windows NT 4 in 1996, Microsoft took a step even further and introduced DCOM, or Distributed COM. By 1996 with help by popularity of Windows 95, corporate intra-network became pretty popular. Client-Server system. Visual Basic application connected to Oracle 6 database. 3-tier, Windows DNA. Inter-machine communication took place. DCOM tried to solve the problem around programs communicates over machine boundary by leveraging COM inter-process communication.\n\nIt allowed programmers feel as if they were writing code that communicated with other COM interfaces over process boundary, even when they were writing code that actually communicated over machine boundary, over corporate, 100Base-T LAN.\n\nDCOM even introduced DCOM over HTTP later so COM interfaces can be exposed over the Internet.\n\nJ2EE/EJB, CORBA, the same story, different companies involved.\n\nSOAP\n----\n\nSOAP, or Simple Object Access Protocol, was born to make inter-machine communication over HTTP a reality. RPC can be replaced with HTTP protocol. Network data representation (NDR) can be replaced with an XML document. But SOAP is still a protocol for Object-RPC systems. It had been so until SOAP version 1.2. It was a tragedy for SOAP that SOAP v1.1 became too popular. Most dummy implementations of it naively believe RPC calls should look the same to programmers regardless of how long “the wire” was. SOAP 1.2 is a messaging protocol. It specifies a message format in XML Infoset which aimed to create an abstraction layer on top of XML document which is defined as text.\n\nSOAP 1.2 defines headers and bodies and processing models; it looks very much like what HTTP is being used today. SOAP 1.2 was intended to use more transfer protocols than just HTTP because HTTP was text based and thought to be slow; SOAP over SMTP could also be possible; SOAP could also be used even in-process communication (XML Infoset can be serialized to a binary format). It was too late; when Microsoft introduced .NET Framework, even most product managers in Microsoft still saw SOAP to see objects, at the time .NET objects written in C#. WSDL does have a way to define document oriented interface as well as legacy, rpc oriented interface. Document oriented interface never became popular. “Wise” brains started to criticize how stupid the idea was to event think about doing RPC over the wire. Coarse-grained messages. Messages, not method calls. Services not objects. Service oriented architecture.\n\nSOA\n---\n\nIt was interesting few months of my carrier during these days, because I was also able to see that there was the other side of the world. Blue pill? Red pill?\n\nIn the other side of thew world, Movable Type made trackback familiar. Dave Winer advocates RSS 0.91 and 2.0. RSS reader applications. NOKIA lifeblog protocol that can post an entry to TypePad from your cellphone.\n\nThese are based on this simple notion - sending an XML document over HTTP (be it GET or POST), is very usable for inter-process communication, without a concrete specification on which all tech giants agree. View source. Blog hacks. (I feel funny that Fiddler the sniffer became very popular these days. It was during these days I first used Fiddler and loved it until I switched to Mac.)\n\nAnd Google Maps. Ajax is not very much a “protocol” but more like a good old client-server communication. Its JavaScript calls fine grained methods. Google servers are fast enough to process that many number of method calls fast.\n\nThere was this buzzword - SOA during the timeframe around my carrier. In SOA, all participating endpoints agree not only on a protocol and a processing model, but they also agree on a very concrete “business process document schema”, at least in their ideal world. RosettaNet, BPEL, BizTalk.\n\nOn the other side though, Ajax exploded. RPC between JavaScript on a browser and an HTTP endpoint got popular among programmers once again. It does feel easier to architect coarse grained message with these important idea of “connections can be lost; messages can be half-broken, etc, etc”. All you do is call a simple method like “add(1, 2)” and get “3” as a result. Programmers love it. RPC is easier than Messaging.\n\nIt was SOA and this idea of “all parties agree on an XML schema” that finished me in that part of the world. It was a ridiculous idea to me given trackbacks and RSS were already working in practice out there.\n\nI joined Amazon as a technical evangelist, to adovoate to programmers. that this part of the world was more fun. I wrote magazine articles about it. I talked about it at some tech conferences. I let other programmers know how fun it would be to write code that does internetwork communication through “API” until I realized it would be fun for me too. I stopped talking and started writing code at Six Apart.\n\nWell now what?\n--------------\n\nFast forward a few years, I am still writing code that is getting large codebase. Some parts in my code communicate inside the system itself, but over HTTP. I heard the word microservices. Where limited number of coarse grained messages are transferred between services in a system. Wait, is this deja vu?\n\nNo, I guess not, partly because it is still about a monolithic system (if you can call “Amazon.com” a monolithic system). It is not about Amazon.com communicates with Google.com (which was what SOA circa 2005 was trying to accomplish). Also everybody in this generation understands that calling an endpoint over HTTP is way more expensive than calling a method inside a process. C10K problems.\n\nWell defined interface, but the parties who must agree on that interfaces are still within a firewall. History doesn’t just repeat; it iterates like it draws a spiral.\n\nI do feel sad when this generation points fingers at and laughs at SOAP/WSDL though. It surely served my carrier. I even feel grateful to Microsoft to adapt to SOAP, for if it were not them, I wasn’t aware of those XML related stuff until a few years later, which should have made my carrier very different.\n\nBy the way, COM taught me this thing called interface based programming too. COM interfaces have functions but not data. COM is not OO - you start by writing an interface definition by this language called IDL. Doesn’t it sound familiar?\n\n"},{"frontmatter":{"slug":"/blog/darvish","date":"Wed, 20 Aug 2014 15:04:00 GMT","title":"他人の理想のために生きてないわ。自分が生きたいように生きるし、言いたいことを言う！ - Darvish Yu","epoch":"1408547040"},"markdownBody":"\n[https://twitter.com/faridyu/status/502102389202370560](https://twitter.com/faridyu/status/502102389202370560)"},{"frontmatter":{"slug":"/blog/matsumoto","date":"Thu, 14 Aug 2014 21:47:25 GMT","title":"やりたいことは本当はすごく沢山あって   だけど大体の物は既にあるわけじゃないですか。\n既にある物に勝つためには\n優れたアイデアか   あと20年続けるモチベーション\nのどちらかが必要だと思います。","epoch":"1408052845"},"markdownBody":"\n\nまつもとゆきひろさん\n\n（ Rebuild.fm EP53 [http://rebuild.fm/53/](http://rebuild.fm/53/) )"},{"frontmatter":{"slug":"/blog/on-startups-how-we-develop-new-features-extremely","date":"Thu, 05 Jun 2014 03:26:11 GMT","title":"On Startups: How we develop new features extremely fast","epoch":"1401938771"},"markdownBody":"\n\n[eventjoy](http://blog.eventjoy.com/post/87807448121/on-startups-how-we-develop-new-features-extremely-fast):\n\n> It’s no secret that we’re a small team, two people to be exact. Yet, people constantly ask us how we’re able to roll out so many large features so quickly. We thought we’d share our design and development process. To do this we’ll use our brand new iOS attendee app as a case study.  \n> \n> **1. Use…**\n\nhttp://blog.eventjoy.com/post/87807448121/on-startups-how-we-develop-new-features-extremely-fast"},{"frontmatter":{"slug":"/blog/covert-redirect-vulnerability-with-oauth-2","date":"Thu, 08 May 2014 02:27:45 GMT","title":"Covert Redirect Vulnerability with OAuth 2","epoch":"1399516065"},"markdownBody":"\n\n[bulknews](http://weblog.bulknews.net/post/85008516879/covert-redirect-vulnerability-with-oauth-2):\n\n> **tl;dr** Covert Redirect Vulnerability is a real, if not new, threat when combined with Implicit Grant Flow (not Code flow)\n> \n> This [Covert Redirect Vulnerability](http://tetraph.com/covert_redirect/oauth2_openid_covert_redirect.html) in OAuth 2 is an interesting one.\n> \n> There’s a couple of [defending](http://www.thread-safe.com/2014/05/covert-redirect-and-its-real-impact-on.html) [arguments](http://dannythorpe.com/2014/05/02/tech-analysis-of-serious-security-flaw-in-oauth-openid-discovered/) that this isn’t a flaw in OAuth itself.\n> \n> While I agree that…\n\nOpenID Connect mandates full length matching in redirect\\_uri validation according to other blogs, which should be good enough to avoid this problem, I guess. I have to check spec and implementation though.\n\nhttp://weblog.bulknews.net/post/85008516879/covert-redirect-vulnerability-with-oauth-2"},{"frontmatter":{"slug":"/blog/on-being-an-op-of-openid-connect","date":"Sun, 16 Mar 2014 21:43:57 GMT","title":"On being an OP of OpenID Connect","epoch":"1395006237"},"markdownBody":"\n\nThe more I understand OpenID Connect, the more I understand that it is really only for those few big guys, when it comes to a server implementation.\n\nLooking at some code and spec and it sounds easy to be an Open ID provider (i.e. a server), for it’s as simple as implementing OAuth2 server. But this id\\_token thing is new for me. I got used to using Facebook’s signed\\_request, but only as a client or a relying party. Now when it comes to becoming a server, it seems you must implement this mechanism by yourself.\n\nComponents that construct id\\_token are freely available out there - JSON WebToken, Base64, and HMAC signatures. But the real problem is that you have to manage the cryptographic signature that you rely on very carefully. For example, Google rotates its public key every day (and it says it’s cacheable! is it really?). If it is one of reference implementations, I have to implement it in a similar way too. Sigh.\n\nThis alone makes me believ the points made in my previous entry stronger - this is really only for big guys. If you can’t do this, you can’t afford storing people’s passwords in your system.\n\nI personally have no problem asking Google and Facebook to behave as my identity provider for all websites that require my information to do some awesome stuff. But I also know, as one of those online service providers, that people do hate logging in via a third party token; they prefer give us their email addresses and passwords. It’s likely due to experiences that hurt these people when they gave a service their Facebook access\\_token; a service may have posted something embarrassing on behalf of them, for example.\n\nThose who had hard time in that way may have learned that giving garbage email address and using 1password is the safest way because most service providers like us are potentially evil. All of us.\n\nI wonder how OpenID Connect crosses this hurdle - becoming a server is hard and that may be fine, but it is a big problem if using it as a client (RP) may not help a service grow its user base because of the very issue OpenID Foundation is trying to tackle - our passwords may have to be stored in these many places because that is what users want us to do.\n\n"},{"frontmatter":{"slug":"/blog/is-openid-connect-a-new-version-of-openid","date":"Sun, 09 Mar 2014 19:34:00 GMT","title":"Is OpenID Connect a new version of OpenID?","epoch":"1394393640"},"markdownBody":"\n\nShort answer: who cares?\n\nTl;dr; no.\n\nOpenID Connect seems getting traction. As one of implementers of an OpenID and OpenID 2.0 implementations, I am very curious about why the foundation keeps using the term OpenID with it.\n\nOpenID, to me, was a revolution started by a few brightest minds of our generation. Its aim was to be “a decentralized identity system, but one that’s actually decentralized and doesn’t entirely crumble if one company turns evil or goes out of business.” ([The top page of OpenID.net in 2005)](http://web.archive.org/web/20050924033518/http://www.danga.com/openid/).\n\nIs OpenID Connect still a decentralized identity system that doesn’t crumble if Facebook or Google or AOL or whichever internet giant goes out of business? No, it is not something like that. But who cares? If they go, everybody goes.\n\nIt has been that way OpenID evolved since OpenID 2.0, which allows you to start an authentication process by merely stating you are one of an account holders of “yahoo.com”. OpenID, because it is to verify that you own your own domain such as “www.lukypines.com”, always had to ask you to enter “http://www.luckypines.com”, or just for another example, “http://itsme.yahoo.com” (it is not a valid URI as of this writing). Neither Yahoo nor Google gives each user account its specific subdomain, like LiveJournal and TypePad did, like other hip services did too at that time. So that made insufficient for a person to be an account holder in Google, Yahoo, AOL, … to have him/herself to be an OpenID verified identity because s/he doesn’t own a domain.\n\nOpenID 2.0 solved the problem by adding OP Identifier to the spec. Instead of asking a user a domain s/he owns, it allowed a user to tell a relying party a domain owned by one of internet giants who should identify the user on behalf.\n\nAt this point, OpenID 2.0 becomes a protocol for a select few centralized identity providers, which only a few internet giants could manage your identity. It may be open because the protocol is open, and there are many reference implementations that are open source. but OpenID 2.0 is not “a decentralized identification system” any more.\n\nIt was interesting that OpenID was “a visionary’s tool that never got much commercial adoption” for the OpenID foundation, and OpenID 2.0 had adoption problems because it relies on XML (“[What is the history of OpenID?” - http://openid.net/connect/faq/](http://openid.net/connect/faq/)).\n\nThose are probably true; as one of Six Apart employees (2006 - 2010), I saw OpenID implementations everywhere; but those are all by visionaries and none was by those giants. It was a start-up project. I saw a lot of implementations of many many things that relied on XML. But those were not by someones who should be considered majority enough.\n\nNow, finally, OpenID Connect launches. It’s not even an OpenID 3.0. It’s just a profile of OAuth 2.0. I am confused; why shouldn’t it be OAuth 2.0 + Authentication Profile? I mean, those concerns that OpenID Foundation raises are valid; relying too much on HTTP redirection don’t work well in this native mobile app eco-system. XML is a thing in the past; JSON is the only pragmatic option of serializing an opaque data between two computer systems and communicate them over the wire. But it doesn’t solve the problem in a way the visionaries saw in OpenID. Why does it have to be a version of OpenID?\n\nI may be feeling a little too nostalgic; it was good old days for me when everything around me looked brighter. I am not saying OpenID Connect or OpenID Foundation is a BS. As a matter of fact I’ll implement it to our own service soon. But I can’t help but feeling uncomfortable when someone says it is the “Identity Layer of the Internet”, when all it does is to encourage us to rely on very small number of giants and forget about my owning my own identity. OpenID encouraged me to be free from authority; OpenID Connect encourages me to rely on it.\n\nYes, I’m interested in [Camilstore](https://camlistore.org/) too although I haven’t done anything with it yet.\n\n"},{"frontmatter":{"slug":"/blog/my-first-public-presentation-in-english","date":"Thu, 06 Mar 2014 04:12:10 GMT","title":"My first public presentation in English","epoch":"1394079130"},"markdownBody":"\n\nDue to our CEO’s absence, I took the stage of one of the greatest meetup around the world, New York Tech Meetup last night, to introduce ColorSync. You can watch it here until it would be taken down (I’ll appear at around 31:10).\n\n[http://mlb.mlb.com/media/player/entry.jsp?calendar\\_event\\_id=14-403620-2014-03-04&source=NYTM](http://mlb.mlb.com/media/player/entry.jsp?calendar_event_id=14-403620-2014-03-04&source=NYTM)\n\nIt was supposed to be a 3 minutes lightning talk. 3 minutes! I have never done anything that short. It was also an introduction to our product, for which people in the company has different opinion about what to talk. But in the end I ignored most of the suggestions because of two reasons; it wouldn’t be anything closer to good if I didn’t speak what I thought I’d talk in 3 minutes in my second language. And I was also pretty confident of myself being able to give a good talk. After all, I used to be a professional conference speaker for more than five years. No colleagues in the office saw my speaking, so it was understandable that they were worried about me. But it was what I was very good at doing. Now they must know. I see very positive tweets which makes me proud.  \nThe question now is, should I seek more opportunities to talk about something in public? I found that it was surprisingly comfortable giving the talk last night. Maybe I should. But then the next question is if I have something to talk about which could be interesting to audiences. That, I must answer myself.\n\n"},{"frontmatter":{"slug":"/blog/why-did-i-want-to-stop-using-jquery-as-much-as-i","date":"Sun, 16 Feb 2014 21:01:02 GMT","title":"Why did I want to stop using jQuery as much as I can and move on to AngularJS","epoch":"1392584462"},"markdownBody":"\n\nI didn’t like to use jQuery very much when I started building Peatix. What I didn’t like most was the fact that by merely writing $(’.class-yeah-its-just-a-marker’) you could jump here from there and everywhere. I am a crazy old man. I learned how to program by learning Structured Programming. Yes, that “avoid goto, everything you write you can write in a sequence, loop or conditional blocks”. Yeah I am old. What are you looking at?\n\nTo me jQuery selector is a GOTO in BASIC (and any other languages). It’s actually a feature that only advanced programmers can use it correctly for a few correct reasons. Yet similar to GOTO in BASIC, it is always those someones who are not real programmers who don’t have solid understanding of whatever programming paradigm who tend to use jQuery. And the code written by those someones breaks my beautifully structured, object oriented, whatever what'ed code.\n\nWell, jQuery is not to be blamed. JQuery is great. JQuery saved us a lot. But it’s time for me to move on to more structured way of writing frontend code.\n\nEnter AngularJS. Until last week I only took a few glances of its tutorial, and read a few articles about it. It sounded like you would be able to separate your HTML completely from someone else’s jQuery. It sounded nice.\n\nI started writing my first AngularJS controller. The web site that I took had a simple search function and the backend API is quite nicely written by my colleague. I only had to move the code that was written with Underscore and jQuery to AngularJS.\n\nIt turned out it was actually mere three days job to fully migrate the page to be based on AngularJS. Part of the reason is that the page was already well separated into logic and view thanks to my colleague. I saw a few holes in here and there while I was working on that page, which I will explain in the following entry.\n\nSo far I like how I write AngularJS controller and a few helper modules, and tests. It doesn’t make me feel anxious like jQuery did when I wrote a jQuery selector that was too vulnerable to a change to HTML/CSS.\n\n"},{"frontmatter":{"slug":"/blog/moved-to-t-mobile","date":"Sun, 09 Feb 2014 20:54:00 GMT","title":"Moved to T-Mobile","epoch":"1391979240"},"markdownBody":"\n\nI have been on Straight Talk $45 plan for two months after I got a Nexus 5. It has been very fine (I’m in Manhattan, New York most of the time) but I used the line in a very limited amount of time.\n\nI also see that I will be accompanied by someone daily real soon, which means she will also need a phone line. Then I remembered [the post by Tatsuhiko about how T-Mobile was awesome](http://weblog.bulknews.net/post/68863788422/t-mobile-global-roaming-awesome). We will also occasionally go abroad (Japan mostly), and I have already had issues that I haven’t had a cell phone line while I was in Tokyo several times by now. If T-Mobile family plan with unlimited international roaming (data and text) works for him, it should work well for us too.\n\nThe concern though is that T-Mobile doesn’t have too good reputation regarding coverage and quality of bandwidth. \n\nCall drops, LTE unstable, narrower coverage, weaker signals, etc. Well, since I am on a sim-free phone, I can test drive it for a month with a prepaid plan and see how/if it works, can’t I?\n\nIt turned out that [Walmart has this value offer that you can get unlimited text and data (first 5GB on LTE) and 100 minutes of talk](http://www.walmart.com/ip/T-Mobile-SIM-Kit/24099996). I don’t need many talk minutes (hey, I received exactly 2 calls from someone, and called absolutely nobody in these few months over cell phone), I probably need some data. Straight Talk was 2.5GB on LTE in 30 days cycle. That’s doubled on this T-Mobile/Walmart plan.\n\nSo, here I am; I just activated my Nexus 5 on the T-mo line. Let’s see how the carrier is “the fastest nationwide 4G LTE network” as it is advertising as of Feb. 2014.\n\n"},{"frontmatter":{"slug":"/blog/david-heinemeier-hansson-at-startup-school-2008","date":"Wed, 05 Feb 2014 21:01:00 GMT","title":"David Heinemeier Hansson at Startup School 2008","epoch":"1391634060"},"markdownBody":"\n\nDavid Heinemeier Hansson at Startup School 2008 (with slides)\n\n[https://www.youtube.com/watch?v=0CDXJ6bMkMY](https://www.youtube.com/watch?v=0CDXJ6bMkMY)\n\nIt was from 5 years ago??? Yet it is so relevant to me, to us right now. I don’t have much idea about what VCs would do to you because I only met one and only one person from a VC in person. And I wasn’t too impressed on what he was trying to tell us; I didn’t see anything relevant to the state of our company at the point when I heard him talking. He was too loud to be ignored either.\n\nI don’t know if it would be better not to get a VC on board, but I know that without their money we are not where where we are today because we would have run out of money. But then again, does it have to be that way, that we hire this many people, have a decent office, run this many servers and pay this money to some  web services that can be replaced with a few OSS stack.\n\n"},{"frontmatter":{"slug":"/blog/collecting-pictures-using-instagram-api","date":"Sun, 02 Feb 2014 20:33:00 GMT","title":"Collecting pictures using Instagram API","epoch":"1391373180"},"markdownBody":"\n\nInstagram has an API. That’s good. But its document was a little bit confusing to me. This is how I ended up implementing Perl code that populates Instagram pictures that have a hashtag of my interest.\n\nFirst confusion - is authentication required or not?\n----------------------------------------------------\n\nTo get information of pictures with a hashtag, Instagram API offers “Tag” endpoints. [The document about Tag endpoint](http://instagram.com/developer/endpoints/tags/)s says though it requires authentication, thus requires access_token parameter. But wait, whose access_token does it require, when all I want is information about publicly viewable photos with a hashtag?\n\nI thought it didn’t make sense to give my own access_token that does work on behalf of my own user account. Or any user account. My intention was to implement a backend service that works periodically to collect pictures with certain hashtags. I just ignored the document and just gave my app’s client_id that I generated on Instagram console. As expected, it worked.\n\n### Point #1: you probably don’t need access_token and all of those pita steps to get OAuth2 access token from a user account.\n\nSecond confusion: how do I paginate the results to get more results?\n--------------------------------------------------------------------\n\nTwitter API does a great job by offering since_id and max_id parameters, so we as a client developer doesn’t have to consider duplicated tweets from two result sets from API calls. [Twitter documentation about “Working with Timelines”](https://dev.twitter.com/docs/working-with-timelines) is really well written and easy to understand what we should do.\n\nSince Instagram offers same type of data that is a stream of posts ordered by pictures posted descending, it has similar pagination parameters; it is only more confusing.\n\nInstagram API names those paramters as max_tag_id and min_tag_id. [Instagram API also returns “pagination” meta data](http://instagram.com/developer/endpoints/#pagination) in API results, which has “next_url” value which looks something like this\n\n[https://api.instagram.com/v1/tags/puppy/media/recent?client_id=xxx&max_tag_id=1391400658711](https://api.instagram.com/v1/tags/puppy/media/recent?client_id=xxx&max_tag_id=1391400658711)\n\nIt also carries two more important values\n\n```\n{  \n    \"next_max_tag_id\":\"1391400658711\",  \n    \"min_tag_id\":\"1391400689156\"  \n}\n```\n\nCalling the API by using the next_url value (note that it has the same id as next_max_tag_id has) returns the next set of results which carries the newest pictures whose id is less than max_tag_id.\n\nAs my goal is to get pictures periodically, storing next_max_tag_id for the next iteration doesn’t make sense because specifying that value narrows down the results to something that I always have collected.\n\nSo, I rather stored min_tag_id and called the API next time with something like \n\n[https://api.instagram.com/v1/tags/puppy/media/recent?client_id=xxx&min_tag_id=1391400658711](https://api.instagram.com/v1/tags/puppy/media/recent?client_id=xxx&min_tag_id=1391400658711)\n\nUnfortunately, this returned nothing. It was obvious after a few more experiments and scratching my head, that because what I wanted was pictures posted after the biggest id, but Instagram calls it “min_tag_id”! Instagram calls it small ids as “max”, or “next” when in fact it is actually about getting older data, instead of new data that I assumed.\n\nSo the final code of mine stores the “min_tag_id” in the database for the next iteration, and calls the API with min_tag_id=<stored min_tag_id>, to avoid getting data that is already in our database.\n\n### Point #2: “max” and “next” could mean different things to different heads.\n\nIn my humble opinion though, Twitter API does smarter job here as it called those parameters as since_id and max_id, less confusing at least to me.\n\n"},{"frontmatter":{"slug":"/blog/migrating-google-login-from-openid-20-to-google-3","date":"Sun, 26 Jan 2014 18:03:00 GMT","title":"Migrating Google login from OpenID 2.0 to Google+ (Part 3 of 3)","epoch":"1390759380"},"markdownBody":"\n([Part 1 of this story](/blog/migrating-google-login-from-openid-20-to-google-1))\n\nGoogle+ Sign In returns id_token that is a JWT encoded information of various stuff related to the account, along with access_token which could be used to call other Google APIs. In order to get the OpenID ident for the user, I had to decode JWT and that requires a key to decode it.\n\n[According to another document about how to validate id_token](https://developers.google.com/accounts/docs/OAuth2Login#validatinganidtoken), the key is a x509 public key whose private key is used to sign the JWT. According to the same document, the key could be retrieved by going to [https://www.googleapis.com/oauth2/v1/certs](https://www.googleapis.com/oauth2/v1/certs). Simple wget returned this.\n\n```\n{\n  \"5f1f4e771e997d21a08add83acbf18c6cb4e5473\": \"-----BEGIN CERTIFICATE-----\\nMIICIDCCAYm..Mh+Y=\\n-----END CERTIFICATE-----\\n\",\n \"729d763c1d8254b3a7f8dd6df19edea7ca836b7b\": \"-----BEGIN CERTIFICATE-----\\nMIICITCC...DeQG1z\\n-----END CERTIFICATE-----\\n\"\n}\n```\n\nThere are two. Which one was used? Going back to the [JWT spec](https://metacpan.org/pod/JSON::WebToken) turned out that a \"JWT is represented as a sequence of URL-safe parts separated by period (\".\") characters\". And after also reading through the code of the cpan module, I tried to decode the first part of MIME encoded JWT.\n\n\n```\n1> x MIME::Base64::decode_base64( substr( $id_token, 0, index( $id_token, '.' ) ) );\n{\"alg\":\"RS256\",\"kid\":\"5f1f4e771e997d21a08add83acbf18c6cb4e5473\"}\n```\n\nYeah, the \"kid\" is what I just saw in the JSON returned from /certs endpoint from Google. This should be what $key should have.\n\n```\n2> $key = \"-----BEGIN CERTIFICATE-----\\nMIICIDCCAYm..Mh+Y=\\n-----END CERTIFICATE-----\\n\",\n3> x JSON::WebToken->decode($id_token, $key);\nunrecognized key format\n```\n\nOh cpan module, how I &hellip;. what?\n\nFast forward what really happened during my struggle to find it out, it turned out that the error was from [Crypt::OpenSSL::RSA](http://api.metacpan.org/source/PERLER/Crypt-OpenSSL-RSA-0.28/RSA.pm), because it doesn't accept a certificate, but does a public key. Of course. Simple openssl command should give it to me.\n\n```\n$ openssl x509 -pubkey\n-----BEGIN CERTIFICATE-----\nMIICIDCCAYm\n...\nMh+Y=\n-----END CERTIFICATE-----\n-----BEGIN PUBLIC KEY-----\nMIGfMA0GCSqGSIb3DQE\n...\n```\n\nGiving this public key as $key finally gave me what I wanted.\n\n```\n$VAR1 = {\n  'exp' => 1390759437,\n 'iss' => 'accounts.google.com',\n'openid_id' => 'https://www.google.com/accounts/o8/id?id=AItOawkFEXN4pg8u52IM_YLN86LIolXUxqKrU3M',\n'sub' => '11843998...',\n 'azp' => '...',\n 'iat' => 1390755537,\n...\n };\n```\n\nAaaahhhh, it was about 3 hours of writing code, reading documents, experimenting and failing and starting over, but here I got my OpenID ident by logging in through Google+ Sign In. With this and \"sub\" key, Peatix can now find an account associated to an Google account no matter if the user was created by our OpenID implementation before, or by the new Google+ implementation.\n\nFinally, **big kudos to Google**. Google does a good job of supporting long-time users of their API backend. Seriously, it was really easy in the end, after I was able to decode the statement \"data-openidrealm is what you need\" to something like\n\n```\n\"Yeah, Google should not have implemented two separate code in order to support both Hybrid and Pure server flow; that means I should be able to specify this \"openidrealm\" somewhere in my pure OAuth2 code.\"\n```\n\n\nAfter decoding the statement to something like that, I was able to find other mentions to openid.realm in their documents, which told me how to do Pure server-side OpenID migration.\n\n"},{"frontmatter":{"slug":"/blog/migrating-google-login-from-openid-20-to-google-2","date":"Sun, 26 Jan 2014 17:15:00 GMT","title":"Migrating Google login from OpenID 2.0 to Google+ (Part 2 of 3)","epoch":"1390756500"},"markdownBody":"\n\n([Part 1 of this story](/blog/migrating-google-login-from-openid-20-to-google-1))\n\n\nGoogle published an article in December 2013 that says \"[Upgrading to Google+ Sign In](https://developers.google.com/+/api/auth-migration)\". There it says that if you add \"data-openidrealm\" to your G+ button, you could get the user\"s OpenID ident. Woah! That will solve my problem!\n\n\nReading through the document, though, turned out that it might not be that easy - because the document tells how to do that only by using either Client-side flow of Hybrid server-side flow - some things they recommend. Our sign-in however is completely done in Pure server-side flow (because it was OpenID before). Migrating the \"Login\" button itself to adapt to Hybrid flow is not impossible but it will be painful.\n\n\nSo, I started to hack over the document. The document basically says you just add your OpenID realm (or [trust_root](https://metacpan.org/pod/Net::OpenID::ClaimedIdentity#trust_root)) to data-openidrealm attribute to your button\"s element, and it does all the heavy lifting. It then links to the [page that describes OpenID Connect.](https://developers.google.com/accounts/docs/OpenID#openid-connect) It says that\n\n\n```\nFor applications that use OpenID 2.0, the authentication request URI may include an openid.realm parameter.\n```\n\n\nOK that\"s what I want to hear. So this code should do\n\n\n```\nmy $uri = URI->new( 'https://accounts.google.com/o/oauth2/auth' );\n $uri->query_form(\n client_id => _config()->{ client_id }\n , response_type => 'code'\n , scope => 'https://www.googleapis.com/auth/plus.login'\n , access_type => 'online'\n , redirect_uri => _redirect_uri()\n , state => time()\n , 'openid.realm' => 'http://peatix.com'\n );\n return $c->res->redirect( $uri );\n```\n\n\nThe only part that is really different from what we use for Facebook login (which is also based on OAuth2 but different draft version) is the last argument that sets &lsquo;openid.realm\".\n\n\nThe response to the /token endpoint was something like this\n\n\n```\n{\n \"access_token\" : \"...\",\n \"token_type\" : \"Bearer\",\n \"expires_in\" : 3600,\n \"id_token\" : \"eyJhbGciOiJSUzI1NiIsImtpZCI6IjVmMWY0ZTc3MWU5OTdkMjFhMDhhZGQ4M2FjYmYxOGM2Y2I0ZTU0NzMifQ.eyJ...\"\n}\n```\n\n\n&hellip; well, where is my OpenID ident?\n\n\nI went back to the [OpenID Connect page](https://developers.google.com/accounts/docs/OpenID#map-identifiers) again and found that id_token is what should bear it inside. OK. what is this id_token thing?\n\n\nId_token is encoded in the format called JSON WebToken. It\"s basically cryptographically signed JSON. I must decode this in order to get the OpenID ident. Searching metacpan.org easily found this gem, er, cpan module, [JSON::WebToken](https://metacpan.org/pod/JSON::WebToken). Oh Perl mongers, how I love thou! Now I just need to `cpanm JSON::WebToken` and call JSON::WebToken->decode( $res->{ id_token }, $key ). Wait, what should I use as $key?\n\n\nNext up: [finding the key to decode JWT](/blog/migrating-google-login-from-openid-20-to-google-3).\n\n"},{"frontmatter":{"slug":"/blog/migrating-google-login-from-openid-20-to-google-1","date":"Sun, 26 Jan 2014 16:30:00 GMT","title":"Migrating Google login from OpenID 2.0 to Google+ (Part 1 of 3)","epoch":"1390753800"},"markdownBody":"\n\nWe've been allowing users to use their Google accounts to login to Peatix. Until yesterday, however, the code was based on OpenID 2.0 protocol.\n\n```\ncommit   \nAuthor: Fumiaki Yoshimatsu  \nDate: Mon Sep 12 05:08:37 2011 +0000  \n Added Google OpenID to the list of signin-able external service.\n```\n\nWhen I thought I’d implement Google login, there have been two options (iirc); OpenID or OAuth 1.0a. IIRC, OAuth2 based login protocol was still in beta or something that it could change later. At that time around September 2011, people still believed that OAuth2 would obsolete OAuth 1.0a. Well, OAuth2 could change, OAuth1 has no future, I’d had to say OpenID was the best choice. And I implemented it.\n\nFast forward two years, It’s got obvious that\n\n*   [OAuth2 is a dead end](http://hueniverse.com/2012/07/on-leaving-oauth/). At least as an “authentication protocol”.\n*   OpenID lives and lives well, although “techy”(cough cough) people tends to see OpenID dead.\n*   OAuth 1.0a lives in a good shape - Twitter won the bet.\n\nIn other words, in a discussion of “which authentication protocol should I use in 2014?”, OpenID is still not a bad choice.\n\nWhen it comes to Google login however, there is a different story. “Google+” login that Google is encouraging to implement is based on OAuth2 protocol. And it gives you many benefits over OpenID - seamless login between Android and web for example. By 2014, it became natural for me to want to migrate our OpenID based authentication to Google+.\n\nI started research around that idea around June 2013 (that was when I filed a story in our Pivotal Tracker project). There were two simple requirements:\n\n*   Users can login via Google+ / OAuth2 protocol\n*   Existing users with Google OpenID associated to their accounts can still login to land to the same Peatix account without too much hassle\n\nThe second part was tough though, because we only keep [$verified_identity->url](https://metacpan.org/pod/Net::OpenID::VerifiedIdentity#vident-url) in our storage which looks like this\n\n[https://www.google.com/accounts/o8/id?id=AItOawkFEXN4pg8u52IM_YLN86LIolXUxqKrU3M](https://www.google.com/accounts/o8/id?id=AItOawkFEXN4pg8u52IM_YLN86LIolXUxqKrU3M)\n\nAnd it didn’t seem like Google would give us both Google+ ID and OpenID for an account so Peatix could match two to migrate the user to Google+ in a seamless manner.\n\nIt didn’t seem like so, until December 2013.\n\nTo be continued to [Part 2 of this story about migrating Google OpenID to Google+](/blog/migrating-google-login-from-openid-20-to-google-2).\n\n"},{"frontmatter":{"slug":"/blog/reloaded","date":"Sun, 26 Jan 2014 15:55:29 GMT","title":"Reloaded","epoch":"1390751729"},"markdownBody":"\n\nIt’s been a while since I posted the last entry to my blog. Two years later, I think I thought I’d restart blogging again.\n\n"},{"frontmatter":{"slug":"/blog/2013-10-26_My-first-post-on-Medium-172afc8d9d05","date":"Sat, 26 Oct 2013 04:14:47 GMT","title":"My first post on Medium","epoch":"1382760887"},"markdownBody":"\nHello? Anybody here?\n\n"},{"frontmatter":{"slug":"/blog/mt-42xapi","date":"Tue, 25 Nov 2008 06:52:00 GMT","title":"MT 4.2xではてな認証APIプラグインを使うには","epoch":"1227595920"},"markdownBody":"\nどうも御無沙汰しております。コメントでご指摘を受けて気がついたのですが、MT 4.2xだとはてな認証APIプラグインが動いていませんでした。すいません。4.2x用に修正したものを[こちら](/downloads/HatenaAuth42.zip)に置いておきましたので、差し替えてご利用ください。コメントでご指摘くださった方、ありがとうございました。"},{"frontmatter":{"slug":"/blog/mt","date":"Fri, 16 May 2008 12:09:00 GMT","title":"MTでも絵文字使おうぜプラグイン、アリマス","epoch":"1210939740"},"markdownBody":"\nシックス・アパート株式会社から昨日アナウンスのあったとおり、TypePad上のブログを携帯電話で閲覧できるようにするサーバーソフトウェア、TypeCastが、オープンソースプロジェクトとして公開されました。このソフトウェアを利用すれば、Movable Type上のブログやコメントを、TypeCast経由で携帯から閲覧できるようになるわけです。\n\n。。。携帯か。携帯でブログ。。。あんま読まないな、俺。\n\nそれよりMTユーザーに朗報なのは、TypeCastとともに、TypePadで利用している絵文字画像がGPLおよびCCというご自由にお使いくださいライセンスで公開されたことです。ですよね？MTでも絵文字使いたいですよね？\n\n![MT Emoji](/images/mt-emoji.png)\n\n皆様の熱いご要望にお応えして、Movable Type 4.1で動作する、TypeCastとともに公開された絵文字画像をWYSIWYGエディタで利用するためのプラグインを公開しました。Six Apartのsubversionリポジトリからダウンロードできます。\n\n[ZIPファイル](/downloads/EmoticonButton.zip)\n\nZIPファイルを解凍したら、EmoticonButtonフォルダの中にあるpluginsフォルダの中身を、自分のMTのpluginsフォルダに、mt-staticフォルダの中身を同じく自分のMTのmt-staticフォルダに、それぞれコピーすればOKです。ブログ記事作成画面に絵文字ボタンが追加されるでしょう。\n\nあ、TypeCastと併用すれば、このプラグインで書いた絵文字付きブログ記事をちゃんと携帯で見られるよ。あ、見られるはず。試してない。\n\nEnjoy!\n\n2008-05-20 追記\n\nこのプラグインをインストールすると、画像の挿入などの他のボタンが動かなくなるバグを修正した新しい版をアップしました。"},{"frontmatter":{"slug":"/blog/idmovable_type_4","date":"Mon, 27 Aug 2007 11:16:00 GMT","title":"はてなIDでMovable Type 4ベースのブログにコメントを","epoch":"1188213360"},"markdownBody":"\n[先日のhack-a-thon](http://web.archive.org/web/20110731171219/http://www.sixapart.jp/techtalk/2007/07/movable_type_4_hackathon.html)で[MTHatenaStar](/blog/mthatenastar)以外に実はもう1つプラグインを作っていて、でも諸般の事情で完成に至らなかったものがありました。hack-a-thonの最後の発表で、動くところまで見せたんですが、MTHatenaStarと立て続けにやったので、「どんだけはてな好きやねん」などと突っ込まれる始末。\n\n今朝から[id:naoya](https://d.hatena.ne.jp/naoya)さんのヘルプを受けて完成したので公開します。MT4のコメント投稿者認証フレームワークと[はてな認証API](https://auth.hatena.ne.jp)を使って、はてなIDでコメントできるようにするというものです。このブログでもインストールしてあるので、はてなIDでコメントしてみてください。\n\n使い方\n\n1. [はてな認証API](https://auth.hatena.ne.jp)のページで、ご自分のブログ用のAPIキーと秘密鍵を取得する。このとき、コールバックURLとして指定するURLは、コメントフォームのPOST先、つまりCGIPath＋CommentScriptの値（既定ではmt-comments.cgi）にします。ブログのURLではないのでご注意。\n1. プラグインのHatenaAuthフォルダを丸ごと、pluginsフォルダにコピーする。\n1. MT4にログインし、プラグインの設定画面で、1で取得したAPIキーと秘密鍵を設定し、保存する。\n1. ブログの設定画面から登録/認証タブに進み、Hatena IDをチェックして有効にする。\n1. ダイナミックパブリッシングをしておらず、しかもこれまで匿名でのコメントしか受け付けないようにしていた場合は、個別のブログ記事アーカイブを再構築する。\n1. はてなIDでコメントできます。\n\n[ダウンロード](/downloads/HatenaAuth.zip)"},{"frontmatter":{"slug":"/blog/mthatenastar","date":"Sat, 25 Aug 2007 05:50:00 GMT","title":"MTHatenaStar作った","epoch":"1188021000"},"markdownBody":"\n[はてなスター](https://s.hatena.ne.jp)をMovable Typeで表示する方法はいろんな人が書いてるけど、やっぱHTMLタグ書くのはMTらしくないだろーってことで、今日のhack-a-thonでMTHatenaStarを書いてみた。\n\n出力するHTMLについては、「[さりげないはてなスター](https://blog.fuktommy.com/1184165578)」に書いてあったHTMLを丸ごといただきました。ありがとうございます。\n\n使い方\n\n1. はてなスターにブログを登録し、トークンを入手する。\n1. プラグインフォルダにMTHatenaStarフォルダを丸ごと放り込む。\n1. プラグインの設定画面で、１で取得したトークンを入力し、保存する。\n1. はてなスターを表示したい場所に<$MTHatenaStar$>を書く。ただし、MTEntriesのコンテキスト内（つまり、MTEntryTitleとかを使える場所）に。\n1. 再構築（ダイナミックパブリッシングなら不要）。\n1. はて☆スタ。\n\nはてなスターを表示する場所に<script>タグも一緒に表示されちゃうのがいやなときや、はてなの指示通りに<head>の中に書きたいときは、<$MTHatenaStarScript$>タグを使えば、<script>だけを別途出力できます。ただし、処理の関係で<$MTHatenaStar$>よりは前に置く必要があります。\n\nもちろんダイナミックパブリッシング対応だよ！\n\n[ダウンロード](/downloads/MTHatenaStar.zip)"},{"frontmatter":{"slug":"/blog/movable_type_and_brads_thought","date":"Sat, 18 Aug 2007 01:46:00 GMT","title":"Movable Type and \"Brad's Thoughts on the Social Graph\"","epoch":"1187401560"},"markdownBody":"\n\n> I've been thinking a lot about the social graph for awhile now: aggregating the graph, decentralization, social network portability, etc.\n>\n> [Thoughts on the Social Graph](https://bradfitz.com/social-graph-problem/)\n\n\n昨年の今頃、MT 3.3を出してMTEにかかって、MT 3.3のバグ直して、まだMT4には専任のPMがいなくて、って時期に、MT4って何なのか、何を実装すればMTを4と呼べるようになるのか、つらつら考えていたころに社内のブログに投稿したのがこれ。\n\n> MT 4が当社のフラッグシップであり続けるためには、どこかでたしか社長が書いてましたが、トラックバックに続く「仕組み」の創造が不可欠です。また最近のブログ界隈では、プライバシーコントロールや社内SNSに代表される、オープン過ぎないつながりがキーコンセプトになっているようです。当社では当然Voxがその波を思い切り先導しようとしています。\n> \n> Voxとは別のベクトルで、しかし「つながり」を体感できる仕組みをMTに導入できないものだろうかということで、ない知恵絞って考えたのが下の図です。要するに、お互いにお互いのコネクション情報をオープンなスキーマにあわせてもっておいて、「俺MT」と「君MT」とのつながりは、ただの「他人のブログ」よりも強い結びつきを表現できるようにします。一例を挙げれば、このつながりの中でのコメントやトラックバックはモデレーションされない設定や、このつながりの中でのフィードやブクマの共有、つながりに含まれるMTブログをアグリゲートしたポータルをホストできる機能などが考えられます。Federated MT Networkというわけです。\n> \n> このつながりを表現するデータフォーマットとAPIを公開すれば、トラックバックの夢をもう一度ってことになるんではなかろうか、というのが今なんとなくつらつらと考えているMT 4のBig Pictureです。もちろん僕の個人的な絵なんで、何も決定事項ではありません。\n\n今読むと、いかにもバックエンドの実装しか考えていない人の発想というか、MT4を誰に売ろうとしているのかという部分で根本的に考え方がずれているのが見て取れて興味深いわけです。MT4には結局バックエンドに「トラックバック以来の革新」は盛り込めませんでした。\n\nでも、このアイデアをあきらめたわけではないし、このアイデアは[Rojoから来た現在のGMのChris Alden](http://web.archive.org/web/20130209044837/http://www.sixapart.com/about/press/2006/09/six_apart_acqui_1.html)も温め続けているアイデアであることからして、そのうち実装できるはずなんです。ただ、昨年から抱え続けていたのは、どんなAPIを用意すればこれが実現できるか。OpenIDをベースにするのは間違いないけど、それだけでは足りない。そこに出てきたBrad Fitzpatrickのこのマニフェスト。これはもう注目するしかありません。[Davidが戻ってきてこの手のものをSix Apartの中でやるらしい](http://web.archive.org/web/20130209044837/http://daveman692.livejournal.com/310424.html)し。\n\nMT4ではOpenIDをコメント認証の中心に置いていますが、OpenIDは認証の仕組みを提供するだけで、Trustはそれぞれケースバイケースで行います。よく誤解しているのを耳にします（とBradも来日したときに言っていた）が、OpenIDを認証したプロバイダ（LiveJournal？Vox？AOL？Livedoor？）が誰かということはTrustとは無関係（であるべき）で、だから大手がOpenIDをサポートするしないというのは、Trustネットワークの構築には結びつかない（はず）です。もちろんLiveJournalからのOpenIDは無条件に信頼するという実装をすることもできるけど、それはTrustになりえないと。\n\nそこでほしくなるのは、Voxやnowa、それにFlickrのcontactsやはてなスターのフレンドなどの、認証を前提に作られたつながりネットワークの情報です。つながりの定義に差はあれど、吉松がVoxにログインした状態で行ったVox内部での友人という設定は、その友人がVox（OpenID）で認証を受ける限りにおいては、吉松のMT上でも友人とみなしたい。もしかしてあるVoxのIDがあるはてなのIDと同じ人であることがわかったら、どっちのIDでも同じ人であることをMTでも認識したい。\n\nさらに、このつながりネットワークのプロバイダとしてMTが機能するようになったら。トラックバックを承認するとそのブログとのつながりが生まれ、そのつながりを利用して、それぞれが自分でインストールしたMTにログインしながらも、あたかもある種のSNSにログインしたかのように、つながった先にあるMTと情報を共有できたら。あるOpenIDからのコメントを承認したら、Vox上でも自動的に近所になってくれたら。1年経ってもまだこの辺をつらつらと考えているだけなのでした。\n\nそういうわけでBrad Fitzpatrickの今後に要注目だ。"},{"frontmatter":{"slug":"/blog/atom_api_photo_uploaderlivedoo","date":"Wed, 28 Dec 2005 15:00:00 GMT","title":"Atom API Photo UploaderのLivedoor Pics対応","epoch":"1135782000"},"markdownBody":"\n今日[宮川さんに聞いたFlickrの耳眼コピサイト、Livedoor Pics](https://blog.bulknews.net/mt/archives/001873.html)。でも[API](https://seesaawiki.jp/w/livedoor_pics/d/livedoor%20PICS%20WebService)はAtom PP 0.3でWSSE認証だぜってことで、宮川さんに言われるまで忘れていた拙作の[Atom API Photo Uploader](/blog/_atomapiphotouploader/)を早速テスト。\n\n![Atom API Photo UploaderのLivedoor Pics対応](/images/atomlivedoor.png)\n\nはい、何もいじらずにポストできましたとさ。LivedoorさんGJ！\n"},{"frontmatter":{"slug":"/blog/getxml","date":"Tue, 08 Nov 2005 09:47:00 GMT","title":"GetXMLプラグインの文字化け解消","epoch":"1131443220"},"markdownBody":"\nRSSをサイドに表示できるプラグインはないもんかと思ってたら、[GetXML](http://www.staggernation.com/mtplugins/GetXML/)ってのを見つけた。RSSに限らずXMLを表示できるってので早速導入。Last.fmのXMLを表示するようにテンプレートを書いてrebuildすると、どういうわけだかGetXMLプラグインが表示する部分は日本語が出てるのに、他の部分（本文など）の日本語は全部文字化けした。何じゃこりゃ？\n\nと、化けた文字を見てるうちに思い出したのが[AWSで文字化けしたこのときの記憶](http://blog.bulknews.net/mt/archives/001773.html)。これですよたぶんそうですよ。で、「Perl の Unicode フラグ」って何さってググったらずばり「[Perl 5.8.x Unicode関連](http://web.archive.org/web/20060212151726/http://www.pure.ne.jp/~learner/program/Perl_unicode.html)」ってページ発見。ふむふむ。\n\n1. 今までUTF-8の日本語が問題なく表示されていた。\n1. GetXMLで何かが悪いに違いない。\n1. GetXMLは特に文字をいじってないからXML::Simpleが悪いに違いない。\n1. UTF-8フラグってやつは悪いヤツらしい。\n1. XML::Simpleを通ると、文字列にフラグが付いてしまうのかもね。\n1. じゃあ返された文字列のフラグを外してしまえばいいのでは。\n1. 外すには『utf8::encode($alpha); # UTF8フラグを落す(Encode::encode_utf8 と同じだが、引数を変化させる)』って書いてあるぞ。\n1. GetXMLのget_valueにパッチ当てちゃえ。\n    ```\n    203a204\n    > utf8::encode($text);\n    ```\n1. Save and Rebuild。\n1. おー日本語出た。\n\n勘でも何とかなるもんですね。＜オチなしｗ"},{"frontmatter":{"slug":"/blog/standalonexml","date":"Mon, 28 Mar 2005 01:50:00 GMT","title":"standalone要素、またはXMLもいいけど仕様もね","epoch":"1111974600"},"markdownBody":"\n[前のエントリ](/blog/_atomapiphotouploader)でmiyagawaさんに指摘されてるとおり、<standalone>は[Lifeblogの仕様書](https://cognections.typepad.com/lifeblog/2004/12/lifeblog_postin.html)にちゃんと明記してあったのでした。申し訳ない。XML読む前に仕様書嫁。\n\n要するに、Atomで別々に送ったアイテムをブログのエントリとして結びつけるために使うらしい。Flickrに写真をポストしたいというだけで「Lifeblog API」を使っちまえという不純なハックでは八苦致し方なし。"},{"frontmatter":{"slug":"/blog/_atomapiphotouploader","date":"Sun, 27 Mar 2005 16:05:00 GMT","title":"続続 AtomAPIPhotoUploader","epoch":"1111939500"},"markdownBody":"\nFlickrがLifeblogすなわちAtom PPに対応したってことなんで、拙作のAtomベースのアップローダもFlickrにあっさり対応...、と思いきや苦難が。\n\nまず、Flickrの認証はX-WSSEなんですが、（たしか）昔のTypePadと同じ、[NonceをBase64エンコードしない方式](https://www.xml.com/pub/a/2003/12/17/dive.html)なんで、それに対応しなければならなかった。\n\nまあそれはすでに通った道なんですぐに対応できたものの、いくらやっても「Not a valid mime-type for relation.」という非常に親切なエラーメッセージ。仕方なくNokiaでLifeblogのPC版を落としてインストールし、ポストして、XML on the wireの解析へ。\n\nで、いろいろ変わった実装をしているもんで、どれがキーポイントなのかまた試行錯誤。Lifeblogが生成するAtom entryを見ると、なんと<content>の中身をいわゆるRFC 2045ばりに、72文字という中途半端な数で改行してやがんの。まさか？と思ってそれに対応してみたけど、やっぱりエラーは変わらず。\n\nんで、さらにXMLを眺めてみると、<standalone xmlns=\"http://sixapart.com/atom/typepad#\">1</standalone>という謎の要素を発見。それを挿入してみたらなんと動いちまった。なんてこった。\n\nそういうことで最新バイナリはこちらで最新ソースはこちら。あー、あとFlickrのPOST URLは http://www.flickr.com/services/atom/post/ なんで念のため。リソースには入れてません。\n\n一応書いておきますが、書くまでもなく、こんなのよりFlickrのアップローダのほうが優れています。僕自身Flickrのアップローダつかってます（笑）。まあAtomなんざ「まだ実装すんな」っていう仕様なんで。でも実装がないまま進む仕様化ってのも危険なんで、実装してみるってのに意味はあるかな、と。なんだそりゃ。\n\nこれまでの経緯。\n- [続 AtomAPIPhotoUploader](/blog/-atomapiphotouploader-1/)\n- [改め、AtomAPIPhotoUploader。](/blog/atomapiphotouploader/)\n- [はてなフォトライフアップローダー](/blog/post-43/)\n\nあとついでに発見したのは、NokiaのLifeblog PC版にはCOMAtom.dllというCOMベースのAtom PP実装が含まれてる（と思われる）ってことですね。CreateObject(\"COMAtom.AtomClient\")ってな感じでいけるっぽい。やっぱCOMがキテるらしいってことでオチがつきました。\n\n## Comments:\n\nあーすいません（あやまることでもないんですがｗ\n\n[http://cognections.typepad.com/lifeblog/2004/12/lifeblog_postin.html](http://cognections.typepad.com/lifeblog/2004/12/lifeblog_postin.html)\n\natom:standalone エレメントは Lifeblog 用の atom 拡張で、仕様書の 2.4.1 Posting Lifeblog Items に載っています。\n\n- posted by [miyagawa](https://www.blogger.com/profile/3736463) : 2:02 午前\n\nちょっと前にこういうのつくってました。\n[http://blog.bulknews.net/flickr2typepad.cgi](http://blog.bulknews.net/flickr2typepad.cgi)\n\nこれは flickr から AtomPP (LifeBlog API) をつかって TypePad にポストです。\n\n- posted by [miyagawa](https://www.blogger.com/profile/3736463) : 2:08 午前"},{"frontmatter":{"slug":"/blog/-atomapiphotouploader-1","date":"Sat, 26 Feb 2005 08:36:00 GMT","title":"続 AtomAPIPhotoUploader","epoch":"1109406960"},"markdownBody":"\n[Update](/blog/_atomapiphotouploader/) こちらに新しい版があります。\n\nご要望があったので英語対応してみました。バイナリはこちらでソースはこちら。.NET Frameworkの国際化関係の機能を使ってます。ので、英語圏の人はen-USフォルダとその中のDLLが必須です。en-USフォルダは、EXEと同じ場所になければなりません。日本の人はEXEだけでも動作します（en-USフォルダが存在しても別に問題ありません）。とはいえ英語OS上でまともにテストしてないので、\n\nあとソースは.NET Framework SDK 1.1だけでビルドできるようにしました。VS.NET 2003は必要ありません。っていうかVS.NET 2003を使うと国際化関係のところが壊れますのでご注意を。いえ、これはVS.NET 2003が悪いわけではなく、最初にそう作っちゃったからというだけです。\n\nサテライトアセンブリを作るだけで他の言語にも対応できるはずです。"},{"frontmatter":{"slug":"/blog/atomapiphotouploader","date":"Sun, 20 Feb 2005 09:52:00 GMT","title":"改め、AtomAPIPhotoUploader。","epoch":"1108893120"},"markdownBody":"\n[Update](/blog/-atomapiphotouploader-1/) こちらに新しい版があります。\n\n[Update](/blog/_atomapiphotouploader/) こちらにもっと新しい版があります。\n\n[なおやさんによると、はてな謹製のアップローダが近々出る](http://web.archive.org/web/20131106052629/http://naoya.dyndns.org/~naoya/mt/archives/001597.html)そう。でさらにツッコミも受けたのでTypePad.comにも対応してみました。とはいっても、エンドポイントURLを変えられるようにした（あといくつかエラー対処コード追加した）だけで、Atomに関するコードはほとんどいじってません。標準APIばんざい。新バイナリはこちら。新ソースはこちら。\n\n毎回ユーザー名、パスワード、それにURLまで入れるのウザイって声が聞こえてきそうですが、そういうのを覚えさせておくのもいろいろ気を使うので、あえて外してます。すみません。"},{"frontmatter":{"slug":"/blog/post-43","date":"Sat, 19 Feb 2005 15:29:00 GMT","title":"はてなフォトライフアップローダー","epoch":"1108826940"},"markdownBody":"\n[Update](/blog/atomapiphotouploader/) こちらに新しい版があります。\n\n[Update](/blog/-atomapiphotouploader-1/) こちらにもっと新しい版があります。\n\n[Update](/blog/_atomapiphotouploader/) こちらにもっともっと新しい版があります。\n\nFlickrのをまねてちょっと作ってみた。実行には.NET Framework 1.1が必要です。\n\n![はてなフォトライフアップローダー](/images/20050220001804.jpg)\n\nGUIがダサいのは勘弁して、っていうかソースあるんで誰か直してください。VS.NET 2003 + C# ですけど。"}]},"__N_SSG":true}