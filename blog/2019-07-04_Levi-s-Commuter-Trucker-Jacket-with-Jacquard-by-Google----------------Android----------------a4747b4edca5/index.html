<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-50700-3"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {dataLayer.push(arguments); }
    gtag("js", new Date());

    gtag("config", "UA-50700-3");
 </script><link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;700&amp;display=swap" rel="stylesheet"/><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る</title><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="@fumiakiy"/><meta name="og:title" content="Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る"/><meta name="twitter:title" content="Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る"/><meta name="description" content="Tensorflowで作ったモデルをTensorflow Liteのモデルに変換するまでの苦労話"/><meta name="og:description" content="Tensorflowで作ったモデルをTensorflow Liteのモデルに変換するまでの苦労話"/><meta name="twitter:description" content="Tensorflowで作ったモデルをTensorflow Liteのモデルに変換するまでの苦労話"/><meta name="next-head-count" content="10"/><link rel="preload" href="/_next/static/css/2fb47489fbb39fde9ffa.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2fb47489fbb39fde9ffa.css"/><link rel="preload" href="/_next/static/css/d86083d51fef5516b5a7.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d86083d51fef5516b5a7.css"/><link rel="preload" href="/_next/static/chunks/main-fd8a0b253a51098ec3a6.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-488dc228921f1fdbc0e7.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.1b5fae6dde8a26be77ac.js" as="script"/><link rel="preload" href="/_next/static/chunks/e82d01500e11e0131e78851aa17fd9f5e63d6c88.616ccd5db4903f86882e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-012ce3de4391df2d9dee.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bpostname%5D-b07b452b8f3bf1c1e2fd.js" as="script"/></head><body><div id="__next"><div class="blog"><article class="BlogPost_article__3W1r-"><h1>Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る</h1><div class="BlogPost_body__1oPZH"><h3>Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る</h3><h3>TensorflowのモデルをTensorflow Liteのモデルに変換したい</h3><div><a href="前回なんとなく雰囲気で作ったTensorflowのモデル">/blog/2019-06-15_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------1ae6347c67fc</a>をAndroidアプリで利用するには、まずTensorflowで訓練したモデルをTensorflow Liteで読めるものに変換しなければならない。<a href="ドキュメント">https://www.tensorflow.org/lite/convert</a>にはなんだかサラッと<a href="コマンドで変換できそうなことが書いてある">https://www.tensorflow.org/lite/convert/cmdline_examples</a>のでやってみる。</div><div>入力としてモデルを渡すのでまずモデルをシリアライズしてファイルにする。<a href="DNNClassifierにはexport_saved_modelっていうメソッドがある">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#export_saved_model</a>のでこれを呼べばいいんだろうが、こいつに渡す引数がよくわからない。DNNClassifier savedmodelあたりでググり倒してようやく<a href="それっぽいコードを見つけた">https://stackoverflow.com/a/55737532</a>のだが、これは直接Tensorflow Liteのクラスとメソッドを使ってインメモリのオブジェクトを変換する方法で、いやまあ別に動けばいいんだけども、渡しているものも何が何だかわからん。加えて、モデルを作る過程では一切出てこないセッションだのグラフだのというオブジェクトを「そこにある」前提で扱うコードになっている。<code>dnn/input_from_feature_columns/input_layer/concat:0</code>とか<code>dnn/logits/BiasAdd:0</code>とか一体どこから出てきたのか。</div><h3>Tensorflow Liteで予測してみる</h3><div>とりあえず上記のコードを<a href="前回のスクリプト">https://gist.github.com/fumiakiy/c1f8fe23b36b0a8984a12cd2bb54cd0e</a>へコピペして、classifierのtrainをした後で呼び出してみると、確かにconverted_model.tfliteファイルができあがるので、ひとまずこれをTensorflow Liteで推測に使ってみることにした。Tensorflow LiteのドキュメントをみるとPythonでも使えるぽいので、ひとまずスクリプトを書いてみる。</div><div>Tensorflow LiteのドキュメントではInterpreterオブジェクトを作ってrunメソッドを呼び出せば結果がoutput引数に返されるぽいことが書いてあるのだが、PythonのInterpreterオブジェクトにはrunメソッドがない。<a href="ググって見つけたこのコード">https://stackoverflow.com/a/51093144</a>にしたがって、<a href="input_dataだけ自前の配列に変えたスクリプト">https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751</a>を書いて実行してみる。</div><pre><code>$ python tflite1.py

ValueError: Cannot set tensor: Dimension mismatch</code></pre><div>はて。次元が合わないというので、配列の配列にしてみる。</div><pre><code># input_data = e
input_data = [e]
…
$ python tflite1.py
…
ValueError: Cannot set tensor: Dimension mismatch</code></pre><div>同じエラー。配列の要素を一つだけ渡してみる。</div><pre><code># input_data = [e]
input_data = e[0]</code></pre><pre><code>…
$ python tflite1.py
…
ValueError: Cannot set tensor: Dimension mismatch</code></pre><div>またまた同じエラー。何を渡せばいいんだかわからないので、とりあえず第一引数で使っている<code>input_details</code>の中身をダンプしてみることに。</div><pre><code>[
{&#x27;index&#x27;: 0, &#x27;shape&#x27;: array([1], dtype=int32), &#x27;quantization&#x27;: (0.0, 0L), ‘name’: ‘Const’, &#x27;dtype&#x27;: &amp;lt;type &#x27;numpy.int64&#x27;&amp;gt;},
{&#x27;index&#x27;: 1, &#x27;shape&#x27;: array([1], dtype=int32), &#x27;quantization&#x27;: (0.0, 0L), ‘name’: ‘Const_1’, &#x27;dtype&#x27;: &amp;lt;type &#x27;numpy.int64&#x27;&amp;gt;}, …</code></pre><div>つまりinput_details<a href="">0</a>は“0”であると。このtensorはint64型のデータで、shapeは要素数1の配列であるということ…なのかな? ということは、これなら通るのか?</div><pre><code># input_data = e[0]
input_data = [e[0]]</code></pre><pre><code>…
$ python tflite1.py
[[1. 0. 0. 0. 0. 0. 0. 0.]]</code></pre><div>なんか出てきた。要素数8の配列なので、おそらくそれぞれの数値がLABELつまりモデルを作った文字データ(<strong>a, b, c, d, e, h, o, y</strong>)に対応していて、それを示す値が0/1でかえってきた? つまりこのデータから予想される文字は「a」ってこと?</div><div>しかし<strong>e<a href="">0</a></strong>の値しか渡していないのだからこれが正しいわけがないので、input_dataを正しい形にすべく、こんなコードにして、51件の数値を全部渡してみることにした。</div><pre><code># input_data = [e[0]]
for i in (range(len(e) — 1)):
    interpreter.set_tensor(input_details[i][&#x27;index&#x27;], [e[i]])</code></pre><pre><code>…
$ python tflite1.py
[[0.1134394 0.09876031 0.1299585 0.1381347 0.07306363 0.17048864
 0.09710578 0.17904899]]</code></pre><div>なんかそれっぽい値がかえってきた。これがそれぞれの文字かもしれない可能性を表す数値なんだろうか。eではなくyのデータを与えてみると、</div><pre><code># for i in (range(len(e) — 1)):
#     interpreter.set_tensor(input_details[i][&#x27;index&#x27;], [e[i]])
for i in (range(len(y) — 1)):
interpreter.set_tensor(input_details[i][&#x27;index&#x27;], [y[i]])</code></pre><pre><code>…
$ python tflite1.py
[[0. 0. 1. 0. 0. 0. 0. 0.]]</code></pre><div>つまり100%「c」って予想ってこと? ふーむ。output_detailsの方をダンプしてみると、こうなっていて、そういう解釈で良さそうな気がする。</div><pre><code>[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: &amp;lt;type ‘numpy.float32’&amp;gt;}]</code></pre><div>ここまで試行錯誤を重ねて、あとはtrainingのstep数やhidden_unitsの中身やらをあれこれいじってモデルを作り直して、また変換して<a href="tflite1.py">https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751</a>を実行してみて、というのを繰り返してみたが、なんとも今一つの結果しか得られない。らちが開かないので、Tensorflow Liteへの変換過程を変えて、Saved Modelとやらにエクスポートすればもう少しそのファイルに何か書いてあるんじゃなかろうかと、DNNClassifierのexport_saved_modelを呼ぶ方法を探すことにした。</div><h3>TensorflowのDNNClassifierをSavedModelとして出力する</h3><div>もう一度「DNNClassifier “saved model”」あたりでググっていくつかそれっぽいサンプルを見ていてようやく<a href="この記事">http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/</a>の中にコピペできそうなコードを見つけた。早速ちょいちょい書き換えて実行してみる。</div><pre><code>def export_tflite2(classifier, data):
   feature_columns = []
   for i in range(len(data)):
        feature_columns.append(
          tf.feature_column.numeric_column(key=str(i))
        )</code></pre><pre><code>    feature_spec = tf.feature_column.make_parse_example_spec(
                     feature_columns
                   )</code></pre><pre><code>    # Build receiver function, and export.
    serving_input_receiver_fn = tf.estimator.export.
        build_parsing_serving_input_receiver_fn(feature_spec)
    export_dir = classifier.export_savedmodel(
                   &#x27;export&#x27;, serving_input_receiver_fn
                 )
    print(export_dir)</code></pre><div>なんと「export/1562177753」にそれらしきファイルができた。ファイルはバイナリーでそのままでは読めなかったので、これの中身を調べる方法を探すと、<strong>saved_model_cli</strong>というコマンドがある。実行すると、中に見慣れた文字列が。</div><pre><code>$ saved_model_cli show --dir export/1562177753 --all
…
 outputs[&#x27;logits&#x27;] tensor_info:
 dtype: DT_FLOAT
 shape: (-1, 8)
 name: dnn/logits/BiasAdd:0
…</code></pre><div>これは良いものなのでは? 早速<a href="tflite_convertコマンド">https://www.tensorflow.org/lite/convert/cmdline_examples</a>にかけて、Tensorflow Liteのモデルに変換してみる。</div><pre><code>$ tflite_convert --output_file=./model1.tflite --saved_model_dir=export/1562177753
…
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, EXPAND_DIMS, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, TILE. Here is a list of operators for which you will need custom implementations: AsString, ParseExample.</code></pre><div>Tensorflow Liteのランタイムには存在しないオペレーター(ここではAsStringとParseExample)を使っているので、変換できませんと。使っているのは誰なのかもよくわからんので、とにかくググる。<a href="ParseExampleに関しては、このSOの答え">https://stackoverflow.com/a/55693825</a>が見つかった。export_saved_modelするときのやり方を少し変えればいいっぽい。やってみる。</div><pre><code># feature_columns = []
# for i in range(len(data)):
#     feature_columns.append(tf.feature_column.numeric_column(key=str(i)))</code></pre><pre><code># feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)</code></pre><pre><code># Build receiver function, and export.
 # serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)</code></pre><pre><code>features = {}
for i in range(len(data)):
    key = str(i)
    features[key] = tf.convert_to_tensor(np.array(data[i]))</code></pre><pre><code>serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)
 export_dir = classifier.export_savedmodel(&#x27;export&#x27;, serving_input_receiver_fn)</code></pre><div>無事export/1562179024にファイルができたので、先ほどのsaved_model_cliコマンドで中身をみてみるとだいぶ内容が変わっていた。まあ気にせずtflite_convertを再度実行してみる。</div><pre><code>$ tflite_convert — output_file=./model1.tflite — saved_model_dir=export/1562179024
…
ValueError: No ‘serving_default’ in the SavedModel’s SignatureDefs. Possible values are ‘predict’.</code></pre><div>さっきのSOの答えに書いてあったのはこれか、ということでオプションを足して再度実行。SOに書いてあるのとはオプションの名前が違った(signature_def_keyではなくsaved_model_signature_key)。</div><pre><code>$ tflite_convert --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024
…
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, CAST, CONCATENATION, FULLY_CONNECTED, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AsString.</code></pre><div>ParseExampleは使わなくなったけど、AsStringはまだエラーのまま。 — allow_custom_opsをつけて実行すればmodel1.tfliteはできるけども、これを先ほどのTensorflow Liteのコードに読み込むと以下のエラーで結局使えない。</div><pre><code>ValueError: Didn’t find custom op for name ‘AsString’ with version 1
Registration failed.</code></pre><div>AsStringなんて簡単に実装できるんじゃないのかと思って<a href="">https://www.tensorflow.org/lite/guide/ops_custom</a>(custom operatorを自分で実装する方法のドキュメント)を読んでみたけども、C++で書いてTensorflow全体をビルドしなおすってこと? よくわからんが手に負えなさそうなのでやめて、AsStringを使わないような変換を行う方法を探してみる。</div><div>もう一度saved_model_cliを実行してみると、中にDT_STRING型の値を出力するtensorがあるのを見つけた。</div><pre><code>outputs[‘classes’] tensor_info:
 dtype: DT_STRING
 shape: (-1, 1)
 name: dnn/head/predictions/str_classes:0</code></pre><div>カテゴリー分けしたときのカテゴリー名(ラベル)を持っているのだろうか、よくわからないが使わないのでこれをモデルに含まないようにすればいいんじゃないかと。</div><div>tflite_convertコマンドにはoutput_arrayというオプションがあって、これにtensorの名前を指定できるらしい。とりあえず今出力して欲しいのは予測結果だけなので、同じsaved_model_cliの出力にあった「dnn/head/predictions/probabilities:0」だけが出てくればいいやということで、output_arrayに指定してみる。</div><pre><code>$ tflite_convert — output_file=./model1.tflite — saved_model_signature_key=”predict” — saved_model_dir=export/1562179024 — output_array=dnn/head/predictions/probabilities:0
…
ValueError: Invalid tensors ‘dnn/head/predictions/probabilities:0’ were found.</code></pre><div>エラー。いや待てよ、さっきダンプしたoutput_detailsによると、このtensorの名前は「dnn/head/predictions/probabilities」みたい。</div><pre><code>[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: &amp;lt;type ‘numpy.float32’&amp;gt;}]</code></pre><div>というわけで「:0」を除いてみる。</div><pre><code>$ tflite_conver --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024 --output_array=dnn/head/predictions/probabilities</code></pre><div>できた。おお、できたよ。再度<a href="">https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751</a>(Tensorflow Liteで予測するスクリプト)を実行してみると実行自体はできた。出力は相変わらず意味がよくわからないけど。</div><pre><code>$ python tflite1.py</code></pre><pre><code>[[0.11178369 0.10516622 0.11024905 0.13274346 0.10095505 0.15692514
 0.09929805 0.18287939]]</code></pre><h3>Tensorflow Liteのモデルに入力する値</h3><div>同じモデルに色々な文字のデータを入れて試してみても、何か腹落ちするデータが得られないことがしばらく続いて、そろそろやる気もなくなって来たころ。</div><div>何か変だなと思っていたのは、LiteではないTensorflowのclassifierを使ったテストではそれなりに当たりの予測を出すことが多いのに、なぜかLiteになると全然当たらないということ。渡している入力値がおかしいんだろうか。</div><div>もう一度saved_model_cliの出力を見直すと、saved modelを使っていなかったときのコード(Tensorflow Liteのクラスとメソッドを使ってclassifierオブジェクトを直接変換したコード)では入力tensorとして「dnn/input_from_feature_columns/input_layer/concat」を使っていて、それに対して現状のsaved_model_cliの出力の中にはそういう入力tensorは存在せず、代わりに「Const_1」「Const_2」というtensorが全部で51個あるのがわかった。<strong>51個</strong>。これはこっちが渡そうとしている1つ1つのデータを入れる場所に違いないので、ループでset_tensorしている今のコードで大丈夫のはず…なんだけど、<a href="input_details[i][‘index’]と、配列の添字ではなくあえてオブジェクトのindexキーを使ってデータをセットしている">https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751</a>のはなんでなんだろう、と思いついて、input_detailsの中身をダンプしてみると、</div><pre><code>[
{‘index’: 0, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const’, ‘dtype’: &amp;lt;type ‘numpy.int64’&amp;gt;},
{‘index’: 2, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_10’, ‘dtype’: &amp;lt;type ‘numpy.int64’&amp;gt;},
{‘index’: 3, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_11’, ‘dtype’: &amp;lt;type ‘numpy.int64’&amp;gt;}, …</code></pre><div>0番の要素のindexは0だけど、1番の要素のindexは2になっているし、名前もConst_10でConst_1ではない! ということは、テストデータの1番要素の値をinput_details<a href="">1</a>に入れてしまうと、1番のtensorにセットすべき値を2番のtensor(本来10番要素の値を入れる場所)にセットしていることになってしまうのでは?</div><div>配列の要素とtensorの並びを正規化して、それから正しいtensorに値をset_tensorするようにコードを書き直してみた。</div><pre><code># for i in (range(len(y) — 1)):
#     interpreter.set_tensor(input_details[i][&#x27;index&#x27;], [y[i]])</code></pre><pre><code>indices = [0] * 51
for detail in input_details:
    vindex = 0
    m = re.match(r&#x27;Const_(\d+)&#x27;, detail[&#x27;name&#x27;])
    if (m is None):
        vindex = 0
    else:
        vindex = int(m.group(1))
    indices[vindex] = detail[&#x27;index&#x27;]</code></pre><pre><code>for i in range(len(y)-1):
    interpreter.set_tensor(indices[i], [y[i]])</code></pre><div>実行してみる。</div><pre><code>$ python tflite1.py</code></pre><pre><code>[[0. 0. 0. 0. 0. 0. 0. 1.]]</code></pre><div>おお、yをyと予想したっぽい! yをeにして再度実行してみる。</div><pre><code># for i in range(len(y)-1):
#     interpreter.set_tensor(indices[i], [y[i]])
for i in range(len(e)-1):
    interpreter.set_tensor(indices[i], [e[i]])</code></pre><pre><code>$ python tflite1.py</code></pre><pre><code>[[0. 0. 0. 0. 1. 0. 0. 0.]]</code></pre><div>eも正しく予測している! このTensorflow Liteのモデルは良いものなのでは? Androidアプリで実行してみよう!</div><div>というわけで次回へ続く。</div></div></article></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"slug":"/blog/2019-07-04_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------Android----------------a4747b4edca5","date":"Thu, 04 Jul 2019 20:45:26 GMT","title":"Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る","epoch":"1562273126","excerpt":"Tensorflowで作ったモデルをTensorflow Liteのモデルに変換するまでの苦労話"},"markdownBody":"\n### Levi’s Commuter Trucker Jacket with Jacquard by Googleの袖から来たデータを機械学習してAndroidアプリで利用できるモデルを作る\n\n### TensorflowのモデルをTensorflow Liteのモデルに変換したい\n\n[/blog/2019-06-15_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------1ae6347c67fc](前回なんとなく雰囲気で作ったTensorflowのモデル)をAndroidアプリで利用するには、まずTensorflowで訓練したモデルをTensorflow Liteで読めるものに変換しなければならない。[https://www.tensorflow.org/lite/convert](ドキュメント)にはなんだかサラッと[https://www.tensorflow.org/lite/convert/cmdline_examples](コマンドで変換できそうなことが書いてある)のでやってみる。\n\n入力としてモデルを渡すのでまずモデルをシリアライズしてファイルにする。[https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#export_saved_model](DNNClassifierにはexport_saved_modelっていうメソッドがある)のでこれを呼べばいいんだろうが、こいつに渡す引数がよくわからない。DNNClassifier savedmodelあたりでググり倒してようやく[https://stackoverflow.com/a/55737532](それっぽいコードを見つけた)のだが、これは直接Tensorflow Liteのクラスとメソッドを使ってインメモリのオブジェクトを変換する方法で、いやまあ別に動けばいいんだけども、渡しているものも何が何だかわからん。加えて、モデルを作る過程では一切出てこないセッションだのグラフだのというオブジェクトを「そこにある」前提で扱うコードになっている。`dnn/input_from_feature_columns/input_layer/concat:0`とか`dnn/logits/BiasAdd:0`とか一体どこから出てきたのか。\n\n### Tensorflow Liteで予測してみる\n\nとりあえず上記のコードを[https://gist.github.com/fumiakiy/c1f8fe23b36b0a8984a12cd2bb54cd0e](前回のスクリプト)へコピペして、classifierのtrainをした後で呼び出してみると、確かにconverted_model.tfliteファイルができあがるので、ひとまずこれをTensorflow Liteで推測に使ってみることにした。Tensorflow LiteのドキュメントをみるとPythonでも使えるぽいので、ひとまずスクリプトを書いてみる。\n\nTensorflow LiteのドキュメントではInterpreterオブジェクトを作ってrunメソッドを呼び出せば結果がoutput引数に返されるぽいことが書いてあるのだが、PythonのInterpreterオブジェクトにはrunメソッドがない。[https://stackoverflow.com/a/51093144](ググって見つけたこのコード)にしたがって、[https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751](input_dataだけ自前の配列に変えたスクリプト)を書いて実行してみる。\n\n```\n$ python tflite1.py\n\nValueError: Cannot set tensor: Dimension mismatch\n```\n\nはて。次元が合わないというので、配列の配列にしてみる。\n\n```\n# input_data = e\ninput_data = [e]\n…\n$ python tflite1.py\n…\nValueError: Cannot set tensor: Dimension mismatch\n```\n\n同じエラー。配列の要素を一つだけ渡してみる。\n\n```\n# input_data = [e]\ninput_data = e[0]\n```\n\n```\n…\n$ python tflite1.py\n…\nValueError: Cannot set tensor: Dimension mismatch\n```\n\nまたまた同じエラー。何を渡せばいいんだかわからないので、とりあえず第一引数で使っている`input_details`の中身をダンプしてみることに。\n\n```\n[\n{'index': 0, 'shape': array([1], dtype=int32), 'quantization': (0.0, 0L), ‘name’: ‘Const’, 'dtype': \u0026lt;type 'numpy.int64'\u0026gt;},\n{'index': 1, 'shape': array([1], dtype=int32), 'quantization': (0.0, 0L), ‘name’: ‘Const_1’, 'dtype': \u0026lt;type 'numpy.int64'\u0026gt;}, …\n```\n\nつまりinput_details[0][‘index’]は“0”であると。このtensorはint64型のデータで、shapeは要素数1の配列であるということ…なのかな? ということは、これなら通るのか?\n\n```\n# input_data = e[0]\ninput_data = [e[0]]\n```\n\n```\n…\n$ python tflite1.py\n[[1. 0. 0. 0. 0. 0. 0. 0.]]\n```\n\nなんか出てきた。要素数8の配列なので、おそらくそれぞれの数値がLABELつまりモデルを作った文字データ(**a, b, c, d, e, h, o, y**)に対応していて、それを示す値が0/1でかえってきた? つまりこのデータから予想される文字は「a」ってこと?\n\nしかし**e[0]**の値しか渡していないのだからこれが正しいわけがないので、input_dataを正しい形にすべく、こんなコードにして、51件の数値を全部渡してみることにした。\n\n```\n# input_data = [e[0]]\nfor i in (range(len(e) — 1)):\n    interpreter.set_tensor(input_details[i]['index'], [e[i]])\n```\n\n```\n…\n$ python tflite1.py\n[[0.1134394 0.09876031 0.1299585 0.1381347 0.07306363 0.17048864\n 0.09710578 0.17904899]]\n```\n\nなんかそれっぽい値がかえってきた。これがそれぞれの文字かもしれない可能性を表す数値なんだろうか。eではなくyのデータを与えてみると、\n\n```\n# for i in (range(len(e) — 1)):\n#     interpreter.set_tensor(input_details[i]['index'], [e[i]])\nfor i in (range(len(y) — 1)):\ninterpreter.set_tensor(input_details[i]['index'], [y[i]])\n```\n\n```\n…\n$ python tflite1.py\n[[0. 0. 1. 0. 0. 0. 0. 0.]]\n```\n\nつまり100%「c」って予想ってこと? ふーむ。output_detailsの方をダンプしてみると、こうなっていて、そういう解釈で良さそうな気がする。\n\n```\n[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: \u0026lt;type ‘numpy.float32’\u0026gt;}]\n```\n\nここまで試行錯誤を重ねて、あとはtrainingのstep数やhidden_unitsの中身やらをあれこれいじってモデルを作り直して、また変換して[https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751](tflite1.py)を実行してみて、というのを繰り返してみたが、なんとも今一つの結果しか得られない。らちが開かないので、Tensorflow Liteへの変換過程を変えて、Saved Modelとやらにエクスポートすればもう少しそのファイルに何か書いてあるんじゃなかろうかと、DNNClassifierのexport_saved_modelを呼ぶ方法を探すことにした。\n\n### TensorflowのDNNClassifierをSavedModelとして出力する\n\nもう一度「DNNClassifier “saved model”」あたりでググっていくつかそれっぽいサンプルを見ていてようやく[http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/](この記事)の中にコピペできそうなコードを見つけた。早速ちょいちょい書き換えて実行してみる。\n\n```\ndef export_tflite2(classifier, data):\n   feature_columns = []\n   for i in range(len(data)):\n        feature_columns.append(\n          tf.feature_column.numeric_column(key=str(i))\n        )\n```\n\n```\n    feature_spec = tf.feature_column.make_parse_example_spec(\n                     feature_columns\n                   )\n```\n\n```\n    # Build receiver function, and export.\n    serving_input_receiver_fn = tf.estimator.export.\n        build_parsing_serving_input_receiver_fn(feature_spec)\n    export_dir = classifier.export_savedmodel(\n                   'export', serving_input_receiver_fn\n                 )\n    print(export_dir)\n```\n\nなんと「export/1562177753」にそれらしきファイルができた。ファイルはバイナリーでそのままでは読めなかったので、これの中身を調べる方法を探すと、**saved_model_cli**というコマンドがある。実行すると、中に見慣れた文字列が。\n\n```\n$ saved_model_cli show --dir export/1562177753 --all\n…\n outputs['logits'] tensor_info:\n dtype: DT_FLOAT\n shape: (-1, 8)\n name: dnn/logits/BiasAdd:0\n…\n```\n\nこれは良いものなのでは? 早速[https://www.tensorflow.org/lite/convert/cmdline_examples](tflite_convertコマンド)にかけて、Tensorflow Liteのモデルに変換してみる。\n\n```\n$ tflite_convert --output_file=./model1.tflite --saved_model_dir=export/1562177753\n…\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, EXPAND_DIMS, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, TILE. Here is a list of operators for which you will need custom implementations: AsString, ParseExample.\n```\n\nTensorflow Liteのランタイムには存在しないオペレーター(ここではAsStringとParseExample)を使っているので、変換できませんと。使っているのは誰なのかもよくわからんので、とにかくググる。[https://stackoverflow.com/a/55693825](ParseExampleに関しては、このSOの答え)が見つかった。export_saved_modelするときのやり方を少し変えればいいっぽい。やってみる。\n\n```\n# feature_columns = []\n# for i in range(len(data)):\n#     feature_columns.append(tf.feature_column.numeric_column(key=str(i)))\n```\n\n```\n# feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n```\n\n```\n# Build receiver function, and export.\n # serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n```\n\n```\nfeatures = {}\nfor i in range(len(data)):\n    key = str(i)\n    features[key] = tf.convert_to_tensor(np.array(data[i]))\n```\n\n```\nserving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)\n export_dir = classifier.export_savedmodel('export', serving_input_receiver_fn)\n```\n\n無事export/1562179024にファイルができたので、先ほどのsaved_model_cliコマンドで中身をみてみるとだいぶ内容が変わっていた。まあ気にせずtflite_convertを再度実行してみる。\n\n```\n$ tflite_convert — output_file=./model1.tflite — saved_model_dir=export/1562179024\n…\nValueError: No ‘serving_default’ in the SavedModel’s SignatureDefs. Possible values are ‘predict’.\n```\n\nさっきのSOの答えに書いてあったのはこれか、ということでオプションを足して再度実行。SOに書いてあるのとはオプションの名前が違った(signature_def_keyではなくsaved_model_signature_key)。\n\n```\n$ tflite_convert --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024\n…\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing — enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with — allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, CAST, CONCATENATION, FULLY_CONNECTED, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AsString.\n```\n\nParseExampleは使わなくなったけど、AsStringはまだエラーのまま。 — allow_custom_opsをつけて実行すればmodel1.tfliteはできるけども、これを先ほどのTensorflow Liteのコードに読み込むと以下のエラーで結局使えない。\n\n```\nValueError: Didn’t find custom op for name ‘AsString’ with version 1\nRegistration failed.\n```\n\nAsStringなんて簡単に実装できるんじゃないのかと思って[https://www.tensorflow.org/lite/guide/ops_custom](custom operatorを自分で実装する方法のドキュメント)を読んでみたけども、C++で書いてTensorflow全体をビルドしなおすってこと? よくわからんが手に負えなさそうなのでやめて、AsStringを使わないような変換を行う方法を探してみる。\n\nもう一度saved_model_cliを実行してみると、中にDT_STRING型の値を出力するtensorがあるのを見つけた。\n\n```\noutputs[‘classes’] tensor_info:\n dtype: DT_STRING\n shape: (-1, 1)\n name: dnn/head/predictions/str_classes:0\n```\n\nカテゴリー分けしたときのカテゴリー名(ラベル)を持っているのだろうか、よくわからないが使わないのでこれをモデルに含まないようにすればいいんじゃないかと。\n\ntflite_convertコマンドにはoutput_arrayというオプションがあって、これにtensorの名前を指定できるらしい。とりあえず今出力して欲しいのは予測結果だけなので、同じsaved_model_cliの出力にあった「dnn/head/predictions/probabilities:0」だけが出てくればいいやということで、output_arrayに指定してみる。\n\n```\n$ tflite_convert — output_file=./model1.tflite — saved_model_signature_key=”predict” — saved_model_dir=export/1562179024 — output_array=dnn/head/predictions/probabilities:0\n…\nValueError: Invalid tensors ‘dnn/head/predictions/probabilities:0’ were found.\n```\n\nエラー。いや待てよ、さっきダンプしたoutput_detailsによると、このtensorの名前は「dnn/head/predictions/probabilities」みたい。\n\n```\n[{‘index’: 51, ‘shape’: array([1, 8], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘dnn/head/predictions/probabilities’, ‘dtype’: \u0026lt;type ‘numpy.float32’\u0026gt;}]\n```\n\nというわけで「:0」を除いてみる。\n\n```\n$ tflite_conver --output_file=./model1.tflite --saved_model_signature_key=”predict” --saved_model_dir=export/1562179024 --output_array=dnn/head/predictions/probabilities\n```\n\nできた。おお、できたよ。再度[https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751](Tensorflow Liteで予測するスクリプト)を実行してみると実行自体はできた。出力は相変わらず意味がよくわからないけど。\n\n```\n$ python tflite1.py\n```\n\n```\n[[0.11178369 0.10516622 0.11024905 0.13274346 0.10095505 0.15692514\n 0.09929805 0.18287939]]\n```\n\n### Tensorflow Liteのモデルに入力する値\n\n同じモデルに色々な文字のデータを入れて試してみても、何か腹落ちするデータが得られないことがしばらく続いて、そろそろやる気もなくなって来たころ。\n\n何か変だなと思っていたのは、LiteではないTensorflowのclassifierを使ったテストではそれなりに当たりの予測を出すことが多いのに、なぜかLiteになると全然当たらないということ。渡している入力値がおかしいんだろうか。\n\nもう一度saved_model_cliの出力を見直すと、saved modelを使っていなかったときのコード(Tensorflow Liteのクラスとメソッドを使ってclassifierオブジェクトを直接変換したコード)では入力tensorとして「dnn/input_from_feature_columns/input_layer/concat」を使っていて、それに対して現状のsaved_model_cliの出力の中にはそういう入力tensorは存在せず、代わりに「Const_1」「Const_2」というtensorが全部で51個あるのがわかった。**51個**。これはこっちが渡そうとしている1つ1つのデータを入れる場所に違いないので、ループでset_tensorしている今のコードで大丈夫のはず…なんだけど、[https://gist.github.com/fumiakiy/a86a834352c1c2c5a8305e46b3a5e751](input_details[i][‘index’]と、配列の添字ではなくあえてオブジェクトのindexキーを使ってデータをセットしている)のはなんでなんだろう、と思いついて、input_detailsの中身をダンプしてみると、\n\n```\n[\n{‘index’: 0, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const’, ‘dtype’: \u0026lt;type ‘numpy.int64’\u0026gt;},\n{‘index’: 2, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_10’, ‘dtype’: \u0026lt;type ‘numpy.int64’\u0026gt;},\n{‘index’: 3, ‘shape’: array([1], dtype=int32), ‘quantization’: (0.0, 0L), ‘name’: ‘Const_11’, ‘dtype’: \u0026lt;type ‘numpy.int64’\u0026gt;}, …\n```\n\n0番の要素のindexは0だけど、1番の要素のindexは2になっているし、名前もConst_10でConst_1ではない! ということは、テストデータの1番要素の値をinput_details[1][‘index’]に入れてしまうと、1番のtensorにセットすべき値を2番のtensor(本来10番要素の値を入れる場所)にセットしていることになってしまうのでは?\n\n配列の要素とtensorの並びを正規化して、それから正しいtensorに値をset_tensorするようにコードを書き直してみた。\n\n```\n# for i in (range(len(y) — 1)):\n#     interpreter.set_tensor(input_details[i]['index'], [y[i]])\n```\n\n```\nindices = [0] * 51\nfor detail in input_details:\n    vindex = 0\n    m = re.match(r'Const_(\\d+)', detail['name'])\n    if (m is None):\n        vindex = 0\n    else:\n        vindex = int(m.group(1))\n    indices[vindex] = detail['index']\n```\n\n```\nfor i in range(len(y)-1):\n    interpreter.set_tensor(indices[i], [y[i]])\n```\n\n実行してみる。\n\n```\n$ python tflite1.py\n```\n\n```\n[[0. 0. 0. 0. 0. 0. 0. 1.]]\n```\n\nおお、yをyと予想したっぽい! yをeにして再度実行してみる。\n\n```\n# for i in range(len(y)-1):\n#     interpreter.set_tensor(indices[i], [y[i]])\nfor i in range(len(e)-1):\n    interpreter.set_tensor(indices[i], [e[i]])\n```\n\n```\n$ python tflite1.py\n```\n\n```\n[[0. 0. 0. 0. 1. 0. 0. 0.]]\n```\n\neも正しく予測している! このTensorflow Liteのモデルは良いものなのでは? Androidアプリで実行してみよう!\n\nというわけで次回へ続く。\n\n"},"__N_SSG":true},"page":"/blog/[postname]","query":{"postname":"2019-07-04_Levi-s-Commuter-Trucker-Jacket-with-Jacquard-by-Google----------------Android----------------a4747b4edca5"},"buildId":"wszE4MJpGHP-KveUJCtqu","runtimeConfig":{},"nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-e0a799dda1546678fa28.js"></script><script src="/_next/static/chunks/main-fd8a0b253a51098ec3a6.js" async=""></script><script src="/_next/static/chunks/webpack-488dc228921f1fdbc0e7.js" async=""></script><script src="/_next/static/chunks/framework.1b5fae6dde8a26be77ac.js" async=""></script><script src="/_next/static/chunks/e82d01500e11e0131e78851aa17fd9f5e63d6c88.616ccd5db4903f86882e.js" async=""></script><script src="/_next/static/chunks/pages/_app-012ce3de4391df2d9dee.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bpostname%5D-b07b452b8f3bf1c1e2fd.js" async=""></script><script src="/_next/static/wszE4MJpGHP-KveUJCtqu/_buildManifest.js" async=""></script><script src="/_next/static/wszE4MJpGHP-KveUJCtqu/_ssgManifest.js" async=""></script></body></html>